{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1iMaqn7Cm7EveArzxC3ZfZ14eMHjxvMw6","timestamp":1755114116822}],"toc_visible":true,"authorship_tag":"ABX9TyOQ9kz0yLzxKYJMmWBUFGp7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Optax with Flax NNX: Exercises for PyTorch Users\n","\n","This Colab notebook contains a series of exercises designed to help you, a PyTorch user, get hands-on experience with Optax, the primary optimization library in the JAX ecosystem, specifically for training Flax NNX models.\n","We will cover everything from the basics of setting up an optimizer to advanced techniques like learning rate scheduling, per-parameter optimization, and sharding for distributed training.\n","\n","## Setup\n","First, let's install the necessary libraries and set up a simulated multi-device environment. We'll use chex to simulate having 8 CPU devices, which will allow us to explore distributed training concepts without needing multiple physical GPUs/TPUs."],"metadata":{"id":"57CCWeofdudt"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"y6RKMYI3dsYy"},"outputs":[],"source":["!pip install -Uq flax jax optax chex\n","\n","import jax\n","import jax.numpy as jnp\n","from jax.sharding import Mesh, PartitionSpec, NamedSharding\n","import chex\n","import optax\n","from flax import nnx\n","\n","# Simulate an environment with 8 CPU devices for sharding exercises\n","try:\n","  chex.set_n_cpu_devices(8)\n","except RuntimeError as e:\n","  print(f\"Could not set n_cpu_devices: {e}\")\n","  print(\"Sharding exercises may not work as intended. Continuing anyway.\")\n","\n","# Helper to check available devices\n","print(f\"JAX is running on: {jax.default_backend()}\")\n","print(f\"Number of available devices: {jax.device_count()}\")\n","print(f\"Device details: {jax.devices()}\")"]},{"cell_type":"markdown","source":["## Exercise 1: The Basic Training Loop\n","\n","**Concept:** This exercise covers the fundamental workflow of training a Flax NNX model with Optax. You will:\n","\n","1. Define a simple MLP model using flax.nnx.Module.\n","2. Instantiate the model and a basic optax.adam optimizer using flax.nnx.Optimizer.\n","3. Write a Mean Squared Error (MSE) loss function.\n","4. Create a complete, JIT-compiled training step function that takes the model and optimizer as arguments, calculates the loss, computes gradients using flax.nnx.value_and_grad, and updates the model parameters using optimizer.update(model, grads).\n","\n","This process mirrors the standard \"instantiate, calculate loss, backpropagate, step\" cycle in PyTorch but introduces the JAX/Optax equivalents: nnx.Optimizer, nnx.value_and_grad, and optimizer.update().\n","\n","### Instructions\n","\n","Complete the TODO sections in the following code cell to implement the basic training loop."],"metadata":{"id":"bjBKyrSyeiTC"}},{"cell_type":"code","source":["# @title Exercise 1: Implement the Basic Training Loop\n","import jax\n","import jax.numpy as jnp\n","import optax\n","from flax import nnx\n","from typing import Sequence\n","\n","# 1. Define the Model\n","class SimpleMLP(nnx.Module):\n","  \"\"\"A simple Multi-Layer Perceptron.\"\"\"\n","  def __init__(self, features: Sequence[int], *, rngs: nnx.Rngs):\n","    self.layers = []\n","    for i in range(len(features) - 1):\n","        self.layers.append(nnx.Linear(features[i], features[i+1], rngs=rngs))\n","        if i < len(features) - 2:\n","            self.layers.append(nnx.relu)\n","\n","  def __call__(self, x: jax.Array):\n","    for layer in self.layers:\n","        x = layer(x)\n","    return x\n","\n","# 2. Define the Loss Function\n","def mse_loss(model: SimpleMLP, x_batch: jax.Array, y_batch: jax.Array) -> jax.Array:\n","  \"\"\"Calculates the Mean Squared Error loss.\"\"\"\n","  # TODO: Get predictions from the model and calculate the MSE.\n","  # Hint: The model is callable, e.g., model(x_batch).\n","  # YOUR CODE HERE\n","  return loss\n","\n","# 3. Define the Training Step\n","@nnx.jit\n","def train_step(model: SimpleMLP, optimizer: nnx.Optimizer, x_batch: jax.Array, y_batch: jax.Array):\n","  \"\"\"Performs a single training step.\"\"\"\n","  # TODO: Use nnx.value_and_grad to get both the loss and the gradients.\n","  # You'll need a loss function closure that takes only the model as an argument.\n","  def loss_fn_for_grad(model_to_train):\n","      return mse_loss(model_to_train, x_batch, y_batch)\n","\n","  loss_val, grads = # YOUR CODE HERE\n","\n","  # TODO: Update the optimizer with the gradients.\n","  # YOUR CODE HERE\n","\n","  # The optimizer's state is modified in-place by update(), but under jit,\n","  # we must return it to get the new state out.\n","  return model, optimizer, loss_val\n","\n","# --- Boilerplate for running the exercise ---\n","# Create dummy data\n","key = jax.random.key(42)\n","key_model, key_data = jax.random.split(key)\n","din, dmid, dout = 10, 20, 5\n","x_dummy = jax.random.normal(key_data, (32, din))\n","y_dummy = jax.random.normal(key_data, (32, dout))\n","\n","# Instantiate model and optimizer\n","model = SimpleMLP(features=[din, dmid, dout], rngs=nnx.Rngs(key_model))\n","opt = optax.adam(learning_rate=1e-3)\n","optimizer = nnx.Optimizer(model, opt, wrt=nnx.Param)\n","\n","# Training Loop\n","print(\"Starting basic training loop...\")\n","for i in range(101):\n","  optimizer, loss = train_step(optimizer, x_dummy, y_dummy)\n","  if i % 20 == 0:\n","    # The .value attribute is used to get the raw value from a State variable\n","    print(f\"Step {optimizer.step.value}, Loss: {loss:.4f}\")\n","print(\"Basic training loop finished.\")\n","# Verify the model parameters have been updated\n","assert optimizer.step.value == 101"],"metadata":{"id":"JgWREm3-eT_B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Solution 1\n","import jax\n","import jax.numpy as jnp\n","import optax\n","from flax import nnx\n","from typing import Sequence\n","\n","# 1. Define the Model\n","class SimpleMLP(nnx.Module):\n","  \"\"\"A simple Multi-Layer Perceptron.\"\"\"\n","  def __init__(self, features: Sequence[int], *, rngs: nnx.Rngs):\n","    self.layers = []\n","    for i in range(len(features) - 1):\n","        self.layers.append(nnx.Linear(features[i], features[i+1], rngs=rngs))\n","        if i < len(features) - 2:\n","            self.layers.append(nnx.relu)\n","\n","  def __call__(self, x: jax.Array):\n","    for layer in self.layers:\n","        x = layer(x)\n","    return x\n","\n","# 2. Define the Loss Function\n","def mse_loss(model: SimpleMLP, x_batch: jax.Array, y_batch: jax.Array) -> jax.Array:\n","  \"\"\"Calculates the Mean Squared Error loss.\"\"\"\n","  predictions = model(x_batch)\n","  loss = jnp.mean((predictions - y_batch) ** 2)\n","  return loss\n","\n","# 3. Define the Training Step\n","@nnx.jit\n","def train_step(model: SimpleMLP, optimizer: nnx.Optimizer, x_batch: jax.Array, y_batch: jax.Array):\n","  \"\"\"Performs a single training step.\"\"\"\n","  # A closure to capture the current batch of data\n","  def loss_fn_for_grad(model_to_train: SimpleMLP):\n","    return mse_loss(model_to_train, x_batch, y_batch)\n","\n","  # Compute loss and gradients\n","  loss_val, grads = nnx.value_and_grad(loss_fn_for_grad)(model)\n","\n","  # Update the optimizer's state and model parameters\n","  optimizer.update(model, grads)\n","\n","  return model, optimizer, loss_val\n","\n","# --- Boilerplate for running the exercise ---\n","# Create dummy data\n","key = jax.random.key(42)\n","key_model, key_data = jax.random.split(key)\n","din, dmid, dout = 10, 20, 5\n","x_dummy = jax.random.normal(key_data, (32, din))\n","y_dummy = jax.random.normal(key_data, (32, dout))\n","\n","# Instantiate model and optimizer\n","model = SimpleMLP(features=[din, dmid, dout], rngs=nnx.Rngs(key_model))\n","opt = optax.adam(learning_rate=1e-3)\n","optimizer = nnx.Optimizer(model, opt, wrt=nnx.Param)\n","\n","# Training Loop\n","print(\"Starting basic training loop...\")\n","for i in range(101):\n","  optimizer, loss = train_step(optimizer, x_dummy, y_dummy)\n","  if i % 20 == 0:\n","    print(f\"Step {optimizer.step.value}, Loss: {loss:.4f}\")\n","print(\"Basic training loop finished.\")\n","assert optimizer.step.value == 101"],"metadata":{"id":"KZ0wslm9fSW5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 2: Composing Gradient Transformations\n","\n","**Concept:** A core philosophy of Optax is composability. Instead of monolithic optimizers, Optax provides small, chainable \"gradient transformations.\" This exercise demonstrates how to build a custom optimization pipeline by chaining multiple transformations together.\n","\n","You will add gradient clipping and weight decay to the Adam optimizer, creating a more robust optimization rule. This is analogous to combining features that might be built-in flags in a PyTorch optimizer, but here you explicitly build the chain.\n","\n","### Instructions\n","\n","Complete the TODO section to create a chained Optax transformation."],"metadata":{"id":"exY9yGJ5faqc"}},{"cell_type":"code","source":["# @title Exercise 2: Build a Chained Optimizer\n","import jax\n","import jax.numpy as jnp\n","import optax\n","from flax import nnx\n","\n","# --- Using the same model and data setup from Exercise 1 ---\n","key = jax.random.key(42)\n","key_model, key_data = jax.random.split(key)\n","din, dmid, dout = 10, 20, 5\n","x_dummy = jax.random.normal(key_data, (32, din))\n","y_dummy = jax.random.normal(key_data, (32, dout))\n","\n","model_chained = SimpleMLP(features=[din, dmid, dout], rngs=nnx.Rngs(key_model))\n","\n","# Define hyperparameters\n","learning_rate = 1e-3\n","max_grad_norm = 1.0\n","weight_decay = 1e-4\n","\n","# TODO: Create a chained Optax transformation.\n","# The desired order is:\n","# 1. Clip gradients by their global norm (optax.clip_by_global_norm).\n","# 2. Add weight decay (optax.add_decayed_weights).\n","# 3. Apply Adam optimizer updates (optax.adam).\n","# Hint: Use optax.chain([...])\n","opt_chained = optax.chain(\n","    # YOUR CODE HERE\n",")\n","\n","\n","# --- Boilerplate for running the exercise ---\n","# The train_step and mse_loss from Exercise 1 can be reused directly!\n","optimizer_chained = nnx.Optimizer(model_chained, opt_chained, wrt=nnx.Param)\n","\n","print(\"Starting training with chained optimizer...\")\n","for i in range(101):\n","  model_chained, optimizer_chained, loss = train_step(model_chained, optimizer_chained, x_dummy, y_dummy)\n","  if i % 20 == 0:\n","    print(f\"Step {optimizer_chained.step.value}, Loss: {loss:.4f}\")\n","print(\"Chained optimizer training finished.\")"],"metadata":{"id":"UkMUe4wMfW8a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Solution 2\n","import jax\n","import jax.numpy as jnp\n","import optax\n","from flax import nnx\n","\n","# --- Using the same model and data setup from Exercise 1 ---\n","key = jax.random.key(42)\n","key_model, key_data = jax.random.split(key)\n","din, dmid, dout = 10, 20, 5\n","x_dummy = jax.random.normal(key_data, (32, din))\n","y_dummy = jax.random.normal(key_data, (32, dout))\n","\n","model_chained = SimpleMLP(features=[din, dmid, dout], rngs=nnx.Rngs(key_model))\n","\n","# Define hyperparameters\n","max_grad_norm = 1.0\n","weight_decay = 1e-4\n","learning_rate = 1e-3\n","\n","# Create a chained Optax transformation\n","opt_chained = optax.chain(\n","    optax.clip_by_global_norm(max_grad_norm),\n","    optax.add_decayed_weights(weight_decay),\n","    optax.adam(learning_rate)\n",")\n","\n","# --- Boilerplate for running the exercise ---\n","# The train_step and mse_loss from Exercise 1 can be reused directly!\n","optimizer_chained = nnx.Optimizer(model_chained, opt_chained, wrt=nnx.Param)\n","\n","print(\"Starting training with chained optimizer...\")\n","for i in range(101):\n","  model_chained, optimizer_chained, loss = train_step(model_chained, optimizer_chained, x_dummy, y_dummy)\n","  if i % 20 == 0:\n","    print(f\"Step {optimizer_chained.step.value}, Loss: {loss:.4f}\")\n","print(\"Chained optimizer training finished.\")"],"metadata":{"id":"hKTMXfwNf6Bi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 3: Learning Rate Scheduling\n","\n","**Concept:** Dynamically adjusting the learning rate during training is a crucial technique. In Optax, you don't use an external scheduler.step() like in PyTorch. Instead, the schedule is baked directly into the optimizer definition.\n","\n","This exercise asks you to create a learning rate schedule and pass it to your optimizer. Optax will handle the updates automatically at each step. You will implement a warmup-cosine-decay schedule, a very common and effective schedule.\n","\n","### Instructions\n","Complete the TODO sections to define a learning rate schedule and use it in an Adam optimizer."],"metadata":{"id":"Vub5qv9Ll1qk"}},{"cell_type":"code","source":["# @title Exercise 3: Implement a Learning Rate Schedule\n","import jax\n","import jax.numpy as jnp\n","import optax\n","from flax import nnx\n","import matplotlib.pyplot as plt\n","\n","# --- Using the same model and data setup from Exercise 1 ---\n","key = jax.random.key(42)\n","key_model, key_data = jax.random.split(key)\n","din, dmid, dout = 10, 20, 5\n","x_dummy = jax.random.normal(key_data, (32, din))\n","y_dummy = jax.random.normal(key_data, (32, dout))\n","\n","model_scheduled = SimpleMLP(features=[din, dmid, dout], rngs=nnx.Rngs(key_model))\n","\n","# --- Scheduling Hyperparameters ---\n","total_training_steps = 500\n","warmup_fraction = 0.1\n","peak_lr = 1e-3\n","final_lr = 1e-5\n","\n","# TODO: Define a warmup-cosine-decay learning rate schedule.\n","# Hint: Use optax.warmup_cosine_decay_schedule.\n","# It needs an initial value, a peak value, warmup steps, and decay steps.\n","warmup_steps = # YOUR CODE HERE\n","decay_steps = # YOUR CODE HERE\n","\n","lr_schedule_fn = optax.warmup_cosine_decay_schedule(\n","    init_value=0.0,\n","    peak_value=peak_lr,\n","    warmup_steps=warmup_steps,\n","    decay_steps=decay_steps,\n","    end_value=final_lr\n",")\n","\n","\n","# TODO: Create an Adam optimizer that uses this schedule.\n","# Hint: Simply pass the schedule function as the `learning_rate` argument.\n","opt_scheduled = # YOUR CODE HERE\n","\n","\n","# --- Boilerplate for running the exercise ---\n","optimizer_scheduled = nnx.Optimizer(model_scheduled, opt_scheduled, wrt=nnx.Param)\n","\n","# Training Loop\n","print(\"Starting training with scheduled LR...\")\n","lrs = []\n","for i in range(total_training_steps):\n","  # The LR is updated automatically inside train_step\n","  model_scheduled, optimizer_scheduled, loss = train_step(model_scheduled, optimizer_scheduled, x_dummy, y_dummy)\n","  # We can extract the current LR for plotting\n","  # Note: This requires the optimizer state to be on the host.\n","  # In a real scenario, you might not check this every step.\n","  current_lr = lr_schedule_fn(optimizer_scheduled.step.value)\n","  lrs.append(current_lr)\n","  if i % 50 == 0:\n","    print(f\"Step {optimizer_scheduled.step.value}, Loss: {loss:.5f}, LR: {current_lr:.6f}\")\n","print(\"Scheduled LR training finished.\")\n","\n","# Plot the learning rate over time\n","plt.figure(figsize=(10, 4))\n","plt.plot(lrs)\n","plt.title(\"Learning Rate Schedule\")\n","plt.xlabel(\"Training Step\")\n","plt.ylabel(\"Learning Rate\")\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"w55m9SrYf-hF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Solution 3\n","import jax\n","import jax.numpy as jnp\n","import optax\n","from flax import nnx\n","import matplotlib.pyplot as plt\n","\n","# --- Using the same model and data setup from Exercise 1 ---\n","key = jax.random.key(42)\n","key_model, key_data = jax.random.split(key)\n","din, dmid, dout = 10, 20, 5\n","x_dummy = jax.random.normal(key_data, (32, din))\n","y_dummy = jax.random.normal(key_data, (32, dout))\n","\n","model_scheduled = SimpleMLP(features=[din, dmid, dout], rngs=nnx.Rngs(key_model))\n","\n","# --- Scheduling Hyperparameters ---\n","total_training_steps = 500\n","warmup_fraction = 0.1\n","peak_lr = 1e-3\n","final_lr = 1e-5\n","\n","# Define a warmup-cosine-decay learning rate schedule\n","warmup_steps = int(total_training_steps * warmup_fraction)\n","decay_steps = total_training_steps - warmup_steps\n","\n","lr_schedule_fn = optax.warmup_cosine_decay_schedule(\n","    init_value=0.0,\n","    peak_value=peak_lr,\n","    warmup_steps=warmup_steps,\n","    decay_steps=decay_steps,\n","    end_value=final_lr\n",")\n","\n","# Create an Adam optimizer that uses this schedule\n","opt_scheduled = optax.adam(learning_rate=lr_schedule_fn)\n","\n","# --- Boilerplate for running the exercise ---\n","optimizer_scheduled = nnx.Optimizer(model_scheduled, opt_scheduled, wrt=nnx.Param)\n","\n","# Training Loop\n","print(\"Starting training with scheduled LR...\")\n","lrs = []\n","for i in range(total_training_steps):\n","  model_scheduled, optimizer_scheduled, loss = train_step(model_scheduled, optimizer_scheduled, x_dummy, y_dummy)\n","  current_lr = lr_schedule_fn(optimizer_scheduled.step.value)\n","  lrs.append(current_lr)\n","  if i % 50 == 0:\n","    print(f\"Step {optimizer_scheduled.step.value}, Loss: {loss:.5f}, LR: {current_lr:.6f}\")\n","print(\"Scheduled LR training finished.\")\n","\n","# Plot the learning rate over time\n","plt.figure(figsize=(10, 4))\n","plt.plot(lrs)\n","plt.title(\"Learning Rate Schedule\")\n","plt.xlabel(\"Training Step\")\n","plt.ylabel(\"Learning Rate\")\n","plt.grid(True)\n","plt.show()"],"metadata":{"id":"oYZFC9udmXxp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 4: Per-Parameter Optimization\n","\n","**Concept:** It's often beneficial to apply different optimization rules to different model parameters. For example, you might not want to apply weight decay to bias parameters or normalization layer scales. In PyTorch, this is handled with \"parameter groups.\" In Optax, the equivalent is `optax.partition`.\n","\n","This exercise will guide you through:\n","\n","1. Writing a \"labeling function\" that assigns a string label ('bias', 'kernel', or 'other') to each parameter in your model based on its name.\n","2. Using optax.partition to create a composite optimizer that applies different learning rates to biases and kernels.\n","\n","### Instructions\n","Complete the TODO sections to implement per-parameter optimization."],"metadata":{"id":"FFrIYmXEmiBP"}},{"cell_type":"code","source":["# @title Exercise 4: Implement Per-Parameter Optimization\n","import jax\n","import jax.numpy as jnp\n","import optax\n","from flax import nnx\n","\n","# --- Using the same model and data setup from Exercise 1 ---\n","key = jax.random.key(42)\n","key_model, key_data = jax.random.split(key)\n","din, dmid, dout = 10, 20, 5\n","x_dummy = jax.random.normal(key_data, (32, din))\n","y_dummy = jax.random.normal(key_data, (32, dout))\n","\n","model_partitioned = SimpleMLP(features=[din, dmid, dout], rngs=nnx.Rngs(key_model))\n","\n","# 1. Create the parameter labels PyTree\n","# Get a PyTree of the model's parameters to generate labels for\n","params_pytree = nnx.state(model_partitioned, nnx.Param)\n","\n","# TODO: Implement the labeling function.\n","# It should inspect the path to a parameter and return a string label.\n","def label_fn(path, leaf):\n","  \"\"\"Assigns a label to a parameter based on its path.\"\"\"\n","  # The path is a tuple of keys\n","  # We can check the name of the last attribute in the path.\n","  # YOUR CODE HERE\n","\n","# Use tree_map_with_path to apply the labeling function\n","param_labels = jax.tree.map_with_path(label_fn, params_pytree)\n","\n","print(\"Generated Parameter Labels PyTree:\")\n","nnx.display(param_labels)\n","\n","# 2. TODO: Define the partitioned optimizer.\n","# Use optax.partition, providing a dictionary mapping your labels\n","# to different Optax transformations.\n","# - Use Adam with LR 1e-3 for 'kernel'\n","# - Use SGD with LR 5e-3 for 'bias'\n","# - Use Adam with LR 1e-4 for 'other' (a default)\n","partitioned_opt = optax.partition(\n","    transforms={\n","        # YOUR CODE HERE\n","    },\n","    param_labels=param_labels\n",")\n","\n","\n","# --- Boilerplate for running the exercise ---\n","optimizer_partitioned = nnx.Optimizer(model_partitioned, partitioned_opt, wrt=nnx.Param)\n","\n","print(\"\\nStarting training with partitioned optimizer...\")\n","for i in range(101):\n","  model_partitioned, optimizer_partitioned, loss = train_step(model_partitioned, optimizer_partitioned, x_dummy, y_dummy)\n","  if i % 20 == 0:\n","    print(f\"Step {optimizer_partitioned.step.value}, Loss: {loss:.4f}\")\n","print(\"Partitioned optimizer training finished.\")\n","\n","# Verify the optimizer state structure\n","opt_state_structure = jax.tree_util.tree_map(\n","    lambda x: x.__class__.__name__, optimizer_partitioned.state.opt_state\n",")\n","print(\"\\nStructure of the partitioned optimizer's state:\")\n","nnx.display(opt_state_structure)\n","assert 'PartitionState' in str(opt_state_structure)"],"metadata":{"id":"3yZt1Cl6mctx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Solution 4\n","import jax\n","import jax.numpy as jnp\n","import optax\n","from flax import nnx\n","\n","# --- Using the same model and data setup from Exercise 1 ---\n","key = jax.random.key(42)\n","key_model, key_data = jax.random.split(key)\n","din, dmid, dout = 10, 20, 5\n","x_dummy = jax.random.normal(key_data, (32, din))\n","y_dummy = jax.random.normal(key_data, (32, dout))\n","\n","model_partitioned = SimpleMLP(features=[din, dmid, dout], rngs=nnx.Rngs(key_model))\n","\n","# 1. Create the parameter labels PyTree\n","params_pytree = nnx.state(model_partitioned, nnx.Param)\n","\n","def label_fn(path, leaf):\n","  \"\"\"Assigns a label to a parameter based on its path.\"\"\"\n","  param_name = path[-1].name\n","  if 'bias' in param_name:\n","    return 'bias'\n","  elif 'kernel' in param_name:\n","    return 'kernel'\n","  return 'other'\n","\n","param_labels = jax.tree.map_with_path(label_fn, params_pytree)\n","\n","print(\"Generated Parameter Labels PyTree:\")\n","nnx.display(param_labels)\n","\n","# 2. Define the partitioned optimizer\n","partitioned_opt = optax.partition(\n","    transforms={\n","        'kernel': optax.adam(learning_rate=1e-3),\n","        'bias': optax.sgd(learning_rate=5e-3),\n","        'other': optax.adam(learning_rate=1e-4),\n","    },\n","    param_labels=param_labels\n",")\n","\n","# --- Boilerplate for running the exercise ---\n","optimizer_partitioned = nnx.Optimizer(model_partitioned, partitioned_opt, wrt=nnx.Param)\n","\n","print(\"\\nStarting training with partitioned optimizer...\")\n","for i in range(101):\n","  model_partitioned, optimizer_partitioned, loss = train_step(model_partitioned, optimizer_partitioned, x_dummy, y_dummy)\n","  if i % 20 == 0:\n","    print(f\"Step {optimizer_partitioned.step.value}, Loss: {loss:.4f}\")\n","print(\"Partitioned optimizer training finished.\")\n","\n","# Verify the optimizer state structure\n","opt_state_structure = jax.tree_util.tree_map(\n","    lambda x: x.__class__.__name__, optimizer_partitioned.opt_state\n",")\n","print(\"\\nStructure of the partitioned optimizer's state:\")\n","nnx.display(opt_state_structure)\n","assert 'PartitionState' in str(opt_state_structure)"],"metadata":{"id":"Hi03anmEm9B1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 5: Sharding the Model and Optimizer State\n","\n","**Concept:** JAX provides fine-grained control over how data and model parameters are distributed across devices. This is done by explicitly annotating PyTrees (like model parameters or optimizer state) with sharding information.\n","\n","In this exercise, you will:\n","\n","1. Create a 2D device Mesh from our 8 simulated CPUs.\n","2. Define a sharded MLP where the kernel of a linear layer is sharded across the 'model' axis of the mesh (Model Parallelism).\n","3. Create a sharded optimizer whose state (e.g., Adam's momentum and variance vectors) automatically inherits the same sharding as the corresponding model parameters.\n","\n","### Instructions\n","Complete the TODO sections to shard your model and optimizer."],"metadata":{"id":"P3VGU-0Rq0zo"}},{"cell_type":"code","source":["# @title Exercise 5: Sharding Model and Optimizer\n","import jax\n","import jax.numpy as jnp\n","import optax\n","from flax import nnx\n","from jax.sharding import Mesh, PartitionSpec as P\n","import numpy as np\n","\n","# Ensure we have our 8 simulated devices\n","if jax.device_count() != 8:\n","    print(\"Warning: This exercise expects 8 devices. Sharding may not behave as expected.\")\n","\n","# 1. Create a device mesh\n","# We'll create a 2x4 mesh, with a 'data' axis for data parallelism\n","# and a 'model' axis for model parallelism.\n","devices = np.array(jax.devices()).reshape(2, 4)\n","mesh = Mesh(devices, axis_names=('data', 'model'))\n","print(\"Created 2x4 device mesh:\")\n","print(mesh)\n","\n","# 2. Define a sharded model\n","class ShardedMLP(nnx.Module):\n","  def __init__(self, din, dmid, dout, *, rngs: nnx.Rngs):\n","    # TODO: Shard the kernel of the second linear layer.\n","    # The goal is to split the kernel's columns across the 'model' axis.\n","    # This is a form of model parallelism.\n","    # - The first dimension (input features) should be replicated.\n","    # - The second dimension (output features) should be sharded.\n","    # - The bias should also be sharded along the 'model' axis.\n","    # - All other parameters can be replicated (the default).\n","    self.linear1 = nnx.Linear(din, dmid, rngs=rngs)\n","    self.relu = nnx.relu\n","    self.linear2 = nnx.Linear(dmid, dout, rngs=rngs)\n","\n","    # Shard linear1 fully (replicated)\n","    self.linear1.kernel.sharding = P(None, None) # or just P()\n","    self.linear1.bias.sharding = P(None) # or just P()\n","\n","    # Shard linear2 for model parallelism\n","    # YOUR CODE HERE - Replicate rows, shard columns\n","    # YOUR CODE HERE - Shard the bias vector\n","\n","  def __call__(self, x):\n","    x = self.linear1(x)\n","    x = self.relu(x)\n","    x = self.linear2(x)\n","    return x\n","\n","# 3. Create sharded model and optimizer within the mesh context\n","@nnx.jit\n","def create_sharded_model_and_optimizer():\n","  key = jax.random.key(0)\n","  model = ShardedMLP(16, 32, 64, rngs=nnx.Rngs(key))\n","  optimizer = nnx.Optimizer(model, optax.adam(1e-3), wrt=nnx.Param)\n","\n","  # The sharding annotations on the model are automatically picked up.\n","  # Now, we need to ensure the optimizer state gets the same shardings.\n","  # nnx.Optimizer automatically infers this from the model's parameters!\n","  # We just need to use jax.lax.with_sharding_constraint to enforce it\n","  # during JIT compilation.\n","\n","  # Shard model state based on annotations\n","  model_state = nnx.state(model)\n","  model_shardings = nnx.spmd.get_partition_spec(model_state)\n","  sharded_model_state = jax.lax.with_sharding_constraint(model_state, model_shardings)\n","  nnx.update(model, sharded_model_state)\n","\n","  # TODO: Shard the optimizer state.\n","  # The process is identical to sharding the model, but you need to filter\n","  # for the optimizer's state using nnx.optimizer.OptState.\n","  # YOUR CODE HERE\n","  opt_shardings = nnx.spmd.get_partition_spec(opt_state_to_shard)\n","  sharded_opt_state = jax.lax.with_sharding_constraint(\n","      opt_state_to_shard, opt_shardings\n","  )\n","  nnx.update(optimizer, sharded_opt_state)\n","\n","  return model, optimizer\n","\n","# Run the creation function within the mesh context manager\n","with mesh:\n","  sharded_model, sharded_optimizer = create_sharded_model_and_optimizer()\n","\n","\n","# --- Verification ---\n","print(\"\\n--- Verifying Shardings ---\")\n","# Get the sharded state back from the JIT call\n","final_model_state = nnx.state(sharded_model)\n","final_opt_state = nnx.state(sharded_optimizer, nnx.optimizer.OptState)\n","\n","# Check the sharding of the second linear layer's kernel in the model\n","l2_kernel_sharding = final_model_state['layers']['1']['kernel'].sharding\n","print(f\"\\nModel's linear2.kernel sharding: {l2_kernel_sharding}\")\n","assert l2_kernel_sharding == NS(None, 'model')\n","\n","# Check the sharding of the corresponding momentum (m) in the optimizer state\n","# The optimizer state PyTree mirrors the parameter PyTree structure.\n","adam_state = final_opt_state['opt_state'][1] # (trace_state, adam_state)\n","l2_kernel_momentum_sharding = adam_state.m['layers']['1']['kernel'].sharding\n","print(f\"Optimizer's momentum for linear2.kernel sharding: {l2_kernel_momentum_sharding}\")\n","assert l2_kernel_momentum_sharding == NS(None, 'model')\n","\n","print(\"\\nSuccessfully verified that optimizer state sharding matches model parameter sharding.\")"],"metadata":{"id":"K9SE-quUnBhb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Solution 5\n","import jax\n","import jax.numpy as jnp\n","import optax\n","from flax import nnx\n","from jax.sharding import Mesh, PartitionSpec as P\n","import numpy as np\n","\n","# Ensure we have our 8 simulated devices\n","if jax.device_count() != 8:\n","    print(\"Warning: This exercise expects 8 devices. Sharding may not behave as expected.\")\n","\n","# 1. Create a device mesh\n","devices = np.array(jax.devices()).reshape(2, 4)\n","mesh = Mesh(devices, axis_names=('data', 'model'))\n","print(\"Created 2x4 device mesh:\")\n","print(mesh)\n","\n","# 2. Define a sharded model\n","class ShardedMLP(nnx.Module):\n","  def __init__(self, din, dmid, dout, *, rngs: nnx.Rngs):\n","    self.linear1 = nnx.Linear(din, dmid, rngs=rngs)\n","    self.relu = nnx.relu\n","    self.linear2 = nnx.Linear(dmid, dout, rngs=rngs)\n","\n","    # Shard linear1 fully (replicated) - this is often the default\n","    self.linear1.kernel.sharding = P() # Replicated on all axes\n","    self.linear1.bias.sharding = P()   # Replicated on all axes\n","\n","    # Shard linear2 for model parallelism\n","    # Shard the output dimension of the kernel and the bias\n","    self.linear2.kernel.sharding = P(None, 'model') # Replicate rows, shard columns\n","    self.linear2.bias.sharding = P('model')         # Shard the bias vector\n","\n","  def __call__(self, x):\n","    x = self.linear1(x)\n","    x = self.relu(x)\n","    x = self.linear2(x)\n","    return x\n","\n","# 3. Create sharded model and optimizer within the mesh context\n","@nnx.jit\n","def create_sharded_model_and_optimizer():\n","  key = jax.random.key(0)\n","  model = ShardedMLP(16, 32, 64, rngs=nnx.Rngs(key))\n","  optimizer = nnx.Optimizer(model, optax.adam(1e-3), wrt=nnx.Param)\n","\n","  # Shard model state based on annotations\n","  model_state = nnx.state(model)\n","  model_shardings = nnx.spmd.get_partition_spec(model_state)\n","  sharded_model_state = jax.lax.with_sharding_constraint(model_state, model_shardings)\n","  nnx.update(model, sharded_model_state)\n","\n","  # Shard the optimizer state\n","  # Filter for the optimizer's state (step and Optax's internal state)\n","  opt_state_to_shard = nnx.state(optimizer, nnx.optimizer.OptState)\n","  # Infer the sharding specification from the parameter shardings\n","  opt_shardings = nnx.spmd.get_partition_spec(opt_state_to_shard)\n","  # Apply the sharding constraint\n","  sharded_opt_state = jax.lax.with_sharding_constraint(\n","      opt_state_to_shard, opt_shardings\n","  )\n","  nnx.update(optimizer, sharded_opt_state)\n","\n","  return model, optimizer\n","\n","# Run the creation function within the mesh context manager\n","with mesh:\n","  sharded_model, sharded_optimizer = create_sharded_model_and_optimizer()\n","\n","\n","# --- Verification ---\n","print(\"\\n--- Verifying Shardings ---\")\n","# Get the sharded state back from the JIT call\n","final_model_state = nnx.state(sharded_model)\n","final_opt_state = nnx.state(sharded_optimizer, nnx.optimizer.OptState)\n","\n","# Check the sharding of the second linear layer's kernel in the model\n","l2_kernel_sharding = final_model_state['linear2']['kernel'].sharding\n","print(f\"\\nModel's linear2.kernel sharding: {l2_kernel_sharding}\")\n","assert l2_kernel_sharding == P(None, 'model')\n","\n","# Check the sharding of the corresponding momentum (m) in the optimizer state\n","# The optimizer state PyTree mirrors the parameter PyTree structure.\n","# For optax.adam, the state is a tuple of (trace_state, adam_state).\n","# We look inside the AdamState.\n","adam_state = final_opt_state['opt_state'][0]\n","l2_kernel_momentum_sharding = adam_state.mu['linear2']['kernel'].sharding\n","print(f\"Optimizer's momentum for linear2.kernel sharding: {l2_kernel_momentum_sharding}\")\n","assert l2_kernel_momentum_sharding == P(None, 'model')\n","\n","print(\"\\nSuccessfully verified that optimizer state sharding matches model parameter sharding.\")"],"metadata":{"id":"S1PHHOj1rMKP"},"execution_count":null,"outputs":[]}]}