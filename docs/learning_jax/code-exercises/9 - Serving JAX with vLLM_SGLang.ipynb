{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1wU06hEOn87VZwNKG2c5E-lQpv4LII3y2","timestamp":1755114036632},{"file_id":"1vUmOju_8clAPQ4M0aI0PHwypAIE15dIk","timestamp":1743983895983}],"gpuType":"T4","machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Serving a JAX model with vLLM and GPU\n","\n","This notebook shows a simple workflow from a model which is loaded from Hugging Face into JAX, and then served using vLLM.  For brevity we leave out the actual fine-tuning or other alterations in JAX, since this is covered in other tutorials.  This is right on the edge of what can be done in a free Colab GPU instance, so we restart before installing vLLM to free up memory.  As a bonus, this notebook contains a JAX implementation of a Llama 3.2 model, which can be interesting by itself."],"metadata":{"id":"kFSEN6lZVPJ8"}},{"cell_type":"markdown","source":["# Do all the Pips\n","Let's get the downloads out of the way."],"metadata":{"id":"otDKAxr1_Cm7"}},{"cell_type":"code","source":["!pip install -Uq jax[cuda] flax # Install the JAX AI Stack for GPU\n","!pip install -q vllm # We'll need it later"],"metadata":{"id":"DU73PpuC-1Nf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Hugging Face"],"metadata":{"id":"RTQtpU7GCsJX"}},{"cell_type":"markdown","source":["## Download the model from Hugging Face\n","\n","We'll download the model weights in Safetensors format."],"metadata":{"id":"qKOJ3dLAMWho"}},{"cell_type":"code","source":["!huggingface-cli login"],"metadata":{"id":"g_VCX510CpNH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from huggingface_hub import snapshot_download\n","\n","model_id = \"meta-llama/Llama-3.2-1B\"\n","path_to_model_weights = os.path.join('/content', model_id)\n","\n","snapshot_download(repo_id=model_id, local_dir=path_to_model_weights)"],"metadata":{"id":"fA1VIuSXeU8L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the weights from the Safetensors file in Flax format\n","\n","import jax\n","from pathlib import Path\n","from safetensors import safe_open\n","\n","def load_safetensors():\n","  weights = {}\n","  safetensors_files = Path(path_to_model_weights).glob('*.safetensors')\n","\n","  for file in safetensors_files:\n","    with safe_open(file, framework=\"flax\") as f:\n","      for key in f.keys():\n","        print(f\"Loading {key}\")\n","        weights[key] = f.get_tensor(key)\n","  return weights\n","\n","weights = load_safetensors()"],"metadata":{"id":"VQxt8nEsekn2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Llama 3.2-1B JAX Implementation"],"metadata":{"id":"qHz4Zkd6bpib"}},{"cell_type":"code","source":["# # Install the JAX AI Stack for GPU\n","# !pip install -q jax[cuda] jax-ai-stack\n","\n","import jax\n","print(jax.devices())\n","print(jax.__version__)"],"metadata":{"id":"PJvcveFoadvw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from flax import nnx\n","from dataclasses import dataclass\n","import jax.numpy as jnp\n","\n","@dataclass\n","class LlamaConfig:\n","  def __init__(self):\n","    self.dim = 2048\n","    self.n_layers = 16\n","    self.n_heads = 32\n","    self.n_kv_heads = 8\n","    self.head_dim = self.dim // self.n_heads\n","    self.intermediate_size = 14336\n","    self.vocab_size = 128256\n","    self.multiple_of = 256\n","    self.norm_eps = 1e-05\n","    self.rope_theta = 500000.0\n","\n","config = LlamaConfig()\n","\n","class LlamaRMSNorm(nnx.Module):\n","\n","  def __init__(self, dim: int, rngs=None):\n","    self.norm_weights = nnx.Param(jnp.zeros((dim,), dtype=jnp.bfloat16))\n","\n","  @nnx.jit()\n","  def __call__(self, hidden_states):\n","    input_dtype = hidden_states.dtype\n","    hidden_states = hidden_states.astype(jnp.float32)\n","    squared_mean = jnp.mean(jnp.square(hidden_states), axis=-1, keepdims=True)\n","    hidden_states = hidden_states * jnp.reciprocal(jnp.sqrt(squared_mean + config.norm_eps))\n","    return self.norm_weights * hidden_states.astype(input_dtype)\n","\n","class LlamaRotaryEmbedding(nnx.Module):\n","\n","  def __init__(self, dim, base=10000, rngs=None):\n","    self.dim = dim\n","    self.base = base\n","\n","  @nnx.jit()\n","  def __call__(self, position_ids):\n","    inv_freq = 1.0 / (self.base ** (jnp.arange(0, self.dim, 2, dtype=jnp.float32) / self.dim))\n","    inv_freq_expanded = jnp.expand_dims(inv_freq, axis=(0, 1))\n","    position_ids_expanded = jnp.expand_dims(position_ids, axis=(0, 2)).astype(jnp.float32)\n","    freqs = jnp.einsum('bij,bjk->bijk', position_ids_expanded, inv_freq_expanded)\n","    emb = jnp.concatenate([freqs, freqs], axis=-1)\n","    cos = jnp.cos(emb).squeeze(2).astype(jnp.bfloat16)\n","    sin = jnp.sin(emb).squeeze(2).astype(jnp.bfloat16)\n","    return cos, sin\n","\n","class LlamaAttention(nnx.Module):\n","\n","  def __init__(self, layer_idx, rngs=None):\n","    self.q_proj = nnx.Linear(config.dim, config.n_heads * config.head_dim, use_bias=False, rngs=rngs, param_dtype=jnp.bfloat16)\n","    self.k_proj = nnx.Linear(config.dim, config.n_kv_heads * config.head_dim, use_bias=False, rngs=rngs, param_dtype=jnp.bfloat16)\n","    self.v_proj = nnx.Linear(config.dim, config.n_kv_heads * config.head_dim, use_bias=False, rngs=rngs, param_dtype=jnp.bfloat16)\n","    self.o_proj = nnx.Linear(config.n_heads * config.head_dim, config.dim, use_bias=False, rngs=rngs, param_dtype=jnp.bfloat16)\n","    self.rotary_emb = LlamaRotaryEmbedding(config.head_dim, base=config.rope_theta, rngs=rngs)\n","\n","  # Alternative implementation:\n","  # https://github.com/google/flax/blob/5d896bc1a2c68e2099d147cd2bc18ebb6a46a0bd/examples/gemma/positional_embeddings.py#L45\n","  def apply_rotary_pos_emb(self, q, k, cos, sin, unsqueeze_dim=1):\n","    cos = jnp.expand_dims(cos, axis=unsqueeze_dim)\n","    sin = jnp.expand_dims(sin, axis=unsqueeze_dim)\n","    q_embed = (q * cos) + (self.rotate_half(q) * sin)\n","    k_embed = (k * cos) + (self.rotate_half(k) * sin)\n","    return q_embed, k_embed\n","\n","  def rotate_half(self, x):\n","    x1 = x[..., : x.shape[-1] // 2]\n","    x2 = x[..., x.shape[-1] // 2 :]\n","    return jnp.concatenate([-x2, x1], axis=-1)\n","\n","  def repeat_kv(self, hidden_states, n_repeat):\n","    batch, n_kv_heads, seq_len, head_dim = hidden_states.shape\n","    if n_repeat == 1:\n","      return hidden_states\n","    hidden_states = hidden_states[:, :, None, :, :].repeat(n_repeat, axis=2)\n","    return hidden_states.reshape(batch, n_kv_heads * n_repeat, seq_len, head_dim)\n","\n","  @nnx.jit()\n","  def __call__(self, x, position_ids):\n","    batch_size, seq_len, _ = x.shape\n","    query = self.q_proj(x).reshape(batch_size, seq_len, config.n_heads, config.head_dim).transpose((0, 2, 1, 3))\n","    key = self.k_proj(x).reshape(batch_size, seq_len, config.n_kv_heads, config.head_dim).transpose((0, 2, 1, 3))\n","    value = self.v_proj(x).reshape(batch_size, seq_len, config.n_kv_heads, config.head_dim).transpose((0, 2, 1, 3))\n","    # Assuming batch_size=1\n","    cos, sin = self.rotary_emb(position_ids[0])\n","    query, key = self.apply_rotary_pos_emb(query, key, cos, sin)\n","\n","    key = self.repeat_kv(key, config.n_heads // config.n_kv_heads)\n","    value = self.repeat_kv(value, config.n_heads // config.n_kv_heads)\n","\n","    attn_weights = jnp.matmul(query, jnp.transpose(key, (0, 1, 3, 2)))\n","    attn_weights = (attn_weights.astype(jnp.float32) / jnp.sqrt(config.head_dim)).astype(jnp.bfloat16)\n","    attn_weights = jax.nn.softmax(attn_weights.astype(jnp.float32), axis=-1).astype(jnp.bfloat16)\n","    attn_output = jnp.matmul(attn_weights, value).transpose((0, 2, 1, 3)).reshape(batch_size, seq_len, -1)\n","    output = self.o_proj(attn_output)\n","    return output\n","\n","class LlamaMLP(nnx.Module):\n","\n","  def __init__(self, layer_idx, rngs=None):\n","    self.gate_proj = nnx.Linear(config.dim, config.intermediate_size, use_bias=False, rngs=rngs, param_dtype=jnp.bfloat16)\n","    self.up_proj = nnx.Linear(config.dim, config.intermediate_size, use_bias=False, rngs=rngs, param_dtype=jnp.bfloat16)\n","    self.down_proj = nnx.Linear(config.intermediate_size, config.dim, use_bias=False, rngs=rngs, param_dtype=jnp.bfloat16)\n","\n","  @nnx.jit()\n","  def __call__(self, x):\n","    return self.down_proj(jax.nn.silu(self.gate_proj(x)) * self.up_proj(x))\n","\n","class LlamaTransformerBlock(nnx.Module):\n","\n","  def __init__(self, layer_idx, rngs=None):\n","    self.input_layernorm = LlamaRMSNorm(dim=config.dim, rngs=rngs)\n","    self.attention = LlamaAttention(layer_idx=layer_idx, rngs=rngs)\n","    self.post_attention_layernorm = LlamaRMSNorm(dim=config.dim, rngs=rngs)\n","    self.mlp = LlamaMLP(layer_idx=layer_idx, rngs=rngs)\n","\n","  @nnx.jit()\n","  def __call__(self, x, position_ids):\n","    residual = x\n","    x = self.input_layernorm(x)\n","    x = self.attention(x, position_ids)\n","    x = residual + x\n","\n","    residual = x\n","    x = self.post_attention_layernorm(x)\n","    x = self.mlp(x)\n","    x = residual + x\n","    return x\n","\n","class LlamaForCausalLM(nnx.Module):\n","\n","  def __init__(self, rngs=None):\n","    self.token_embed = nnx.Embed(num_embeddings=config.vocab_size, features=config.dim, param_dtype=jnp.bfloat16, rngs=rngs)\n","\n","    self.layers = [LlamaTransformerBlock(layer_idx=idx, rngs=rngs) for idx in range(config.n_layers)]\n","    self.lm_head = nnx.Linear(config.dim, config.vocab_size, use_bias=False, rngs=rngs, param_dtype=jnp.bfloat16)\n","    self.norm = LlamaRMSNorm(dim=config.head_dim, rngs=rngs)\n","\n","  @nnx.jit()\n","  def __call__(self, input_ids, position_ids):\n","    assert input_ids.shape[0] == 1, \"Only batch size 1 is supported\"\n","    x = self.token_embed(input_ids)\n","    for layer in self.layers:\n","        x = layer(x, position_ids)\n","    x = self.norm(x)\n","    logits = self.lm_head(x)\n","    return logits"],"metadata":{"id":"QztvReU0T40B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = LlamaForCausalLM(rngs=nnx.Rngs(0))\n","state = nnx.state(model)\n","nnx.display(state) # This can be very useful"],"metadata":{"id":"dYzGVw1fLT3l"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Map the PyTorch weights to Flax NNX\n","\n","Because of differences in the layer definitions between PyTorch and JAX/Flax NNX we need to alter the shapes of some of the weights.  Here's a quick summary:\n","\n","* **Linear (FC)**: Transpose\n","* **Convolutions**: Transpose from `[outC, inC, kH, kW]` to `[kH, kW, inC, outC]`\n","```\n","# [outC, inC, kH, kW] -> [kH, kW, inC, outC]\n","kernel = jnp.transpose(kernel, (2, 3, 1, 0))\n","```\n","\n","* **Convolutions and FC Layers**:\n","We have to be careful, when we have a model that uses convolutions followed by fc layers (ResNet, VGG, etc). In PyTorch, the activations will have shape [N, C, H, W] after the convolutions and are then reshaped to [N, C * H * W] before being fed to the fc layers. When we port our weights from PyTorch to Flax, the activations after the convolutions will be of shape [N, H, W, C] in Flax. Before we reshape the activations for the fc layers, we have to transpose them to [N, C, H, W].\n","\n","* **BatchNorm**: No change"],"metadata":{"id":"L3Nb2DUsgg3y"}},{"cell_type":"code","source":["# This is specific to the format of a Hugging Face Llama 3.2 checkpoint\n","\n","def update_from_HF_checkpoint(state: nnx.State, weights: dict) -> None:\n","  for wholekey in weights:\n","    keys = wholekey.split('.')\n","    if keys[1] == 'layers':\n","      if keys[3] == 'self_attn':\n","        keys[3] = 'attention'\n","      if keys[1] == 'layers' and keys[3] == 'attention':\n","        state['layers'][int(keys[2])][keys[3]][keys[4]]['kernel'].value = weights[wholekey].T\n","      elif keys[1] == 'layers' and keys[3] == 'mlp':\n","        state['layers'][int(keys[2])][keys[3]][keys[4]]['kernel'].value = weights[wholekey].T\n","      elif keys[1] == 'layers' and keys[3] == 'input_layernorm':\n","        state['layers'][int(keys[2])][keys[3]]['norm_weights'].value = weights[wholekey]\n","      elif keys[1] == 'layers' and keys[3] == 'post_attention_layernorm':\n","        state['layers'][int(keys[2])][keys[3]]['norm_weights'].value = weights[wholekey]\n","    elif keys[1] == 'embed_tokens':\n","      state['token_embed'].embedding.value = weights[wholekey]\n","      state['lm_head'].kernel.value = weights[wholekey].T\n","    elif keys[1] == 'norm':\n","      state['norm'].norm_weights.value = weights[wholekey]\n","\n","update_from_HF_checkpoint(state, weights)\n","nnx.update(model, state)\n","# nnx.display(state)"],"metadata":{"id":"GJXOy1QQc5Yi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained(model_id)\n","input_text = \"The capital of Japan is\"\n","\n","input_ids = tokenizer(input_text, return_tensors=\"jax\")[\"input_ids\"]\n","position_ids = jnp.asarray([jnp.arange(input_ids.shape[1])])\n","\n","for _ in range(15):\n","  logits = model(input_ids, position_ids)\n","  next_token = jnp.argmax(logits[:, -1, :], axis=-1)\n","  input_ids = jnp.concatenate([input_ids, next_token[:, None]], axis=1)\n","  position_ids = jnp.asarray([jnp.arange(input_ids.shape[1])])\n","  print(f\"Generated token: {next_token[0]}\")\n","\n","print(tokenizer.decode(input_ids[0]))"],"metadata":{"id":"x3fPqvgfrdIT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Get the updated model for serving\n","\n","We loaded up our JAX model, and although we didn't make any changes to it in this notebook, in real life we may have done some fine-tuning, alignment, etc.  So now we need to get our updated model so that we can serve it with vLLM."],"metadata":{"id":"j7SlJncxVW7S"}},{"cell_type":"code","source":["state = nnx.state(model) # We already have it, but just to illustrate\n","\n","# This is specific to the format of a Hugging Face Llama 3.2 checkpoint\n","\n","def model_state_to_HF_weights(state: nnx.State) -> dict:\n","  global weights\n","\n","  weights_dict = {}\n","  weights_dict['model.embed_tokens.weight'] = state['token_embed'].embedding.value\n","  weights_dict['model.norm.weight'] = state['norm'].norm_weights.value\n","\n","  for idx, layer in enumerate(state['layers'].values()):\n","    weights_dict[f'model.layers.{idx}.input_layernorm.weight'] = layer['input_layernorm'].norm_weights.value\n","    weights_dict[f'model.layers.{idx}.post_attention_layernorm.weight'] = layer['post_attention_layernorm'].norm_weights.value\n","    weights_dict[f'model.layers.{idx}.self_attn.k_proj.weight'] = layer['attention']['k_proj'].kernel.value.T\n","    weights_dict[f'model.layers.{idx}.self_attn.o_proj.weight'] = layer['attention']['o_proj'].kernel.value.T\n","    weights_dict[f'model.layers.{idx}.self_attn.q_proj.weight'] = layer['attention']['q_proj'].kernel.value.T\n","    weights_dict[f'model.layers.{idx}.self_attn.v_proj.weight'] = layer['attention']['v_proj'].kernel.value.T\n","    weights_dict[f'model.layers.{idx}.mlp.down_proj.weight'] = layer['mlp']['down_proj'].kernel.value.T\n","    weights_dict[f'model.layers.{idx}.mlp.gate_proj.weight'] = layer['mlp']['gate_proj'].kernel.value.T\n","    weights_dict[f'model.layers.{idx}.mlp.up_proj.weight'] = layer['mlp']['up_proj'].kernel.value.T\n","  return weights_dict\n","\n","new_weights = model_state_to_HF_weights(state)"],"metadata":{"id":"Z2C6bWvHHulx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Now convert the new weights back to Safetensors in preparation for serving"],"metadata":{"id":"VMgtTCefWpz3"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","\n","# vLLM wants the weight dictionary flattened\n","def flatten_weight_dict(torch_params, prefix=\"\"):\n","    flat_params = {}\n","    for key, value in torch_params.items():\n","        new_key = f\"{prefix}{key}\" if prefix else key\n","        if isinstance(value, dict):\n","            flat_params.update(flatten_weight_dict(value, new_key + \".\"))\n","        else:\n","            flat_params[new_key] = value\n","    return flat_params\n","\n","servable_weights = flatten_weight_dict(new_weights)"],"metadata":{"id":"zR1ycZh10eri"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Replace the old model with the new model.  Note that we could also\n","# keep the old and save the new model to a new directory\n","from safetensors.flax import save_file\n","save_file(servable_weights, path_to_model_weights + '/model.safetensors')"],"metadata":{"id":"zLEjIsEm5Tud"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Serving with vLLM"],"metadata":{"id":"zaGqNbxag0wY"}},{"cell_type":"markdown","source":["## Runtime > Restart session to free memory\n","\n","We're right on the edge of our GPU memory for a T4 Colab instance."],"metadata":{"id":"3J9dQW0s7xjS"}},{"cell_type":"markdown","source":["# Which models can you serve with vLLM?\n","\n","While safetensors is a required format for the model's weights, vLLM has two other critical requirements that determine compatibility.\n","\n","## Model Architecture is Key\n","The most important factor is the model's architecture. vLLM achieves its high speed by using custom, highly-optimized compute kernels for specific transformer architectures (like Llama, Mixtral, Gemma, Phi-3, etc.).\n","\n","**Supported Architectures Only:** If the model's architecture is not on vLLM's list of supported models, vLLM will not know how to load or run it, regardless of the file format. However vLLM can also run custom models, see below.\n","\n","**Checking Compatibility:** You can check a model's architecture in its config.json file under the \"architectures\" or \"model_type\" field and compare it against the [official vLLM supported models list](https://docs.vllm.ai/en/latest/models/supported_models.html).\n","\n","## More Than Just Weights\n","A `.safetensors` file only contains the model's weights (the numerical parameters). To function, a model also needs its configuration and tokenizer files. When you point vLLM to a model, it expects a complete directory (or a Hugging Face repository identifier) that includes:\n","\n","* `config.json`: Defines the model's architecture, size, and other essential parameters. vLLM reads this first to check for compatibility.\n","\n","* `tokenizer.json` (and related files): Defines how to convert text into tokens that the model can understand.\n","\n","* `model.safetensors` (or sharded versions): The file(s) containing the actual model weights.\n","\n","## Can I serve a model not in the supported models list?\n","\n","Yes!  Check out the [instructions here](https://docs.vllm.ai/en/latest/models/supported_models.html#custom-models to learn how to serve custom models."],"metadata":{"id":"p4OpfaXnCd61"}},{"cell_type":"markdown","source":["## CUDA"],"metadata":{"id":"hc799D81GSVJ"}},{"cell_type":"code","source":["!nvcc --version"],"metadata":{"id":"93aHdF7FDk5-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%env CUDA_HOME=/usr/local/cuda-12.5"],"metadata":{"id":"dXlgdHqiF-UL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Install vLLM"],"metadata":{"id":"fTMbKW2HCUST"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"So9dZbyUCSo0"},"outputs":[],"source":["!pip install -q vllm"]},{"cell_type":"markdown","source":["## Serve the model with vLLM"],"metadata":{"id":"o_TlPnQ9udhj"}},{"cell_type":"code","source":["# Need to restore these after restarting the session\n","import os\n","\n","model_id = \"meta-llama/Llama-3.2-1B\"\n","path_to_model_weights = os.path.join('/content', model_id)"],"metadata":{"id":"4sK7H72b3Bl2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the model into vLLM\n","from vllm import LLM, SamplingParams\n","\n","llm = LLM(model=path_to_model_weights, load_format=\"safetensors\", dtype=\"half\")"],"metadata":{"id":"SdKRic9TCjwI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompts = [\n","    \"Hello, my name is\",\n","    \"The president of the United States is\",\n","    \"The capital of France is\",\n","    \"The future of AI is\",\n","]\n","\n","sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\n","\n","outputs = llm.generate(prompts, sampling_params)\n","for output in outputs:\n","    prompt = output.prompt\n","    generated_text = output.outputs[0].text\n","    print(\"===============================\")\n","    print(f\"Prompt: {prompt}\\nGenerated text: {generated_text}\")\n"],"metadata":{"id":"d1E_AgaUcI-C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"wpXHSzjPEk_u"},"execution_count":null,"outputs":[]}]}