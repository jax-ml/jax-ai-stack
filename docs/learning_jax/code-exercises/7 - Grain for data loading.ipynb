{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1KeLLQiLOy14q9OIPkXjv1lNxpArdqqzf","timestamp":1755113967227},{"file_id":"11x7XXCgvJj33PxSP2289nrHnOdJwoU-c","timestamp":1750353821037}],"toc_visible":true,"authorship_tag":"ABX9TyM8EkHLzJGv8jZyn07u4GN1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Efficient Data Loading with Grain: Exercises for JAX/Flax NNX\n","\n","Welcome! This Colab notebook contains exercises to help you learn Google's Grain library for efficient data loading in JAX. These exercises are designed for developers familiar with PyTorch who are now exploring the JAX ecosystem, including the new Flax NNX API.\n","\n","**Goals of this notebook:**\n","\n","* Understand the core components of Grain: DataSource, Sampler, and Operations.\n","* Learn how to use grain.DataLoader for both sequential and parallel data loading.\n","* Implement custom data transformations.\n","* Explore data sharding for distributed training scenarios.\n","* See how Grain integrates into a conceptual JAX/Flax NNX training loop.\n","* Learn about checkpointing data iterator state for reproducibility.\n","\n","**Simulated Multi-Device Environment:**\n","\n","To demonstrate parallelism and sharding concepts effectively in Colab (which typically provides a single CPU/GPU), this notebook starts by configuring JAX to simulate 8 CPU devices. This is achieved using XLA_FLAGS and chex.set_n_cpu_devices(8).\n","\n","**Let's get started! Please run the next cell to set up the environment.**"],"metadata":{"id":"QqFiEZqVxEA5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"rMAKmTMJw79f"},"outputs":[],"source":["# Environment Setup\n","# This cell configures the environment to simulate multiple CPU devices\n","# and installs necessary libraries.\n","# IMPORTANT: RUN THIS CELL FIRST. If you encounter issues with JAX device\n","# counts later, try 'Runtime -> Restart runtime' in the Colab menu\n","# and run this cell again before any others.\n","import os\n","\n","# Configure JAX to see 8 virtual CPU devices.\n","# This must be done before JAX is imported for the first time in a session.\n","os.environ[\"XLA_FLAGS\"] = \"--xla_force_host_platform_device_count=8\"\n","\n","# Install libraries\n","# We use google-grain for the Grain library.\n","!pip install -Uq grain chex flax jax jaxlib numpy\n","print(\"Libraries installed.\")\n","\n","# Now, import chex and attempt to set_n_cpu_devices.\n","# This must be called after setting XLA_FLAGS and before JAX initializes its backends.\n","import chex\n","\n","try:\n","  chex.set_n_cpu_devices(8)\n","  print(\"chex.set_n_cpu_devices(8) called successfully.\")\n","except RuntimeError as e:\n","  print(f\"Note on chex.set_n_cpu_devices: {e}\")\n","  print(\"This usually means JAX was already initialized. The XLA_FLAGS environment variable should still apply.\")\n","  print(\"If you see issues with device counts, ensure you 'Restart runtime' and run this cell first.\")\n","\n","# Verify JAX device count\n","import jax\n","print(f\"JAX version: {jax.__version__}\")\n","print(f\"JAX found {jax.device_count()} devices.\")\n","print(f\"JAX devices: {jax.devices()}\")\n","\n","if jax.device_count() != 8:\n","  print(\"\\nWARNING: JAX does not see 8 devices. Parallelism/sharding exercises might not behave as expected.\")\n","  print(\"Please try 'Runtime -> Restart runtime' and run this setup cell again first.\")\n","\n","# Common imports for the exercises\n","import jax.numpy as jnp\n","import numpy as np\n","import grain.python as grain # Main Grain API\n","from flax import nnx # Flax NNX API\n","import time # For simulating work and observing performance\n","import copy # For checkpointing example\n","from typing import Dict, Any, List # For type hints\n","import dataclasses # For ShardOptions if needed manually\n","import functools # For functools.partial\n","print(\"Imports complete. Setup finished.\")"]},{"cell_type":"markdown","source":["## Introduction to Grain\n","\n","As highlighted in the lecture, JAX is incredibly fast for numerical computation, especially on accelerators. However, this speed can be bottlenecked by inefficient data loading. Standard Python data loading can struggle due to I/O limitations, CPU-bound transformations, and the Global Interpreter Lock (GIL).\n","\n","**Grain** is Google's solution for high-performance data loading in JAX. Its key goals are:\n","*   **Speed:** Achieved through multiprocessing, shared memory, and prefetching.\n","*   **Determinism:** Ensuring reproducibility in experiments.\n","*   **Flexibility & Simplicity:** Declarative pipeline definition.\n","*   **JAX Ecosystem Focus:** Integrates with concepts like distributed sharding.\n","\n","Conceptually, Grain's `DataLoader` is analogous to PyTorch's `torch.utils.data.DataLoader`. It orchestrates data reading, transformation, batching, and parallelization.\n","\n","**Core Components of `grain.DataLoader` API:**\n","1.  **`DataSource`**: Provides access to individual raw data records (must implement `__len__` and `__getitem__`).\n","2.  **`Sampler`**: Determines the order in which records are loaded and provides seeds for random operations, ensuring reproducibility.\n","3.  **`Operations`**: A list of transformations (e.g., augmentation, filtering, batching) applied sequentially to the records.\n","\n","Let's dive into the exercises!"],"metadata":{"id":"XMV045L50R6_"}},{"cell_type":"markdown","source":["---\n","## Exercise 1: Building Your First `grain.DataLoader` (Sequential)\n","\n","**Goal:** Get familiar with the basic components: `DataSource`, `IndexSampler`, a simple `MapTransform`, and `grain.DataLoader` running in sequential mode (`worker_count=0`).\n","\n","**Instructions:**\n","1.  Define `MySource`, a custom `RandomAccessDataSource`.\n","    *   `__init__`: Store `num_records`.\n","    *   `__len__`: Return `num_records`.\n","    *   `__getitem__`: Given an `idx`, return a dictionary `{'image': image_array, 'label': label_int}`.\n","        *   The `image_array` should be a NumPy array of shape `(32, 32, 3)` with `dtype=np.uint8`. Its values can depend on `idx` (e.g., `np.ones(...) * (idx % 255)`).\n","        *   The `label_int` should be an integer (e.g., `idx % 10`).\n","        *   Handle potential index wrap-around for multiple epochs: `idx = idx % self._num_records`.\n","2.  Instantiate `MySource`.\n","3.  Create an `IndexSampler` for shuffling, running for 1 epoch, with a fixed seed.\n","4.  Define a list of `operations`:\n","    *   A `ConvertToFloat` class inheriting from `grain.MapTransform` that converts the 'image' to `np.float32` and normalizes it to `[0, 1]`.\n","    *   A `grain.Batch` operation to batch 64 items, dropping any remainder.\n","5.  Instantiate `grain.DataLoader` with `worker_count=0` (for debugging/sequential mode).\n","    *   Since `MySource` is in-memory, use `read_options=grain.ReadOptions(num_threads=0)` to disable Grain's internal read threads.\n","6.  Iterate through the `DataLoader` to get the first batch and print its shape and the shape of its labels."],"metadata":{"id":"0-M_gW8d0a0u"}},{"cell_type":"code","source":["# @title Exercise 1: Student Code\n","# 1. Define MySource\n","class MySource(grain.RandomAccessDataSource):\n","  def __init__(self, num_records: int = 1000):\n","    self._num_records = num_records\n","  def __len__(self) -> int:\n","      # TODO: Return the total number of records\n","      # YOUR CODE HERE\n","      return 0 # Replace this\n","\n","  def __getitem__(self, idx: int) -> Dict[str, Any]:\n","      # TODO: Handle potential index wrap-around for multiple epochs\n","      # effective_idx = ...\n","      # YOUR CODE HERE\n","      effective_idx = idx # Replace this\n","\n","      # TODO: Simulate loading data: an image and a label\n","      # image = np.ones(...) * (effective_idx % 255)\n","      # label = effective_idx % 10\n","      # YOUR CODE HERE\n","      image = np.zeros((32,32,3), dtype=np.uint8) # Replace this\n","      label = 0 # Replace this\n","      return {'image': image, 'label': label}\n","\n","# 2. Instantiate MySource\n","# TODO: Create an instance of MySource\n","# source = ...\n","# YOUR CODE HERE\n","source = None # Replace this\n","\n","# 3. Create an IndexSampler\n","# TODO: Create an IndexSampler that shuffles, runs for 1 epoch, and uses seed 42.\n","# num_records should be len(source).\n","# index_sampler = grain.IndexSampler(...)\n","# YOUR CODE HERE\n","index_sampler = None # Replace this\n","\n","# 4. Define Operations\n","# TODO: Define ConvertToFloat transform\n","class ConvertToFloat(grain.MapTransform):\n","  def map(self, features: Dict[str, Any]) -> Dict[str, Any]:\n","    # TODO: Convert 'image' to float32 and normalize to [0, 1].\n","    # Keep 'label' as is.\n","    # YOUR CODE HERE\n","    image = features['image'] # Replace this\n","    return {'image': image.astype(np.float32) / 255.0, 'label': features['label']}\n","\n","# TODO: Create a list of transformations: ConvertToFloat instance, then grain.Batch\n","batch_size = 64, drop_remainder = True\n","# transformations = [...]\n","# YOUR CODE HERE\n","transformations = [] # Replace this\n","\n","# 5. Instantiate DataLoader\n","# TODO: Create a DataLoader with worker_count=0 and appropriate read_options\n","# data_loader_sequential = grain.DataLoader(...)\n","# YOUR CODE HERE\n","data_loader_sequential = None # Replace this\n","\n","# 6. Iterate and print batch info\n","if data_loader_sequential:\n","  print(\"DataLoader configured sequentially.\")\n","  data_iterator_seq = iter(data_loader_sequential)\n","  try:\n","    first_batch_seq = next(data_iterator_seq)\n","    print(f\"Sequential - First batch image shape: {first_batch_seq['image'].shape}\")\n","    print(f\"Sequential - First batch label shape: {first_batch_seq['label'].shape}\")\n","    # Example: Check a value from the first image of the first batch\n","    print(f\"Sequential - Example image value (first item, [0,0,0]): {first_batch_seq['image'][0, 0, 0, 0]}\")\n","    print(f\"Sequential - Example label value (first item): {first_batch_seq['label'][0]}\")\n","  except StopIteration:\n","    print(\"Sequential DataLoader is empty or exhausted.\")\n","else:\n","  print(\"Sequential DataLoader not configured yet.\")"],"metadata":{"id":"AgTRPoyOzi5E","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Exercise 1: Solution\n","# 1. Define MySource\n","class MySource(grain.RandomAccessDataSource):\n","  def __init__(self, num_records: int = 1000):\n","    self._num_records = num_records\n","\n","  def __len__(self) -> int:\n","    return self._num_records\n","\n","  def __getitem__(self, idx: int) -> Dict[str, Any]:\n","    effective_idx = idx % self._num_records # Handle wrap-around\n","    image = np.ones((32, 32, 3), dtype=np.uint8) * (effective_idx % 255)\n","    label = effective_idx % 10\n","    return {'image': image, 'label': label}\n","\n","# 2. Instantiate MySource\n","source = MySource(num_records=1000)\n","print(f\"DataSource created with {len(source)} records.\")\n","\n","# 3. Create an IndexSampler\n","index_sampler = grain.IndexSampler(\n","    num_records=len(source),\n","    shard_options=grain.NoSharding(), # No sharding for this exercise\n","    shuffle=True,\n","    num_epochs=1, # Run for 1 epoch\n","    seed=42\n","    )\n","print(\"IndexSampler created.\")\n","\n","# 4. Define Operations\n","class ConvertToFloat(grain.MapTransform):\n","  def map(self, features: Dict[str, Any]) -> Dict[str, Any]:\n","    # Convert 'image' to float32 and normalize to [0, 1].\n","    # Keep 'label' as is.\n","    image = features['image'].astype(np.float32) / 255.0\n","    return {'image': image, 'label': features['label']}\n","\n","transformations = [\n","    ConvertToFloat(),\n","    grain.Batch(batch_size=64, drop_remainder=True)\n","    ]\n","print(\"Transformations defined.\")\n","\n","# 5. Instantiate DataLoader\n","data_loader_sequential = grain.DataLoader(\n","    data_source=source,\n","    operations=transformations,\n","    sampler=index_sampler,\n","    worker_count=0, # Sequential mode\n","    shard_options=grain.NoSharding(), # Explicitly no sharding for this loader instance\n","    read_options=grain.ReadOptions(num_threads=0) # Dataset is in-memory\n",")\n","\n","# 6. Iterate and print batch info\n","if data_loader_sequential:\n","  print(\"DataLoader configured sequentially.\")\n","  data_iterator_seq = iter(data_loader_sequential)\n","  try:\n","    first_batch_seq = next(data_iterator_seq)\n","    print(f\"Sequential - First batch image shape: {first_batch_seq['image'].shape}\") # Expected: (64, 32, 32, 3)\n","    print(f\"Sequential - First batch label shape: {first_batch_seq['label'].shape}\") # Expected: (64,)\n","    # Example: Check a value from the first image of the first batch\n","    print(f\"Sequential - Example image value (first item, [0,0,0]): {first_batch_seq['image'][0, 0, 0, 0]}\")\n","    print(f\"Sequential - Example label value (first item): {first_batch_seq['label'][0]}\")\n","  except StopIteration:\n","    print(\"Sequential DataLoader is empty or exhausted.\")\n","else:\n","  print(\"Sequential DataLoader not configured yet.\")"],"metadata":{"id":"1yqwAkGm2ihu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","## Exercise 2: Enabling Parallelism with `worker_count`\n","\n","**Goal:** Understand how `worker_count > 0` enables multiprocessing for faster data loading.\n","\n","**Instructions:**\n","1.  Reuse `MySource`, `IndexSampler` (or create a new one if you prefer, e.g., for indefinite epochs: `num_epochs=None`), and `transformations` from Exercise 1.\n","2.  To better observe the potential benefits of parallelism, let's modify `MySource` slightly. Add a small `time.sleep(0.01)` (10 milliseconds) inside `__getitem__` to simulate some I/O or CPU work for each item.\n","3.  Instantiate a new `grain.DataLoader` (e.g., `data_loader_parallel`). This time, set `worker_count` to a value greater than 0 (e.g., 2 or 4). Remember our environment is faking 8 CPUs.\n","4.  Iterate to get the first batch and print its shape info.\n","5.  (Optional) Time how long it takes to get, for example, 10 batches from the sequential loader vs. the parallel loader. You should see a speed-up with the parallel loader, especially with the added `time.sleep`.\n","\n","**A note on pickling:** When `worker_count > 0`, Grain uses multiprocessing. This means all components (DataSource, Sampler, Operations, and custom transform instances) must be picklable by Python's `pickle` module. Simple classes and functions are usually fine, but avoid complex closures or unpicklable objects in your transform logic."],"metadata":{"id":"PbUAwDXf3_K4"}},{"cell_type":"code","source":["# @title Exercise 2: Student Code\n","# 1. Reuse/Recreate components (DataSource with simulated work, Sampler, Operations)\n","# TODO: Define MySourceWithWork, adding time.sleep(0.01) in getitem\n","class MySourceWithWork(grain.RandomAccessDataSource):\n","  def __init__(self, num_records: int = 1000):\n","    self._num_records = num_records\n","\n","  def __len__(self) -> int:\n","    # YOUR CODE HERE\n","    return self._num_records\n","\n","  def __getitem__(self, idx: int) -> Dict[str, Any]:\n","    effective_idx = idx % self._num_records\n","    # TODO: Add time.sleep(0.01) to simulate work\n","    # YOUR CODE HERE\n","\n","    image = np.ones((32, 32, 3), dtype=np.uint8) * (effective_idx % 255)\n","    label = effective_idx % 10\n","    return {'image': image, 'label': label}\n","\n","# TODO: Instantiate MySourceWithWork\n","# source_with_work = ...\n","# YOUR CODE HERE\n","source_with_work = None # Replace this\n","\n","# TODO: Create a new IndexSampler (e.g., for indefinite epochs, num_epochs=None)\n","# Or reuse the one from Ex1 if you reset it or it's for multiple epochs.\n","# For simplicity, let's create one for indefinite epochs.\n","# parallel_sampler = grain.IndexSampler(...)\n","# YOUR CODE HERE\n","parallel_sampler = None # Replace this\n","\n","# Transformations can be reused from Exercise 1\n","# transformations = [ConvertToFloat(), grain.Batch(batch_size=64, drop_remainder=True)]\n","# (Assuming ConvertToFloat is defined from Ex1 solution)\n","\n","# 2. Instantiate DataLoader with worker_count > 0\n","# TODO: Set num_workers (e.g., 4)\n","# num_workers = ...\n","# YOUR CODE HERE\n","num_workers = 0 # Replace this\n","\n","# TODO: Create data_loader_parallel\n","# data_loader_parallel = grain.DataLoader(...)\n","# YOUR CODE HERE\n","data_loader_parallel = None # Replace this\n","\n","# 3. Iterate and print batch info\n","if data_loader_parallel:\n","  print(f\"DataLoader configured with worker_count={num_workers}.\")\n","  data_iterator_parallel = iter(data_loader_parallel)\n","  try:\n","    first_batch_parallel = next(data_iterator_parallel)\n","    print(f\"Parallel - First batch image shape: {first_batch_parallel['image'].shape}\")\n","    print(f\"Parallel - First batch label shape: {first_batch_parallel['label'].shape}\")\n","  except StopIteration:\n","    print(\"Parallel DataLoader is empty or exhausted.\")\n","else:\n","  print(\"Parallel DataLoader not configured yet.\")\n","\n","# 4. (Optional) Timing comparison\n","# Re-create sequential loader with MySourceWithWork for a fair comparison\n","if source_with_work and transformations and index_sampler: # index_sampler from Ex1\n","  data_loader_seq_with_work = grain.DataLoader(\n","      data_source=source_with_work,\n","      operations=transformations, # Reusing from Ex1\n","      sampler=index_sampler, # Reusing from Ex1 (ensure it's fresh or allows re-iteration)\n","      worker_count=0,\n","      shard_options=grain.NoSharding(),\n","      read_options=grain.ReadOptions(num_threads=0)\n","      )\n","  num_batches_to_test = 5 # Small number for quick test\n","\n","if data_loader_seq_with_work:\n","    print(f\"\\nTiming test for {num_batches_to_test} batches:\")\n","    # Sequential\n","    iterator_seq = iter(data_loader_seq_with_work)\n","    start_time = time.time()\n","    try:\n","      for i in range(num_batches_to_test):\n","        batch = next(iterator_seq)\n","        if i == 0: print(f\"  Seq batch 1 label sum: {batch['label'].sum()}\") # to ensure work is done\n","    except StopIteration:\n","      print(\"Sequential loader exhausted early.\")\n","    end_time = time.time()\n","    print(f\"Sequential ({num_batches_to_test} batches) took: {end_time - start_time:.4f} seconds\")\n","\n","if data_loader_parallel:\n","    # Parallel\n","    # Ensure sampler is fresh for parallel loader if it was used above\n","    # For this optional part, let's use a fresh sampler for the parallel loader\n","    # to avoid StopIteration if the previous sampler was single-epoch and exhausted.\n","    fresh_parallel_sampler = grain.IndexSampler(\n","        num_records=len(source_with_work),\n","        shard_options=grain.NoSharding(),\n","        shuffle=True,\n","        num_epochs=None, # Indefinite\n","        seed=43 # Different seed or same, for this test it's about speed\n","    )\n","    data_loader_parallel_for_timing = grain.DataLoader(\n","        data_source=source_with_work,\n","        operations=transformations, # Reusing from Ex1\n","        sampler=fresh_parallel_sampler,\n","        worker_count=num_workers if num_workers > 0 else 2, # Ensure parallelism\n","        shard_options=grain.NoSharding(),\n","        read_options=grain.ReadOptions(num_threads=0)\n","    )\n","    iterator_parallel = iter(data_loader_parallel_for_timing)\n","    start_time = time.time()\n","    try:\n","      for i in range(num_batches_to_test):\n","        batch = next(iterator_parallel)\n","        if i == 0: print(f\"  Parallel batch 1 label sum: {batch['label'].sum()}\") # to ensure work is done\n","    except StopIteration:\n","      print(\"Parallel loader exhausted early.\")\n","    end_time = time.time()\n","    print(f\"Parallel ({num_batches_to_test} batches, {num_workers if num_workers > 0 else 2} workers) took: {end_time - start_time:.4f} seconds\")\n","else:\n","  print(\"Skipping optional timing: source_with_work, transformations, or index_sampler not defined.\")"],"metadata":{"id":"kJ9xr9N43tcH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Exercise 2: Solution\n","# 1. Reuse/Recreate components\n","# Define MySourceWithWork, adding time.sleep(0.01) in getitem\n","class MySourceWithWork(grain.RandomAccessDataSource):\n","  def __init__(self, num_records: int = 1000):\n","    self._num_records = num_records\n","\n","  def __len__(self) -> int:\n","    return self._num_records\n","\n","  def __getitem__(self, idx: int) -> Dict[str, Any]:\n","    effective_idx = idx % self._num_records\n","    time.sleep(0.01) # Simulate 10ms of work per item\n","    image = np.ones((32, 32, 3), dtype=np.uint8) * (effective_idx % 255)\n","    label = effective_idx % 10\n","    return {'image': image, 'label': label}\n","\n","source_with_work = MySourceWithWork(num_records=1000)\n","print(f\"MySourceWithWork created with {len(source_with_work)} records.\")\n","\n","# Sampler for parallel loading (indefinite epochs for robust testing)\n","parallel_sampler = grain.IndexSampler(\n","    num_records=len(source_with_work),\n","    shard_options=grain.NoSharding(),\n","    shuffle=True,\n","    num_epochs=None, # Run indefinitely\n","    seed=42\n","    )\n","print(\"Parallel IndexSampler created.\")\n","\n","# Transformations can be reused from Exercise 1 solution\n","# Ensure ConvertToFloat is defined (it was in Ex1 solution cell)\n","if 'ConvertToFloat' not in globals(): # Basic check\n","  class ConvertToFloat(grain.MapTransform): # Redefine if not in current scope\n","    def map(self, features: Dict[str, Any]) -> Dict[str, Any]:\n","      image = features['image'].astype(np.float32) / 255.0\n","      return {'image': image, 'label': features['label']}\n","\n","    print(\"Redefined ConvertToFloat for safety.\")\n","\n","transformations_ex2 = [\n","    ConvertToFloat(),\n","    grain.Batch(batch_size=64, drop_remainder=True)\n","    ]\n","print(\"Transformations for Ex2 ready.\")\n","\n","# 2. Instantiate DataLoader with worker_count > 0\n","num_workers = 4 # Use 4 workers; JAX is configured for 8 virtual CPUs\n","# Max useful workers often related to num CPU cores available.\n","data_loader_parallel = grain.DataLoader(\n","    data_source=source_with_work,\n","    operations=transformations_ex2,\n","    sampler=parallel_sampler,\n","    worker_count=num_workers,\n","    shard_options=grain.NoSharding(),\n","    read_options=grain.ReadOptions(num_threads=0) # Data source simulates work but is \"in-memory\"\n","    )\n","\n","# 3. Iterate and print batch info\n","if data_loader_parallel:\n","  print(f\"DataLoader configured with worker_count={num_workers}.\")\n","  data_iterator_parallel = iter(data_loader_parallel)\n","  try:\n","    first_batch_parallel = next(data_iterator_parallel)\n","    print(f\"Parallel - First batch image shape: {first_batch_parallel['image'].shape}\")\n","    print(f\"Parallel - First batch label shape: {first_batch_parallel['label'].shape}\")\n","  except StopIteration:\n","    print(\"Parallel DataLoader is empty or exhausted.\")\n","else:\n","  print(\"Parallel DataLoader not configured yet.\")\n","\n","# 4. (Optional) Timing comparison\n","# Create a fresh IndexSampler for the sequential loader for a fair comparison start\n","# (num_epochs=1 to match typical test for sequential pass)\n","seq_sampler_for_timing = grain.IndexSampler(\n","    num_records=len(source_with_work),\n","    shard_options=grain.NoSharding(),\n","    shuffle=True,\n","    num_epochs=1, # Single epoch for this timing test\n","    seed=42\n","    )\n","\n","data_loader_seq_with_work = grain.DataLoader(\n","    data_source=source_with_work,\n","    operations=transformations_ex2,\n","    sampler=seq_sampler_for_timing,\n","    worker_count=0,\n","    shard_options=grain.NoSharding(),\n","    read_options=grain.ReadOptions(num_threads=0)\n","    )\n","num_batches_to_test = 5 # Number of batches to fetch for timing\n","print(f\"\\nTiming test for {num_batches_to_test} batches (each item has 0.01s simulated work):\")\n","\n","# Sequential\n","iterator_seq = iter(data_loader_seq_with_work)\n","start_time_seq = time.time()\n","try:\n","  for i in range(num_batches_to_test):\n","    batch_seq = next(iterator_seq)\n","    if i == 0 and num_batches_to_test > 0 : print(f\" Seq batch 1 label sum: {batch_seq['label'].sum()}\") # to ensure work is done\n","except StopIteration:\n","  print(f\"Sequential loader exhausted before {num_batches_to_test} batches.\")\n","end_time_seq = time.time()\n","print(f\"Sequential ({num_batches_to_test} batches) took: {end_time_seq - start_time_seq:.4f} seconds\")\n","\n","# Parallel\n","# Use a fresh sampler for the parallel loader for timing to ensure it's not exhausted\n","# and runs for enough batches.\n","parallel_sampler_for_timing = grain.IndexSampler(\n","    num_records=len(source_with_work),\n","    shard_options=grain.NoSharding(),\n","    shuffle=True,\n","    num_epochs=None, # Indefinite, or ensure enough for num_batches_to_test\n","    seed=43 # Can be same or different seed\n","    )\n","\n","data_loader_parallel_for_timing = grain.DataLoader(\n","    data_source=source_with_work,\n","    operations=transformations_ex2,\n","    sampler=parallel_sampler_for_timing,\n","    worker_count=num_workers,\n","    shard_options=grain.NoSharding(),\n","    read_options=grain.ReadOptions(num_threads=0)\n","    )\n","\n","iterator_parallel_timed = iter(data_loader_parallel_for_timing)\n","start_time_parallel = time.time()\n","try:\n","  for i in range(num_batches_to_test):\n","    batch_par = next(iterator_parallel_timed)\n","    if i == 0 and num_batches_to_test > 0 : print(f\" Parallel batch 1 label sum: {batch_par['label'].sum()}\") # to ensure work is done\n","except StopIteration:\n","  print(f\"Parallel loader exhausted before {num_batches_to_test} batches.\")\n","end_time_parallel = time.time()\n","print(f\"Parallel ({num_batches_to_test} batches, {num_workers} workers) took: {end_time_parallel - start_time_parallel:.4f} seconds\")\n","\n","if end_time_parallel - start_time_parallel < end_time_seq - start_time_seq:\n","  print(\"Parallel loading was faster, as expected!\")\n","else:\n","  print(\"Parallel loading was not significantly faster. This might happen for very small num_batches_to_test due to overhead, or if simulated work is too little.\")"],"metadata":{"id":"WvQYsi5C5e7_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","---\n","## Exercise 3: Custom Deterministic Transformations (`MapTransform`)\n","\n","**Goal:** Implement a custom data transformation that behaves deterministically.\n","\n","**Instructions:**\n","1.  Define a custom class `OneHotEncodeLabel` that inherits from `grain.MapTransform`.\n","    *   Its `__init__` method should take `num_classes`.\n","    *   Its `map(self, features: Dict[str, Any])` method should:\n","        *   Take the input `features` dictionary.\n","        *   Convert the `features['label']` (an integer) into a one-hot encoded NumPy array of type `np.float32`. The length of this array should be `num_classes`.\n","        *   Update `features['label']` with this new one-hot array.\n","        *   Return the modified `features` dictionary.\n","2.  Reuse `MySource` (the one without `time.sleep`) and `IndexSampler` from Exercise 1 (or create new ones).\n","3.  Create a new list of `operations` that includes:\n","    *   An instance of your `OneHotEncodeLabel` (e.g., with `num_classes=10`, matching `idx % 10` from `MySource`).\n","    *   The `ConvertToFloat` transform (if not already applied to image).\n","    *   `grain.Batch`.\n","4.  Instantiate a `grain.DataLoader` (you can use `worker_count=0` or `>0`).\n","5.  Iterate to get the first batch and print the shape of the one-hot encoded labels and an example label vector."],"metadata":{"id":"ELtzCFF7-a-E"}},{"cell_type":"code","source":["# @title Exercise 3: Student Code\n","# 1. Define OneHotEncodeLabel\n","class OneHotEncodeLabel(grain.MapTransform):\n","  def __init__(self, num_classes: int):\n","    # TODO: Store num_classes\n","    # YOUR CODE HERE\n","    self._num_classes = 0 # Replace this\n","\n","  def map(self, features: Dict[str, Any]) -> Dict[str, Any]:\n","    label = features['label']\n","    # TODO: Create one-hot encoded version of the label\n","    # one_hot_label = np.zeros(...)\n","    # one_hot_label[label] = 1.0\n","    # YOUR CODE HERE\n","    one_hot_label = np.array([label]) # Replace this\n","\n","    features['label'] = one_hot_label\n","    return features\n","\n","# 2. Reuse/Create DataSource and Sampler\n","# TODO: Instantiate MySource (from Ex1, no sleep)\n","# source_ex3 = ...\n","# YOUR CODE HERE\n","source_ex3 = None # Replace this\n","\n","# TODO: Instantiate an IndexSampler (e.g., from Ex1, or a new one)\n","# sampler_ex3 = grain.IndexSampler(...)\n","# YOUR CODE HERE\n","sampler_ex3 = None # Replace this\n","\n","# 3. Create new list of operations\n","# TODO: Instantiate OneHotEncodeLabel\n","num_classes_for_ohe = 10\n","one_hot_encoder = OneHotEncodeLabel(num_classes=num_classes_for_ohe)\n","# YOUR CODE HERE\n","one_hot_encoder = None # Replace this\n","\n","# TODO: Define transformations_ex3 list including one_hot_encoder,\n","# ConvertToFloat (if not already applied), and grain.Batch\n","# (Assuming ConvertToFloat is defined from Ex1 solution)\n","# transformations_ex3 = [...]\n","# YOUR CODE HERE\n","transformations_ex3 = [] # Replace this\n","\n","# 4. Instantiate DataLoader\n","# TODO: Create data_loader_ex3\n","# data_loader_ex3 = grain.DataLoader(...)\n","# YOUR CODE HERE\n","data_loader_ex3 = None # Replace this\n","\n","# 5. Iterate and print batch info\n","if data_loader_ex3:\n","  print(\"DataLoader with OneHotEncodeLabel configured.\")\n","  iterator_ex3 = iter(data_loader_ex3)\n","  try:\n","    first_batch_ex3 = next(iterator_ex3)\n","    print(f\"Custom MapTransform - Batch image shape: {first_batch_ex3['image'].shape}\")\n","    print(f\"Custom MapTransform - Batch label shape: {first_batch_ex3['label'].shape}\") # Expected: (batch_size, num_classes)\n","    if first_batch_ex3['label'].size > 0:\n","      print(f\"Custom MapTransform - Example one-hot label: {first_batch_ex3['label'][0]}\")\n","  except StopIteration:\n","    print(\"DataLoader for Ex3 is empty or exhausted.\")\n","else:\n","  print(\"DataLoader for Ex3 not configured yet.\")"],"metadata":{"id":"POurO6hp7-Mo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Exercise 3: Solution\n","# 1. Define OneHotEncodeLabel\n","class OneHotEncodeLabel(grain.MapTransform):\n","  def __init__(self, num_classes: int):\n","    self._num_classes = num_classes\n","\n","  def map(self, features: Dict[str, Any]) -> Dict[str, Any]:\n","    label_scalar = features['label']\n","    one_hot_label = np.zeros(self._num_classes, dtype=np.float32)\n","    one_hot_label[label_scalar] = 1.0\n","\n","    # Create a new dictionary to avoid modifying the input dict in place if it's reused\n","    # by other transforms or parts of the pipeline, though often direct modification is fine.\n","    # For safety and clarity, let's return a new dict or an updated copy.\n","    updated_features = features.copy()\n","    updated_features['label'] = one_hot_label\n","    return updated_features\n","\n","# 2. Reuse/Create DataSource and Sampler\n","# Using MySource from Exercise 1 solution (no artificial sleep)\n","if 'MySource' not in globals(): # Basic check\n","  class MySource(grain.RandomAccessDataSource): # Redefine if not in current scope\n","    def __init__(self, num_records: int = 1000):\n","      self._num_records = num_records\n","    def __len__(self) -> int:\n","      return self._num_records\n","\n","    def __getitem__(self, idx: int) -> Dict[str, Any]:\n","      effective_idx = idx % self._num_records\n","      image = np.ones((32, 32, 3), dtype=np.uint8) * (effective_idx % 255)\n","      label = effective_idx % 10\n","      return {'image': image, 'label': label}\n","  print(\"Redefined MySource for Ex3.\")\n","\n","source_ex3 = MySource(num_records=1000)\n","sampler_ex3 = grain.IndexSampler(\n","  num_records=len(source_ex3),\n","  shard_options=grain.NoSharding(),\n","  shuffle=True,\n","  num_epochs=1,\n","  seed=42\n","  )\n","print(\"DataSource and Sampler for Ex3 ready.\")\n","\n","# 3. Create new list of operations\n","num_classes_for_ohe = 10 # Matches idx % 10 in MySource\n","one_hot_encoder = OneHotEncodeLabel(num_classes=num_classes_for_ohe)\n","\n","# Ensure ConvertToFloat is defined\n","if 'ConvertToFloat' not in globals():\n","  class ConvertToFloat(grain.MapTransform):\n","    def map(self, features: Dict[str, Any]) -> Dict[str, Any]:\n","      image = features['image'].astype(np.float32) / 255.0\n","      return {'image': image, 'label': features['label']} # Pass label through\n","print(\"Redefined ConvertToFloat for Ex3.\")\n","\n","transformations_ex3 = [\n","  ConvertToFloat(), # Apply first to have float images\n","  one_hot_encoder, # Then one-hot encode labels\n","  grain.Batch(batch_size=64, drop_remainder=True)\n","  ]\n","print(\"Transformations for Ex3 defined.\")\n","\n","# 4. Instantiate DataLoader\n","data_loader_ex3 = grain.DataLoader(\n","  data_source=source_ex3,\n","  operations=transformations_ex3,\n","  sampler=sampler_ex3,\n","  worker_count=0, # Can be > 0 as well, OneHotEncodeLabel is picklable\n","  shard_options=grain.NoSharding(),\n","  read_options=grain.ReadOptions(num_threads=0)\n","  )\n","\n","# 5. Iterate and print batch info\n","if data_loader_ex3:\n","  print(\"DataLoader with OneHotEncodeLabel configured.\")\n","  iterator_ex3 = iter(data_loader_ex3)\n","  try:\n","    first_batch_ex3 = next(iterator_ex3)\n","    print(f\"Custom MapTransform - Batch image shape: {first_batch_ex3['image'].shape}\")\n","    print(f\"Custom MapTransform - Batch label shape: {first_batch_ex3['label'].shape}\") # Expected: (64, 10)\n","    if first_batch_ex3['label'].size > 0:\n","      print(f\"Custom MapTransform - Example one-hot label (first item): {first_batch_ex3['label'][0]}\")\n","    original_label_example = np.argmax(first_batch_ex3['label'][0])\n","    print(f\"Custom MapTransform - Decoded original label (first item): {original_label_example}\")\n","  except StopIteration:\n","    print(\"DataLoader for Ex3 is empty or exhausted.\")\n","else:\n","  print(\"DataLoader for Ex3 not configured yet.\")"],"metadata":{"id":"asNGKCuy_TMz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","---\n","## Exercise 4: Custom Randomized Transformations (`RandomMapTransform`)\n","\n","**Goal:** Implement a custom transformation that involves randomness while ensuring reproducibility using Grain's mechanisms.\n","\n","**Instructions:**\n","1.  Define a custom class `RandomBrightnessAdjust` that inherits from `grain.RandomMapTransform`.\n","    *   Its `random_map(self, features: Dict[str, Any], rng: np.random.Generator) -> Dict[str, Any]` method should:\n","        *   Take `features` and an `rng` (NumPy random number generator).\n","        *   **Crucially, use the provided `rng` for all random operations.** This ensures that the same record, when processed with the same initial seed for the sampler, gets the same \"random\" augmentation.\n","        *   Generate a random brightness factor using `rng.uniform(0.7, 1.3)`.\n","        *   Multiply the `features['image']` (assuming it's already float and normalized) by this factor.\n","        *   Clip the image values to stay within `[0.0, 1.0]` using `np.clip()`.\n","        *   Return the modified `features`.\n","2.  Reuse `MySource`, `IndexSampler` (ensure it has a `seed`), and `ConvertToFloat` from previous exercises.\n","3.  Create a list of `operations` including `ConvertToFloat`, your `RandomBrightnessAdjust`, and `grain.Batch`.\n","4.  Instantiate two `DataLoader` instances (`dl_run1`, `dl_run2`) with the **exact same configuration** (same source, sampler instance or sampler with same seed, operations, worker_count).\n","5.  Iterate and get the first batch from `dl_run1`. Print a sample pixel value.\n","6.  Reset the iterator or re-create the sampler if necessary (if `num_epochs=1`). Then, get the first batch from `dl_run2`. Print the same sample pixel value.\n","7.  **Verify:** The pixel values should be identical, demonstrating reproducible random augmentation.\n","8.  (Optional) Change the seed in the `IndexSampler` for `dl_run2` and observe that the pixel values now differ."],"metadata":{"id":"Lx0nsNlQBWDx"}},{"cell_type":"code","source":["# @title Exercise 4: Student Code\n","# 1. Define RandomBrightnessAdjust\n","class RandomBrightnessAdjust(grain.RandomMapTransform):\n","  def random_map(self, features: Dict[str, Any], rng: np.random.Generator) -> Dict[str, Any]:\n","    # TODO: Ensure image is float (e.g. by placing ConvertToFloat before this in ops)\n","    image = features['image']\n","\n","    # TODO: Generate a random brightness factor using the provided rng\n","    # brightness_factor = rng.uniform(...)\n","    # YOUR CODE HERE\n","    brightness_factor = 1.0 # Replace this\n","\n","    # TODO: Apply brightness adjustment and clip\n","    # adjusted_image = np.clip(...)\n","    # YOUR CODE HERE\n","    adjusted_image = image # Replace this\n","\n","    # Create a new dictionary or update a copy\n","    updated_features = features.copy()\n","    updated_features['image'] = adjusted_image\n","    return updated_features\n","\n","# 2. Reuse/Create DataSource, Sampler, ConvertToFloat\n","# TODO: Instantiate MySource (from Ex1)\n","# source_ex4 = ...\n","# YOUR CODE HERE\n","source_ex4 = None # Replace this\n","\n","# TODO: Instantiate an IndexSampler with a seed (e.g., seed=42, num_epochs=1 or None)\n","# sampler_ex4_seed42 = grain.IndexSampler(...)\n","# YOUR CODE HERE\n","sampler_ex4_seed42 = None # Replace this\n","\n","# (Assuming ConvertToFloat is defined from Ex1 solution)\n","\n","# 3. Create list of operations\n","# TODO: Instantiate RandomBrightnessAdjust\n","# random_brightness_adjuster = ...\n","# YOUR CODE HERE\n","random_brightness_adjuster = None # Replace this\n","\n","# TODO: Define transformations_ex4 list: ConvertToFloat, random_brightness_adjuster, grain.Batch\n","# transformations_ex4 = [...]\n","# YOUR CODE HERE\n","transformations_ex4 = [] # Replace this\n","\n","# 4. Instantiate two DataLoaders with the same config\n","# TODO: Create dl_run1\n","# dl_run1 = grain.DataLoader(...)\n","# YOUR CODE HERE\n","dl_run1 = None # Replace this\n","\n","# TODO: Create dl_run2 (using the exact same sampler instance or a new one with the same seed)\n","# dl_run2 = grain.DataLoader(...)\n","# YOUR CODE HERE\n","dl_run2 = None # Replace this\n","\n","# 5. & 6. Iterate and compare\n","pixel_to_check = (0, 0, 0, 0) # Batch_idx, H, W, C\n","if dl_run1:\n","  print(\"--- Run 1 (seed 42) ---\")\n","  iterator_run1 = iter(dl_run1)\n","  try:\n","    batch1_run1 = next(iterator_run1)\n","    value_run1 = batch1_run1['image'][pixel_to_check]\n","    print(f\"Run 1 - Pixel {pixel_to_check} value: {value_run1}\")\n","  except StopIteration:\n","    print(\"dl_run1 exhausted.\")\n","    value_run1 = None\n","else:\n","  print(\"dl_run1 not configured.\")\n","  value_run1 = None\n","\n","if dl_run2:\n","  print(\"\\n--- Run 2 (seed 42, same sampler) ---\")\n","  # If sampler_ex4_seed42 was single-epoch and already used by dl_run1,\n","  # dl_run2 might be empty. For robust test, ensure sampler allows re-iteration\n","  # or use a new sampler instance with the same seed.\n","  # If sampler_ex4_seed42 had num_epochs=None, iter(dl_run2) is fine.\n","  # If num_epochs=1, you might need to re-create sampler_ex4_seed42 for dl_run2\n","  # or ensure dl_run1 didn't exhaust it (e.g. by not fully iterating it).\n","  # For this exercise, assume sampler_ex4_seed42 can be re-used or is fresh for dl_run2.\n","  iterator_run2 = iter(dl_run2)\n","  try:\n","    batch1_run2 = next(iterator_run2)\n","    value_run2 = batch1_run2['image'][pixel_to_check]\n","    print(f\"Run 2 - Pixel {pixel_to_check} value: {value_run2}\")\n","\n","    # 7. Verify\n","    if value_run1 is not None and value_run2 is not None:\n","      if np.allclose(value_run1, value_run2):\n","        print(\"\\nSUCCESS: Pixel values are identical. Randomness is reproducible!\")\n","      else:\n","        print(f\"\\nFAILURE: Pixel values differ. value1={value_run1}, value2={value_run2}\")\n","  except StopIteration:\n","    print(\"dl_run2 exhausted. This might happen if the sampler was single-epoch and already used.\")\n","    value_run2 = None\n","else:\n","  print(\"dl_run2 not configured.\")\n","\n","# 8. (Optional) Test with a different seed\n","# TODO: Create sampler_ex4_seed100 (seed=100)\n","# sampler_ex4_seed100 = grain.IndexSampler(...)\n","# YOUR CODE HERE\n","sampler_ex4_seed100 = None # Replace this\n","\n","# TODO: Create dl_run3 with sampler_ex4_seed100\n","# dl_run3 = grain.DataLoader(...)\n","# YOUR CODE HERE\n","dl_run3 = None # Replace this\n","\n","if dl_run3:\n","  print(\"\\n--- Run 3 (seed 100) ---\")\n","  iterator_run3 = iter(dl_run3)\n","  try:\n","    batch1_run3 = next(iterator_run3)\n","    value_run3 = batch1_run3['image'][pixel_to_check]\n","    print(f\"Run 3 - Pixel {pixel_to_check} value: {value_run3}\")\n","    if value_run1 is not None and not np.allclose(value_run1, value_run3):\n","      print(\"SUCCESS: Pixel values differ from Run 1 (seed 42), as expected with a new seed.\")\n","    elif value_run1 is not None:\n","      print(\"NOTE: Pixel values are the same as Run 1. Check seed or logic.\")\n","  except StopIteration:\n","    print(\"dl_run3 exhausted.\")\n","else:\n","  print(\"\\nOptional part (dl_run3) not configured.\")"],"metadata":{"id":"nrZtIbOiAyOC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Exercise 4: Solution\n","# 1. Define RandomBrightnessAdjust\n","class RandomBrightnessAdjust(grain.RandomMapTransform):\n","  def random_map(self, features: Dict[str, Any], rng: np.random.Generator) -> Dict[str, Any]:\n","    image = features['image'] # Assumes image is already float, e.g. from ConvertToFloat\n","    # Generate a random brightness factor using the provided rng\n","    brightness_factor = rng.uniform(0.7, 1.3)\n","\n","    # Apply brightness adjustment and clip\n","    adjusted_image = image * brightness_factor\n","    adjusted_image = np.clip(adjusted_image, 0.0, 1.0)\n","\n","    updated_features = features.copy()\n","    updated_features['image'] = adjusted_image\n","    return updated_features\n","\n","# 2. Reuse/Create DataSource, Sampler, ConvertToFloat\n","if 'MySource' not in globals(): # Basic check for MySource\n","  class MySource(grain.RandomAccessDataSource):\n","    def __init__(self, num_records: int = 1000):\n","      self._num_records = num_records\n","    def __len__(self) -> int:\n","      return self._num_records\n","    def __getitem__(self, idx: int) -> Dict[str, Any]:\n","      effective_idx = idx % self._num_records\n","      image = np.ones((32, 32, 3), dtype=np.uint8) * (effective_idx % 255)\n","      label = effective_idx % 10\n","      return {'image': image, 'label': label}\n","  print(\"Redefined MySource for Ex4.\")\n","source_ex4 = MySource(num_records=1000)\n","\n","# Sampler with a fixed seed. num_epochs=None allows re-iteration for multiple DataLoaders.\n","# If num_epochs=1, the sampler instance can only be fully iterated once.\n","# For this test, using num_epochs=None or re-creating the sampler for each DataLoader is safest.\n","# Let's use num_epochs=None to allow the same sampler instance to be used.\n","sampler_ex4_seed42 = grain.IndexSampler(\n","  num_records=len(source_ex4),\n","  shard_options=grain.NoSharding(),\n","  shuffle=True,\n","  num_epochs=None, # Allow indefinite iteration\n","  seed=42\n","  )\n","print(\"DataSource and Sampler (seed 42) for Ex4 ready.\")\n","\n","if 'ConvertToFloat' not in globals(): # Basic check for ConvertToFloat\n","  class ConvertToFloat(grain.MapTransform):\n","    def map(self, features: Dict[str, Any]) -> Dict[str, Any]:\n","      image = features['image'].astype(np.float32) / 255.0\n","      return {'image': image, 'label': features['label']}\n","  print(\"Redefined ConvertToFloat for Ex4.\")\n","\n","# 3. Create list of operations\n","random_brightness_adjuster = RandomBrightnessAdjust()\n","transformations_ex4 = [\n","  ConvertToFloat(),\n","  random_brightness_adjuster,\n","  grain.Batch(batch_size=64, drop_remainder=True)\n","]\n","print(\"Transformations for Ex4 defined.\")\n","\n","# 4. Instantiate two DataLoaders with the same config\n","# Using worker_count > 0 to also test picklability of RandomBrightnessAdjust\n","num_workers_ex4 = 2\n","dl_run1 = grain.DataLoader(\n","  data_source=source_ex4,\n","  operations=transformations_ex4,\n","  sampler=sampler_ex4_seed42, # Same sampler instance\n","  worker_count=num_workers_ex4,\n","  shard_options=grain.NoSharding(),\n","  read_options=grain.ReadOptions(num_threads=0)\n","  )\n","\n","dl_run2 = grain.DataLoader(\n","  data_source=source_ex4,\n","  operations=transformations_ex4,\n","  sampler=sampler_ex4_seed42, # Same sampler instance\n","  worker_count=num_workers_ex4,\n","  shard_options=grain.NoSharding(),\n","  read_options=grain.ReadOptions(num_threads=0)\n","  )\n","print(f\"DataLoaders for Run1 and Run2 created with worker_count={num_workers_ex4}.\")\n","\n","# 5. & 6. Iterate and compare\n","pixel_to_check = (0, 0, 0, 0) # Batch_idx=0, H=0, W=0, C=0\n","print(\"\\n--- Run 1 (seed 42) ---\")\n","iterator_run1 = iter(dl_run1)\n","\n","try:\n","  batch1_run1 = next(iterator_run1)\n","  value_run1 = batch1_run1['image'][pixel_to_check]\n","  print(f\"Run 1 - Pixel {pixel_to_check} value: {value_run1}\")\n","except StopIteration:\n","  print(\"dl_run1 exhausted.\")\n","  value_run1 = None\n","\n","print(\"\\n--- Run 2 (seed 42, same sampler instance) ---\")\n","iterator_run2 = iter(dl_run2) # Gets a new iterator from the DataLoader\n","\n","try:\n","  batch1_run2 = next(iterator_run2)\n","  value_run2 = batch1_run2['image'][pixel_to_check]\n","  print(f\"Run 2 - Pixel {pixel_to_check} value: {value_run2}\")\n","  # 7. Verify\n","  if value_run1 is not None and value_run2 is not None:\n","    if np.allclose(value_run1, value_run2):\n","      print(\"\\nSUCCESS: Pixel values are identical. Randomness is reproducible with the same sampler instance!\")\n","    else:\n","      print(f\"\\nFAILURE: Pixel values differ. value1={value_run1}, value2={value_run2}. This shouldn't happen if sampler is re-used correctly.\")\n","except StopIteration:\n","  print(\"dl_run2 exhausted.\")\n","\n","# 8. (Optional) Test with a different seed\n","sampler_ex4_seed100 = grain.IndexSampler(\n","  num_records=len(source_ex4),\n","  shard_options=grain.NoSharding(),\n","  shuffle=True,\n","  num_epochs=None,\n","  seed=100 # Different seed\n","  )\n","\n","dl_run3 = grain.DataLoader(\n","  data_source=source_ex4,\n","  operations=transformations_ex4,\n","  sampler=sampler_ex4_seed100, # Sampler with different seed\n","  worker_count=num_workers_ex4,\n","  shard_options=grain.NoSharding(),\n","  read_options=grain.ReadOptions(num_threads=0)\n","  )\n","print(\"\\nDataLoader for Run3 (seed 100) created.\")\n","print(\"\\n--- Run 3 (seed 100) ---\")\n","iterator_run3 = iter(dl_run3)\n","\n","try:\n","  batch1_run3 = next(iterator_run3)\n","  value_run3 = batch1_run3['image'][pixel_to_check]\n","  print(f\"Run 3 - Pixel {pixel_to_check} value: {value_run3}\")\n","  if value_run1 is not None and not np.allclose(value_run1, value_run3):\n","    print(\"SUCCESS: Pixel values differ from Run 1 (seed 42), as expected with a new sampler seed.\")\n","  elif value_run1 is not None:\n","    print(\"NOTE: Pixel values are the same as Run 1. This is unexpected if seeds are different.\")\n","except StopIteration:\n","  print(\"dl_run3 exhausted.\")"],"metadata":{"id":"JmdOMp4sIInr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","## Exercise 5: Data Sharding for Distributed Training\n","\n","**Goal:** Understand how Grain handles data sharding, essential for distributed training where each JAX process needs a unique slice of data.\n","\n","**Background:**\n","In a real distributed JAX setup, you'd have multiple Python processes. Each process would call `jax.process_index()` to know its ID and `jax.process_count()` for the total number of processes. `grain.sharding.ShardByJaxProcess()` is a helper that automatically uses these values.\n","\n","Since we are in a single Colab notebook (simulating one JAX process, even with multiple virtual devices), we can't directly run multiple JAX processes. Instead, we will manually create `grain.ShardOptions` to simulate what would happen on two different processes.\n","\n","**Instructions:**\n","1.  Reuse `MySource` and `transformations` (e.g., `ConvertToFloat` and `grain.Batch`) from previous exercises.\n","2.  Define `shard_count = 2`.\n","3.  **Simulate Process 0:**\n","    *   Create `shard_options_p0 = grain.ShardOptions(shard_index=0, shard_count=shard_count, drop_remainder=True)`.\n","    *   Create an `IndexSampler` (`sampler_p0`) using these `shard_options_p0`. Ensure it shuffles and uses a common seed (e.g., 42).\n","    *   Create a `DataLoader` (`dl_p0`) using this `sampler_p0` and the `shard_options_p0` passed to the DataLoader itself.\n","    *   Iterate through `dl_p0` and collect all unique labels from the first few batches (or all batches if `num_epochs=1`).\n","4.  **Simulate Process 1:**\n","    *   Create `shard_options_p1 = grain.ShardOptions(shard_index=1, shard_count=shard_count, drop_remainder=True)`.\n","    *   Create an `IndexSampler` (`sampler_p1`) using `shard_options_p1` (same seed as `sampler_p0`).\n","    *   Create a `DataLoader` (`dl_p1`) using `sampler_p1` and `shard_options_p1`.\n","    *   Iterate through `dl_p1` and collect all unique labels.\n","5.  **Verify:**\n","    *   Print the set of unique labels obtained by \"Process 0\" and \"Process 1\".\n","    *   Confirm that these two sets of labels are largely distinct (they might have minor overlaps if shuffling leads to boundary items being similar by chance, but the bulk of data indices processed should be different). The key is that the *indices* sampled by `sampler_p0` and `sampler_p1` should be disjoint.\n","    *   The `drop_remainder=True` in `ShardOptions` ensures that if the dataset size isn't perfectly divisible by `shard_count`, some data might be dropped to ensure shards are equal or nearly equal (depending on implementation details).\n","\n","**Note on `shard_options` in `IndexSampler` vs `DataLoader`:**\n","The `shard_options` argument to `grain.DataLoader` is the primary way to enable sharding for a JAX process. The `DataLoader` will then ensure its underlying sampler (even if you provide a non-sharded one) respects these global sharding options for the current JAX process. If you provide an `IndexSampler` that is *already* sharded, its sharding must be compatible with the `DataLoader`'s `shard_options`. For simplicity and clarity in distributed settings, passing `ShardByJaxProcess()` or manually configured `ShardOptions` to the `DataLoader` is typical."],"metadata":{"id":"OGMGYA7wK1Nx"}},{"cell_type":"code","source":["# @title Exercise 5: Student Code\n","# 1. Reuse DataSource and basic transformations\n","# TODO: Instantiate MySource (from Ex1)\n","# source_ex5 = ...\n","# YOUR CODE HERE\n","source_ex5 = None # Replace this\n","\n","# TODO: Define basic_transformations_ex5 (e.g., ConvertToFloat, Batch)\n","# (Assuming ConvertToFloat is defined)\n","# basic_transformations_ex5 = [...]\n","# YOUR CODE HERE\n","basic_transformations_ex5 = [] # Replace this\n","\n","# 2. Define shard_count\n","shard_count = 2\n","common_seed = 42\n","num_epochs_for_sharding_test = 1 # To make collection of all labels feasible\n","\n","# 3. Simulate Process 0\n","# TODO: Create shard_options_p0\n","# shard_options_p0 = grain.ShardOptions(...)\n","# YOUR CODE HERE\n","shard_options_p0 = None # Replace this\n","\n","# TODO: Create sampler_p0. Pass shard_options_p0 to the IndexSampler.\n","# sampler_p0 = grain.IndexSampler(...)\n","# YOUR CODE HERE\n","sampler_p0 = None # Replace this\n","\n","# TODO: Create dl_p0. Pass shard_options_p0 to the DataLoader as well.\n","# dl_p0 = grain.DataLoader(...)\n","# YOUR CODE HERE\n","dl_p0 = None # Replace this\n","\n","labels_p0 = set()\n","if dl_p0:\n","  print(\"--- Simulating Process 0 ---\")\n","  # YOUR CODE HERE: Iterate through dl_p0 and collect all unique original labels.\n","  # Remember that labels might be batched. You need to iterate through items in a batch.\n","  # For simplicity, if your MySource generates labels like idx % 10,\n","  # you can try to collect the indices that were sampled.\n","  # Or, more directly, collect the 'label' field from each item.\n","  # To get original indices, you might need a transform that passes index through.\n","  # Let's collect the 'label' values directly.\n","  pass # Replace with iteration logic\n","\n","# 4. Simulate Process 1\n","# TODO: Create shard_options_p1\n","# shard_options_p1 = grain.ShardOptions(...)\n","# YOUR CODE HERE\n","shard_options_p1 = None # Replace this\n","\n","# TODO: Create sampler_p1\n","# sampler_p1 = grain.IndexSampler(...)\n","# YOUR CODE HERE\n","sampler_p1 = None # Replace this\n","\n","# TODO: Create dl_p1\n","# dl_p1 = grain.DataLoader(...)\n","# YOUR CODE HERE\n","dl_p1 = None # Replace this\n","\n","labels_p1 = set()\n","if dl_p1:\n","  print(\"\\n--- Simulating Process 1 ---\")\n","  # YOUR CODE HERE: Iterate through dl_p1 and collect all unique labels.\n","  pass # Replace with iteration logic\n","\n","# 5. Verify\n","print(f\"\\n--- Verification (Total records in source: {len(source_ex5) if source_ex5 else 'N/A'}) ---\")\n","print(f\"Unique labels collected by Process 0 (count {len(labels_p0)}): sorted {sorted(list(labels_p0))[:20]}...\")\n","print(f\"Unique labels collected by Process 1 (count {len(labels_p1)}): sorted {sorted(list(labels_p1))[:20]}...\")\n","if labels_p0 and labels_p1:\n","  intersection = labels_p0.intersection(labels_p1)\n","  if not intersection:\n","    print(\"\\nSUCCESS: No overlap in labels between Process 0 and Process 1. Sharding works as expected!\")\n","  else:\n","    print(f\"\\nNOTE: Some overlap in labels found (count {len(intersection)}): {intersection}.\")\n","    print(\"This can happen if labels are not unique per index, or if sharding logic has issues.\")\n","    print(\"With MySource's label = idx % 10, an overlap in labels is expected even if indices are disjoint.\")\n","    print(\"A better test would be to collect original indices if possible.\")\n","\n","# For a more direct test of sharding of indices:\n","# We can define a DataSource that returns the index itself.\n","class IndexSource(grain.RandomAccessDataSource):\n","  def __init__(self, num_records: int):\n","    self._num_records = num_records\n","  def __len__(self) -> int:\n","    return self._num_records\n","  def __getitem__(self, idx: int) -> int:\n","    return idx % self._num_records # Return the index\n","index_source = IndexSource(num_records=100) # Smaller source for easier inspection\n","\n","idx_sampler_p0 = grain.IndexSampler(len(index_source), shard_options_p0, shuffle=False, num_epochs=1, seed=common_seed)\n","idx_sampler_p1 = grain.IndexSampler(len(index_source), shard_options_p1, shuffle=False, num_epochs=1, seed=common_seed)\n","\n","# DataLoader for indices (no batching, just to see raw sampled indices)\n","# Note: DataLoader expects dicts. Let's make IndexSource return {'index': idx}\n","class IndexDictSource(grain.RandomAccessDataSource):\n","  def __init__(self, num_records: int): self._num_records = num_records\n","  def __len__(self) -> int:\n","    return self._num_records\n","  def __getitem__(self, idx: int) -> Dict[str,int]:\n","    return {'index': idx % self._num_records}\n","index_dict_source = IndexDictSource(num_records=100)\n","\n","# Samplers for IndexDictSource\n","idx_dict_sampler_p0 = grain.IndexSampler(len(index_dict_source), shard_options_p0, shuffle=False, num_epochs=1, seed=common_seed)\n","idx_dict_sampler_p1 = grain.IndexSampler(len(index_dict_source), shard_options_p1, shuffle=False, num_epochs=1, seed=common_seed)\n","\n","# DataLoaders for IndexDictSource\n","# Pass shard_options to DataLoader as well.\n","if shard_options_p0 and shard_options_p1:\n","  dl_indices_p0 = grain.DataLoader(index_dict_source, [], idx_dict_sampler_p0, worker_count=0, shard_options=shard_options_p0)\n","  dl_indices_p1 = grain.DataLoader(index_dict_source, [], idx_dict_sampler_p1, worker_count=0, shard_options=shard_options_p1)\n","  indices_from_p0 = {item['index'] for item in dl_indices_p0} if dl_indices_p0 else set()\n","  indices_from_p1 = {item['index'] for item in dl_indices_p1} if dl_indices_p1 else set()\n","\n","print(f\"\\n--- Verification of INDICES (Source size: {len(index_dict_source)}) ---\")\n","print(f\"Indices from P0 (count {len(indices_from_p0)}, shuffle=False): {sorted(list(indices_from_p0))}\")\n","print(f\"Indices from P1 (count {len(indices_from_p1)}, shuffle=False): {sorted(list(indices_from_p1))}\")\n","\n","if indices_from_p0 and indices_from_p1:\n","  idx_intersection = indices_from_p0.intersection(indices_from_p1)\n","  if not idx_intersection:\n","    print(\"SUCCESS: No overlap in INDICES. Sharding of data sources works correctly!\")\n","  else:\n","    print(f\"FAILURE: Overlap in INDICES found: {idx_intersection}\")\n","else:\n","  print(\"Skipping index verification part as shard_options are not defined.\")\n","  print(\"\\nReminder: In a real distributed setup, you'd use grain.sharding.ShardByJaxProcess() \"\n","  \"and JAX would manage jax.process_index() automatically for each process.\")"],"metadata":{"id":"206ZCYxCKpSc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Exercise 5: Solution\n","# Redefine MySource for Ex5 to include 'original_index'.\n","class MySource(grain.RandomAccessDataSource):\n","  def __init__(self, num_records: int = 1000):\n","    self._num_records = num_records\n","  def __len__(self) -> int:\n","    return self._num_records\n","  def __getitem__(self, idx: int) -> Dict[str, Any]:\n","    effective_idx = idx % self._num_records\n","    image = np.ones((32, 32, 3), dtype=np.uint8) * (effective_idx % 255)\n","    label = effective_idx % 10 # Label is idx % 10\n","    # For better sharding verification, let's also pass the original index\n","    return {'image': image, 'label_test': label, 'original_index': effective_idx}\n","\n","print(\"Redefined MySource for Ex5 to include 'original_index'.\")\n","source_ex5 = MySource(num_records=1000)\n","\n","# Redefine ConvertToFloat for Ex5 to include 'original_index'.\n","class ConvertToFloat(grain.MapTransform):\n","  def map(self, features: Dict[str, Any]) -> Dict[str, Any]:\n","    # This transform should pass through all keys it doesn't modify\n","    updated_features = features.copy()\n","    updated_features['image'] = features['image'].astype(np.float32) / 255.0\n","    return updated_features\n","print(\"Redefined ConvertToFloat for Ex5.\")\n","\n","# We will collect 'original_index' after batching, so batch must preserve it.\n","# grain.Batch by default collates features with the same name.\n","basic_transformations_ex5 = [\n","  ConvertToFloat(),\n","  grain.Batch(batch_size=64, drop_remainder=True) # drop_remainder for batching\n","  ]\n","print(\"DataSource and Transformations for Ex5 ready.\")\n","\n","# 2. Define shard_count\n","shard_count = 2\n","common_seed = 42\n","num_epochs_for_sharding_test = 1\n","\n","# 3. Simulate Process 0\n","shard_options_p0 = grain.ShardOptions(shard_index=0, shard_count=shard_count, drop_remainder=True) # drop_remainder for sharding\n","\n","# Sampler for Process 0. It's important that the sampler itself is sharded.\n","# The DataLoader's shard_options will also apply this sharding if the sampler isn't already sharded,\n","# or verify consistency if it is.\n","sampler_p0 = grain.IndexSampler(\n","  num_records=len(source_ex5),\n","  shard_options=shard_options_p0, # Shard the sampler\n","  shuffle=True, # Shuffle for more realistic scenario\n","  num_epochs=num_epochs_for_sharding_test,\n","  seed=common_seed\n","  )\n","\n","dl_p0 = grain.DataLoader(\n","  data_source=source_ex5,\n","  operations=basic_transformations_ex5,\n","  sampler=sampler_p0,\n","  worker_count=0, # Keep it simple for verification\n","  shard_options=shard_options_p0, # Also inform DataLoader about the sharding context\n","  read_options=grain.ReadOptions(num_threads=0)\n","  )\n","\n","indices_p0 = set()\n","if dl_p0:\n","  print(\"--- Simulating Process 0 ---\")\n","  for batch in dl_p0:\n","    indices_p0.update(batch['original_index'].tolist()) # Collect original indices\n","  print(f\"Process 0 collected {len(indices_p0)} unique indices.\")\n","\n","# 4. Simulate Process 1\n","shard_options_p1 = grain.ShardOptions(shard_index=1, shard_count=shard_count, drop_remainder=True)\n","sampler_p1 = grain.IndexSampler(\n","  num_records=len(source_ex5),\n","  shard_options=shard_options_p1, # Shard the sampler\n","  shuffle=True, # Use same shuffle setting and seed for apples-to-apples comparison of sharding logic\n","  num_epochs=num_epochs_for_sharding_test,\n","  seed=common_seed # Same seed ensures shuffle order is same before sharding\n","  )\n","\n","dl_p1 = grain.DataLoader(\n","  data_source=source_ex5,\n","  operations=basic_transformations_ex5,\n","  sampler=sampler_p1,\n","  worker_count=0,\n","  shard_options=shard_options_p1, # Inform DataLoader\n","  read_options=grain.ReadOptions(num_threads=0)\n","  )\n","\n","indices_p1 = set()\n","if dl_p1:\n","  print(\"\\n--- Simulating Process 1 ---\")\n","  for batch in dl_p1:\n","    indices_p1.update(batch['original_index'].tolist()) # Collect original indices\n","  print(f\"Process 1 collected {len(indices_p1)} unique indices.\")\n","\n","# 5. Verify\n","print(f\"\\n--- Verification of original_indices (Total records in source: {len(source_ex5)}) ---\")\n","# Showing a few from each for brevity\n","print(f\"Unique original_indices from P0 (first 20 sorted): {sorted(list(indices_p0))[:20]}...\")\n","print(f\"Unique original_indices from P1 (first 20 sorted): {sorted(list(indices_p1))[:20]}...\")\n","expected_per_shard = len(source_ex5) // shard_count # Due to drop_remainder=True in ShardOptions\n","print(f\"Expected records per shard (approx, due to drop_remainder in sharding): {expected_per_shard}\")\n","print(f\"Actual for P0: {len(indices_p0)}, P1: {len(indices_p1)}\")\n","\n","if indices_p0 and indices_p1:\n","  intersection = indices_p0.intersection(indices_p1)\n","  if not intersection:\n","    print(\"\\nSUCCESS: No overlap in original_indices between Process 0 and Process 1. Sharding works!\")\n","  else:\n","    print(f\"\\nFAILURE: Overlap in original_indices found (count {len(intersection)}): {sorted(list(intersection))[:20]}...\")\n","    print(\"This should not happen if sharding is correct and seeds/shuffle are consistent.\")\n","else:\n","  print(\"Could not perform intersection test as one or both sets of indices are empty.\")\n","  total_unique_indices_seen = len(indices_p0.union(indices_p1))\n","  print(f\"Total unique indices seen across both simulated processes: {total_unique_indices_seen}\")\n","\n","# With drop_remainder=True in sharding, total might be less than len(source_ex5)\n","# if len(source_ex5) is not divisible by shard_count.\n","# Example: 1000 records, 2 shards. Each gets 500. Total 1000.\n","# Example: 1001 records, 2 shards. drop_remainder=True means each gets 500. Total 1000. 1 record dropped.\n","print(\"\\nReminder: In a real distributed JAX application:\")\n","print(\"1. Each JAX process would run this script (or similar).\")\n","print(\"2. shard_options = grain.sharding.ShardByJaxProcess(drop_remainder=True) would be used.\")\n","print(\"3. jax.process_index() and jax.process_count() would provide the correct shard info automatically.\")\n","print(\"4. The IndexSampler and DataLoader would be configured with these auto-detected shard_options.\")"],"metadata":{"id":"J-BAVny4NRDH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","---\n","## Exercise 6: Integrating Grain with a JAX/Flax NNX (Conceptual) Loop\n","\n","**Goal:** Understand how a Grain `DataLoader` feeds data into a typical JAX/Flax NNX training loop. This exercise is conceptual regarding model training (no actual weight updates) but practical in terms of data flow.\n","\n","**Instructions:**\n","1.  **Define a Simple Flax NNX Model:**\n","    *   Create a class `SimpleNNXModel` inheriting from `nnx.Module`.\n","    *   In `__init__`, initialize an `nnx.Linear` layer. The input features should match the flattened image dimensions (e.g., `32*32*3`), and output features can be `num_classes` (e.g., 10). Remember to pass `rngs` for parameter initialization.\n","    *   Implement `__call__(self, x)`: it should flatten the input image `x` (if it's `B, H, W, C`) and pass it through the linear layer.\n","2.  **Define a Conceptual `train_step`:**\n","    *   This JAX function should be JIT-compiled (`@jax.jit`).\n","    *   It takes the `model` (your `SimpleNNXModel` instance) and a `batch` from Grain.\n","    *   Inside, it performs a forward pass: `logits = model(batch['image'])`.\n","    *   It calculates a dummy loss, e.g., `loss = jnp.mean(logits)`. (No real loss computation or gradients needed for this exercise).\n","    *   It returns the `loss` and the `model`. In a real training scenario using `nnx.Optimizer`, the optimizer would update the model's parameters in-place. The `train_step` function would typically return the `loss`, the updated `model`, and the updated `optimizer` state to be used in the next iteration.\n","3.  **Set up DataLoader:**\n","    *   Use `MySource` (the one that yields `{'image': ..., 'label': ...}`), an `IndexSampler` (e.g., for a few epochs), and `transformations` (e.g., `ConvertToFloat`, `grain.Batch`).\n","    *   Instantiate a `grain.DataLoader`.\n","4.  **Write the Training Loop:**\n","    *   Initialize your `SimpleNNXModel` with an appropriate JAX PRNG key.\n","    *   Get an iterator from your `DataLoader`.\n","    *   Loop for a fixed number of steps (e.g., 100):\n","        *   Get the `next_batch` from the iterator. Handle `StopIteration` if the loader is exhausted.\n","        *   Call your `train_step` function with the current `model` and `next_batch`.\n","        *   Print the dummy loss occasionally."],"metadata":{"id":"rFyUOM_PbAEV"}},{"cell_type":"code","source":["# @title Exercise 6: Student Code\n","# 1. Define SimpleNNXModel\n","class SimpleNNXModel(nnx.Module):\n","  def __init__(self, din: int, dout: int, *, rngs: nnx.Rngs):\n","    # TODO: Initialize an nnx.Linear layer\n","    # self.linear = nnx.Linear(...)\n","    # YOUR CODE HERE\n","    self.linear = None # Replace this\n","  def __call__(self, x: jax.Array):\n","    # TODO: Flatten the input image (if B, H, W, C) and pass through linear layer\n","    # x_flat = x.reshape((x.shape[0], -1))\n","    # return self.linear(x_flat)\n","    # YOUR CODE HERE\n","    return x # Replace this\n","\n","# 2. Define train_step\n","def train_step(model: SimpleNNXModel, batch: Dict[str, jax.Array]):\n","  # TODO: Perform forward pass: model(batch['image'])\n","  # logits = ...\n","  # YOUR CODE HERE\n","  logits = model(batch['image']) # Assuming model handles it\n","  # TODO: Calculate a dummy loss (e.g., mean of logits)\n","  # loss = ...\n","  # YOUR CODE HERE\n","  loss = jnp.array(0.0) # Replace this\n","\n","  # In a real scenario, you'd also compute gradients and update model parameters here.\n","  # For this exercise, we just return the loss and the original model.\n","  return loss, model\n","\n","# 3. Set up DataLoader\n","# TODO: Instantiate MySource (from Ex1, or the one with 'original_index' if you prefer)\n","# source_ex6 = ...\n","# YOUR CODE HERE\n","source_ex6 = None # Replace this\n","\n","# TODO: Instantiate an IndexSampler for a few epochs (e.g., 2 epochs)\n","# sampler_ex6 = grain.IndexSampler(...)\n","# YOUR CODE HERE\n","sampler_ex6 = None # Replace this\n","\n","# TODO: Define transformations_ex6 (e.g., ConvertToFloat, grain.Batch)\n","# (Assuming ConvertToFloat is defined)\n","# transformations_ex6 = [...]\n","# YOUR CODE HERE\n","transformations_ex6 = [] # Replace this\n","\n","# TODO: Instantiate data_loader_ex6\n","# data_loader_ex6 = grain.DataLoader(...)\n","# YOUR CODE HERE\n","data_loader_ex6 = None # Replace this\n","\n","# 4. Write the Training Loop\n","if data_loader_ex6: # Proceed only if DataLoader is configured\n","  # TODO: Initialize SimpleNNXModel\n","  # image_height, image_width, image_channels = 32, 32, 3\n","  # num_classes_ex6 = 10\n","  # model_key = jax.random.key(0)\n","  # model_ex6 = SimpleNNXModel(...)\n","  # YOUR CODE HERE\n","  model_ex6 = None # Replace this\n","\n","if model_ex6:\n","  # TODO: Get an iterator from data_loader_ex6\n","  # grain_iterator_ex6 = ...\n","  # YOUR CODE HERE\n","  grain_iterator_ex6 = iter([]) # Replace this\n","\n","  num_steps = 100\n","  print(f\"\\nStarting conceptual training loop for {num_steps} steps...\")\n","  for step in range(num_steps):\n","    try:\n","      # TODO: Get next_batch from iterator\n","      # next_batch = ...\n","      # YOUR CODE HERE\n","      next_batch = None # Replace this\n","      if next_batch is None:\n","        raise StopIteration # Simulate exhaustion if not implemented\n","    except StopIteration:\n","      print(f\"DataLoader exhausted at step {step}. Ending loop.\")\n","      break\n","\n","    # TODO: Call train_step\n","    # loss, model_ex6 = train_step(model_ex6, next_batch) # model_ex6 isn't actually updated here\n","    # YOUR CODE HERE\n","    loss = jnp.array(0.0) # Replace this\n","\n","    if step % 20 == 0 or step == num_steps - 1:\n","      print(f\"Step {step}: Dummy Loss = {loss.item():.4f}\")\n","      print(\"Conceptual training loop finished.\")\n","    else:\n","      print(\"Model for Ex6 not initialized.\")\n","else:\n","  print(\"DataLoader for Ex6 not configured.\")"],"metadata":{"id":"Ml0niZnwQSIF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Exercise 6: Solution\n","# 1. Define SimpleNNXModel\n","class SimpleNNXModel(nnx.Module):\n","  def __init__(self, din: int, dout: int, *, rngs: nnx.Rngs):\n","    self.linear = nnx.Linear(din, dout, rngs=rngs)\n","  def __call__(self, x: jax.Array) -> jax.Array:\n","    # Assuming x is (B, H, W, C)\n","    batch_size = x.shape[0]\n","    x_flat = x.reshape((batch_size, -1)) # Flatten H, W, C dimensions\n","    return self.linear(x_flat)\n","\n","# 2. Define conceptual train_step\n","def train_step(model: SimpleNNXModel, batch: Dict[str, jax.Array]):\n","  # Perform forward pass\n","  logits = model(batch['image']) # model.call is invoked\n","  # Calculate a dummy loss\n","  loss = jnp.mean(logits**2) # Example: mean of squared logits\n","\n","  # In a real training step:\n","  # # 1. Define a loss function.\n","  # def loss_fn(model):\n","  #   logits = model(batch['image'])\n","  #   # loss_value = ... (e.g., optax.softmax_cross_entropy_with_integer_labels)\n","  #   return jnp.mean(logits**2) # Using dummy loss from exercise\n","  #\n","  # # 2. Calculate gradients.\n","  # grads = nnx.grad(loss_fn, wrt=nnx.Param)(model)\n","  #\n","  # # 3. Update the model's parameters in-place using the optimizer.\n","  # #    Note: The optimizer is defined outside the train step.\n","  # #    e.g., optimizer = nnx.Optimizer(model, optax.adam(1e-3), wrt=nnx.Param)\n","  # optimizer.update(model, grads)\n","  #\n","  # # 4. Return the updated model and optimizer state.\n","  return model, optimizer, loss\n","\n","# 3. Set up DataLoader\n","# Redefine MySource for Ex6.\n","class MySource(grain.RandomAccessDataSource):\n","  def __init__(self, num_records: int = 1000):\n","    self._num_records = num_records\n","  def __len__(self) -> int:\n","    return self._num_records\n","  def __getitem__(self, idx: int) -> Dict[str, Any]:\n","    effective_idx = idx % self._num_records\n","    image = np.ones((32, 32, 3), dtype=np.uint8) * (effective_idx % 255)\n","    label = effective_idx % 10\n","    return {'image': image, 'label': label}\n","print(\"Redefined MySource for Ex6.\")\n","\n","source_ex6 = MySource(num_records=1000)\n","sampler_ex6 = grain.IndexSampler(\n","  num_records=len(source_ex6),\n","  shard_options=grain.NoSharding(),\n","  shuffle=True,\n","  num_epochs=2, # Run for 2 epochs\n","  seed=42\n","  )\n","\n","# Redefine ConvertToFloat for Ex6.\n","class ConvertToFloat(grain.MapTransform):\n","  def map(self, features: Dict[str, Any]) -> Dict[str, Any]:\n","    updated_features = features.copy()\n","    updated_features['image'] = features['image'].astype(np.float32) / 255.0\n","    return updated_features\n","print(\"Redefined ConvertToFloat for Ex6.\")\n","\n","transformations_ex6 = [\n","  ConvertToFloat(),\n","  grain.Batch(batch_size=64, drop_remainder=True)\n","  ]\n","\n","data_loader_ex6 = grain.DataLoader(\n","  data_source=source_ex6,\n","  operations=transformations_ex6,\n","  sampler=sampler_ex6,\n","  worker_count=2, # Use a couple of workers\n","  shard_options=grain.NoSharding(),\n","  read_options=grain.ReadOptions(num_threads=0)\n","  )\n","print(\"DataLoader for Ex6 configured.\")\n","\n","# 4. Write the Training Loop\n","# Define image dimensions and number of classes\n","image_height, image_width, image_channels = 32, 32, 3\n","input_dim = image_height * image_width * image_channels\n","num_classes_ex6 = 10\n","\n","# Initialize SimpleNNXModel\n","# NNX modules are typically initialized outside JIT, then their state can be passed.\n","# For this conceptual example, the model instance itself is passed.\n","model_key = jax.random.key(0)\n","model_ex6 = SimpleNNXModel(din=input_dim, dout=num_classes_ex6, rngs=nnx.Rngs(params=model_key))\n","print(f\"SimpleNNXModel initialized. Input dim: {input_dim}, Output dim: {num_classes_ex6}\")\n","\n","# Get an iterator from data_loader_ex6\n","grain_iterator_ex6 = iter(data_loader_ex6)\n","num_steps = 100 # Total steps for the conceptual loop\n","\n","print(f\"\\nStarting conceptual training loop for {num_steps} steps...\")\n","for step_idx in range(num_steps):\n","  try:\n","    # Get next_batch from iterator\n","    next_batch = next(grain_iterator_ex6)\n","  except StopIteration:\n","    print(f\"DataLoader exhausted at step {step_idx}. Ending loop.\")\n","    # Example: Re-initialize iterator if sampler allows multiple epochs\n","    # if sampler_ex6.num_epochs is None or sampler_ex6.num_epochs > 1 (and we tracked current epoch):\n","      # print(\"Re-initializing iterator for new epoch...\")\n","      # grain_iterator_ex6 = iter(data_loader_ex6)\n","      # next_batch = next(grain_iterator_ex6)\n","    # else:\n","    break # Exit loop if truly exhausted\n","\n","  # Call train_step\n","  # JAX arrays in batch are automatically handled by jax.jit\n","  _, loss = train_step(model_ex6, next_batch) # model_ex6 state isn't actually updated here\n","\n","  if step_idx % 20 == 0 or step_idx == num_steps - 1:\n","    print(f\"Step {step_idx}: Dummy Loss = {loss.item():.4f}\") # .item() to get Python scalar from JAX array\n","print(\"Conceptual training loop finished.\")"],"metadata":{"id":"qNHGUhUbdAgF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","## Exercise 7: Checkpointing and Resuming Data Iteration\n","\n","**Goal:** Understand how to save and restore the state of a Grain data iterator for reproducible experiments, especially when resuming long training runs.\n","\n","**Background:**\n","Grain's `DataLoader` produces an iterator when you call `iter(data_loader)`. This iterator (`grain.PyGrainDatasetIterator`) has `get_state()` and `set_state()` methods. These allow you to capture the internal state of the iteration (e.g., current position, RNG states for samplers/transforms) and restore it later. For full experiment checkpointing, this data iterator state should be saved alongside your model parameters (often using a library like Orbax).\n","\n","**Instructions:**\n","1.  Set up a `DataLoader` (e.g., using `MySource`, an `IndexSampler` with `num_epochs=None` for indefinite iteration and a seed, and some basic `transformations`).\n","2.  Get an iterator (`iterator1`) from this `DataLoader`.\n","3.  Iterate a few times (e.g., 3 batches) using `next(iterator1)` and store the last batch obtained.\n","4.  **Save State:** Call `saved_iterator_state = iterator1.get_state()`.\n","5.  **Simulate Resumption:**\n","    *   Get a *new* iterator (`iterator2`) from the *same* `DataLoader` instance.\n","    *   **Restore State:** Call `iterator2.set_state(saved_iterator_state)`.\n","6.  Iterate once using `next(iterator2)` to get a batch (`resumed_batch`).\n","7.  **Verify:**\n","    *   The `resumed_batch` obtained from `iterator2` should be the *same* as the batch that *would have come after* the last batch from `iterator1`.\n","    *   To verify this:\n","        *   After getting `saved_iterator_state` from `iterator1`, call `next(iterator1)` one more time to get the `expected_next_batch_from_iterator1`.\n","        *   Compare `resumed_batch` (from `iterator2` after `set_state`) with `expected_next_batch_from_iterator1`. Their contents (e.g., image data) should be identical."],"metadata":{"id":"cr7s9TH_h0B-"}},{"cell_type":"code","source":["# @title Exercise 7: Student Code\n","# 1. Set up DataLoader\n","# TODO: Instantiate MySource (e.g., from Ex1)\n","# source_ex7 = ...\n","# YOUR CODE HERE\n","source_ex7 = None # Replace this\n","\n","# TODO: Instantiate an IndexSampler (num_epochs=None, seed=42)\n","# sampler_ex7 = grain.IndexSampler(...)\n","# YOUR CODE HERE\n","sampler_ex7 = None # Replace this\n","\n","# TODO: Define transformations_ex7 (e.g., ConvertToFloat, Batch)\n","# (Assuming ConvertToFloat is defined)\n","# transformations_ex7 = [...]\n","# YOUR CODE HERE\n","transformations_ex7 = [] # Replace this\n","\n","# TODO: Instantiate data_loader_ex7\n","# data_loader_ex7 = grain.DataLoader(...)\n","# YOUR CODE HERE\n","data_loader_ex7 = None # Replace this\n","\n","if data_loader_ex7:\n","  # 2. Get iterator1\n","  # TODO: iterator1 = iter(...)\n","  # YOUR CODE HERE\n","  iterator1 = iter([]) # Replace this\n","\n","# 3. Iterate a few times\n","num_initial_iterations = 3\n","print(f\"--- Initial Iteration (iterator1) for {num_initial_iterations} batches ---\")\n","last_batch_iterator1 = None\n","for i in range(num_initial_iterations):\n","  try:\n","    # TODO: last_batch_iterator1 = next(...)\n","    # YOUR CODE HERE\n","    last_batch_iterator1 = {} # Replace this\n","    print(f\"iterator1, batch {i+1} - first label: {last_batch_iterator1.get('label', [None])[0]}\")\n","  except StopIteration:\n","    print(\"iterator1 exhausted prematurely.\")\n","    break\n","\n","# 4. Save State\n","# TODO: saved_iterator_state = iterator1.get_state()\n","# YOUR CODE HERE\n","saved_iterator_state = None # Replace this\n","print(f\"\\nIterator state saved. Type: {type(saved_iterator_state)}\")\n","\n","# For verification: get the *next* batch from iterator1 *after* saving state\n","expected_next_batch_from_iterator1 = None\n","if saved_iterator_state is not None: # Ensure state was actually saved\n","  try:\n","    # TODO: expected_next_batch_from_iterator1 = next(...)\n","    # YOUR CODE HERE\n","    expected_next_batch_from_iterator1 = {} # Replace this\n","    print(f\"Expected next batch (from iterator1 after get_state) - first label: {expected_next_batch_from_iterator1.get('label', [None])[0]}\")\n","  except StopIteration:\n","    print(\"iterator1 exhausted when trying to get expected_next_batch.\")\n","\n","# 5. Simulate Resumption\n","# TODO: Get iterator2 from the same data_loader_ex7\n","# iterator2 = iter(...)\n","# YOUR CODE HERE\n","iterator2 = iter([]) # Replace this\n","\n","if saved_iterator_state is not None:\n","  # TODO: iterator2.set_state(...)\n","  # YOUR CODE HERE\n","  print(\"\\n--- Resumed Iteration (iterator2) ---\")\n","  print(\"Iterator state restored to iterator2.\")\n","else:\n","  print(\"\\nSkipping resumption, saved_iterator_state is None.\")\n","\n","# 6. Iterate once from iterator2\n","resumed_batch = None\n","if saved_iterator_state is not None: # Only if state was set\n","  try:\n","    # TODO: resumed_batch = next(...)\n","    # YOUR CODE HERE\n","    resumed_batch = {} # Replace this\n","    print(f\"Resumed batch (from iterator2 after set_state) - first label: {resumed_batch.get('label', [None])[0]}\")\n","  except StopIteration:\n","    print(\"iterator2 exhausted immediately after set_state.\")\n","\n","# 7. Verify\n","if expected_next_batch_from_iterator1 is not None and resumed_batch is not None:\n","  # Compare 'image' data of the first element in the batch\n","  # TODO: Perform comparison (e.g., np.allclose on image data)\n","  # are_identical = np.allclose(...)\n","  # YOUR CODE HERE\n","  are_identical = False # Replace this\n","\n","  if are_identical:\n","    print(\"\\nSUCCESS: Resumed batch is identical to the expected next batch. Checkpointing works!\")\n","  else:\n","    print(\"\\nFAILURE: Resumed batch differs from the expected next batch.\")\n","    # print(f\"Expected image data (sample): {expected_next_batch_from_iterator1['image'][0,0,0,:3]}\")\n","    # print(f\"Resumed image data (sample): {resumed_batch['image'][0,0,0,:3]}\")\n","elif saved_iterator_state is not None:\n","  print(\"\\nVerification inconclusive: could not obtain both expected and resumed batches.\")\n","else:\n","  print(\"DataLoader for Ex7 not configured.\")"],"metadata":{"id":"M-y5f4x-gzmA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Exercise 7: Solution\n","# 1. Set up DataLoader\n","# Redefine MySource for Ex7.\n","class MySource(grain.RandomAccessDataSource):\n","  def __init__(self, num_records: int = 1000):\n","    self._num_records = num_records\n","  def __len__(self) -> int:\n","    return self._num_records\n","  def __getitem__(self, idx: int) -> Dict[str, Any]:\n","    effective_idx = idx % self._num_records\n","    image = np.ones((32, 32, 3), dtype=np.uint8) * (effective_idx % 255)\n","    label = effective_idx % 10\n","    # Add original_index for easier verification if needed, though label might suffice\n","    return {'image': image, 'label': label, 'original_index': effective_idx}\n","print(\"Redefined MySource for Ex7.\")\n","\n","source_ex7 = MySource(num_records=1000)\n","sampler_ex7 = grain.IndexSampler(\n","  num_records=len(source_ex7),\n","  shard_options=grain.NoSharding(),\n","  shuffle=True, # Shuffling makes the test more robust\n","  num_epochs=None, # Indefinite iteration\n","  seed=42\n","  )\n","\n","# Redefine ConvertToFloat for Ex7.\n","class ConvertToFloat(grain.MapTransform):\n","  def map(self, features: Dict[str, Any]) -> Dict[str, Any]:\n","    updated_features = features.copy()\n","    updated_features['image'] = features['image'].astype(np.float32) / 255.0\n","    return updated_features\n","print(\"Redefined ConvertToFloat for Ex7.\")\n","\n","transformations_ex7 = [\n","  ConvertToFloat(),\n","  grain.Batch(batch_size=64, drop_remainder=True)\n","  ]\n","\n","data_loader_ex7 = grain.DataLoader(\n","  data_source=source_ex7,\n","  operations=transformations_ex7,\n","  sampler=sampler_ex7,\n","  worker_count=0, # Simpler for state verification, but works with >0 too\n","  shard_options=grain.NoSharding(),\n","  read_options=grain.ReadOptions(num_threads=0)\n","  )\n","print(\"DataLoader for Ex7 configured.\")\n","\n","# 2. Get iterator1\n","iterator1 = iter(data_loader_ex7)\n","\n","# 3. Iterate a few times\n","num_initial_iterations = 3\n","print(f\"--- Initial Iteration (iterator1) for {num_initial_iterations} batches ---\")\n","last_batch_iterator1 = None\n","for i in range(num_initial_iterations):\n","  try:\n","    last_batch_iterator1 = next(iterator1)\n","    # Using 'original_index' for more robust check than just 'label'\n","    print(f\"iterator1, batch {i+1} - first original_index: {last_batch_iterator1['original_index'][0]}\")\n","  except StopIteration:\n","    print(\"iterator1 exhausted prematurely.\")\n","    break\n","\n","# 4. Save State\n","# Make a deep copy if you plan to continue using iterator1 and don't want\n","# its state object to be modified if Python passes by reference internally (usually not an issue for simple state).\n","# For PyGrainDatasetIterator, get_state() returns a new state object.\n","saved_iterator_state = iterator1.get_state()\n","print(f\"\\nIterator state saved. Type: {type(saved_iterator_state)}\")\n","\n","# For verification: get the next batch from iterator1 after saving state\n","expected_next_batch_from_iterator1 = None\n","if saved_iterator_state is not None:\n","  try:\n","    expected_next_batch_from_iterator1 = next(iterator1)\n","    print(f\"Expected next batch (from iterator1 after get_state) - first original_index: {expected_next_batch_from_iterator1['original_index'][0]}\")\n","  except StopIteration:\n","    print(\"iterator1 exhausted when trying to get expected_next_batch.\")\n","\n","# 5. Simulate Resumption\n","# Get a new iterator from the same DataLoader instance.\n","iterator2 = iter(data_loader_ex7)\n","if saved_iterator_state is not None:\n","  iterator2.set_state(saved_iterator_state)\n","  print(\"\\n--- Resumed Iteration (iterator2) ---\")\n","  print(\"Iterator state restored to iterator2.\")\n","else:\n","  print(\"\\nSkipping resumption, saved_iterator_state is None.\")\n","\n","# 6. Iterate once from iterator2\n","resumed_batch = None\n","if saved_iterator_state is not None:\n","  try:\n","    resumed_batch = next(iterator2)\n","    print(f\"Resumed batch (from iterator2 after set_state) - first original_index: {resumed_batch['original_index'][0]}\")\n","  except StopIteration:\n","    print(\"iterator2 exhausted immediately after set_state. This means the saved state was at the very end.\")\n","\n","# 7. Verify\n","if expected_next_batch_from_iterator1 is not None and resumed_batch is not None:\n","  # Compare 'image' data and 'original_index' of the first element in the batch for robustness\n","  # (Labels might repeat, indices are better for this check if available)\n","  expected_img_sample = expected_next_batch_from_iterator1['image'][0]\n","  resumed_img_sample = resumed_batch['image'][0]\n","  expected_idx_sample = expected_next_batch_from_iterator1['original_index'][0]\n","  resumed_idx_sample = resumed_batch['original_index'][0]\n","  are_indices_identical = (expected_idx_sample == resumed_idx_sample)\n","  are_images_identical = np.allclose(expected_img_sample, resumed_img_sample)\n","\n","  are_identical = are_indices_identical and are_images_identical\n","\n","  if are_identical:\n","      print(\"\\nSUCCESS: Resumed batch is identical to the expected next batch. Checkpointing works!\")\n","  else:\n","      print(\"\\nFAILURE: Resumed batch differs from the expected next batch.\")\n","      if not are_indices_identical:\n","          print(f\"  - Mismatch in first original_index: Expected {expected_idx_sample}, Got {resumed_idx_sample}\")\n","      if not are_images_identical:\n","          print(f\"  - Mismatch in image data for first element.\")\n","          # print(f\"    Expected image data (sample [0,0,0]): {expected_img_sample[0,0,0]}\")\n","          # print(f\"    Resumed image data (sample [0,0,0]): {resumed_img_sample[0,0,0]}\")\n","elif saved_iterator_state is not None: # If state was saved but verification couldn't complete\n","  if expected_next_batch_from_iterator1 is None and resumed_batch is None:\n","    print(\"\\nVERIFICATION NOTE: Both iterators seem to be at the end of the dataset after the initial iterations. This is valid if the dataset was short.\")\n","  else:\n","    print(\"\\nVerification inconclusive: could not obtain both expected and resumed batches for comparison.\")\n","    print(f\" expected_next_batch_from_iterator1 is None: {expected_next_batch_from_iterator1 is None}\")\n","    print(f\" resumed_batch is None: {resumed_batch is None}\")\n","else: # If DataLoader itself wasn't configured\n","  print(\"DataLoader for Ex7 not configured.\")"],"metadata":{"id":"vVcRIZdujpFW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","---\n","## Conclusion and Further Exploration\n","\n","Congratulations on completing the exercises! You should now have a good understanding of:\n","*   The fundamental components of Grain (`DataSource`, `Sampler`, `Operations`).\n","*   How to construct and use `grain.DataLoader` for efficient data input, including parallel loading.\n","*   Implementing custom deterministic and random transformations.\n","*   The basics of data sharding for distributed training.\n","*   How Grain iterators fit into a JAX/Flax NNX training loop.\n","*   Saving and restoring data iterator state for reproducibility.\n","\n","**Key Takeaways (Recap from Slides):**\n","*   **Use Grain:** Solves JAX data bottlenecks for better performance.\n","*   **Boost Speed:** Use `DataLoader(worker_count > 0)` for parallelism.\n","*   **Ensure Reproducibility:** Use samplers/seeds & `RandomMapTransform`'s provided `rng`.\n","*   **Distribute:** Use `grain.sharding.ShardByJaxProcess` (or manual `ShardOptions`) for JAX sharding.\n","*   **Save Everything:** Checkpoint data iterator state (e.g., via Orbax for comprehensive checkpointing) along with your model state.\n","\n","**Further Exploration:**\n","*   **Orbax Integration:** For robust checkpointing in real-world projects, explore integrating Grain with [Orbax](https://github.com/google/orbax). Orbax can manage saving and loading your Grain iterator state alongside your Flax model parameters and optimizer states atomically. **Note:** If you are migrating a project from NNX v0.10, be aware that the checkpoint structure for models with RNGs (like `Dropout` or `BatchNorm`) has changed. You will need to use a migration script to update old checkpoints to the v0.11 format, as described in the [official migration guide](https://flax.readthedocs.io/en/latest/migrating/nnx_010_to_nnx_011.html).\n","*   **Different Data Sources:** Explore reading from various on-disk formats (e.g., TFRecord, RecordIO) using appropriate `DataSource` implementations or by integrating with libraries like TFDS.\n","*   **Performance Profiling:** Use JAX's profiling tools to identify and optimize data loading bottlenecks in more complex scenarios.\n","\n","We hope these exercises have been helpful in your journey with JAX and Grain!"],"metadata":{"id":"yDU9idjEmcRD"}}]}