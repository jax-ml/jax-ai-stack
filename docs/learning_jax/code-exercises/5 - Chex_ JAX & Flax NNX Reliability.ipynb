{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"19EP-EMOlRQ9LTWr8KpcGeiiXCKI76lWY","timestamp":1755113898559}],"toc_visible":true,"authorship_tag":"ABX9TyPYH8zwEBLMAY390LkhEEty"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":[" # Chex Exercises: Building Robust JAX & Flax NNX Applications\n","\n"," Welcome! This notebook contains exercises to help you practice using Chex\n"," with JAX and Flax NNX, based on the concepts covered in the lecture.\n","\n"," **Goal:** Solidify your understanding of how Chex enhances reliability and\n"," debuggability in JAX-based projects.\n","\n"," **Instructions:**\n"," 1. Read the problem description for each exercise.\n"," 2. Fill in the `TODO` sections with your code.\n"," 3. Run the cells to test your solutions.\n"," 4. Compare your results with the expected outcomes or hints provided.\n","\n","\n"," Let's get started!"],"metadata":{"id":"qrr2EgAVNLIL"}},{"cell_type":"code","source":["# Run this cell first to install and import necessary libraries.\n","!pip install -Uq flax chex\n","\n","import jax\n","import jax.numpy as jnp\n","import chex\n","import flax\n","from flax import nnx\n","import functools # For functools.partial\n","\n","# Helper to reset trace counter for assert_max_traces exercises\n","def reset_trace_counter():\n","    chex.clear_trace_counter()\n","    # For some JAX versions, a small trick might be needed to fully reset\n","    # internal JAX caches if you're re-running cells aggressively.\n","    # This is usually not needed for these exercises if cells are run in order.\n","\n","print(f\"JAX version: {jax.__version__}\")\n","print(f\"Chex version: {chex.__version__}\")\n","print(f\"Flax version: {flax.__version__}\")\n","print(f\"Running on: {jax.default_backend()}\")"],"metadata":{"id":"J9gmo6dRNGTV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Section 1: Core Chex Assertions\n","Chex provides a suite of assertion functions to validate array properties.\n","Let's practice with the most common ones."],"metadata":{"id":"pJjjwML9Otv-"}},{"cell_type":"markdown","source":["### Exercise 1.1: `chex.assert_shape` and `chex.assert_type`\n","Complete the `process_data` function below.\n","- Add assertions to check if `input_array` has a shape of `(3, None)`\n","  (meaning 3 rows, any number of columns).\n","- Add an assertion to check if `input_array` has a `jnp.float32` dtype.\n","- Add an assertion to check if `output_array` has a shape of `(3, 1)`."],"metadata":{"id":"i_p5oqtqMGFG"}},{"cell_type":"code","source":["def process_data_v1(input_array: chex.Array) -> chex.Array:\n","  \"\"\"Processes an array, asserting shapes and types.\"\"\"\n","  # TODO: Assert input_array shape is (3, None)\n","  chex.assert_shape(input_array, <TODO>)\n","\n","  # TODO: Assert input_array type is jnp.float32\n","  chex.assert_type(<TODO>)\n","\n","  # Simulate some processing that reduces the last dimension to 1\n","  output_array = input_array[:, :1] * 2.0\n","\n","  # TODO: Assert output_array shape is (3, 1)\n","  chex.assert_shape(output_array, (3, 1))\n","\n","  return output_array\n","\n","# Test cases\n","key = jax.random.PRNGKey(0)\n","valid_input = jax.random.normal(key, (3, 5), dtype=jnp.float32)\n","print(\"Testing with valid input...\")\n","result = process_data_v1(valid_input)\n","print(f\"Successfully processed valid input. Output shape: {result.shape}\\n\")\n","\n","print(\"Testing with invalid shape input...\")\n","invalid_shape_input = jax.random.normal(key, (4, 5), dtype=jnp.float32)\n","try:\n","  process_data_v1(invalid_shape_input)\n","except AssertionError as e:\n","  print(f\"Caught expected error for invalid shape:\\n{e}\\n\")\n","\n","print(\"Testing with invalid type input...\")\n","invalid_type_input = jnp.ones((3, 5), dtype=jnp.int32)\n","try:\n","  process_data_v1(invalid_type_input)\n","except AssertionError as e:\n","  print(f\"Caught expected error for invalid type: {e}\\n\")"],"metadata":{"id":"DGvjG5t7N3Ef"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Exercise 1.1 Solution"],"metadata":{"id":"PjTjZN_LL8Rx"}},{"cell_type":"code","source":["def process_data_v1(input_array: chex.Array) -> chex.Array:\n","  \"\"\"Processes an array, asserting shapes and types.\"\"\"\n","  # TODO: Assert input_array shape is (3, None)\n","  chex.assert_shape(input_array, (3, None))\n","\n","  # TODO: Assert input_array type is jnp.float32\n","  chex.assert_type(input_array, expected_types=jnp.float32)\n","\n","  # Simulate some processing that reduces the last dimension to 1\n","  output_array = input_array[:, :1] * 2.0\n","\n","  # TODO: Assert output_array shape is (3, 1)\n","  chex.assert_shape(output_array, (3, 1))\n","\n","  return output_array\n","\n","# Test cases\n","key = jax.random.PRNGKey(0)\n","valid_input = jax.random.normal(key, (3, 5), dtype=jnp.float32)\n","print(\"Testing with valid input...\")\n","result = process_data_v1(valid_input)\n","print(f\"Successfully processed valid input. Output shape: {result.shape}\\n\")\n","\n","print(\"Testing with invalid shape input...\")\n","invalid_shape_input = jax.random.normal(key, (4, 5), dtype=jnp.float32)\n","try:\n","  process_data_v1(invalid_shape_input)\n","except AssertionError as e:\n","  print(f\"Caught expected error for invalid shape:\\n{e}\\n\")\n","\n","print(\"Testing with invalid type input...\")\n","invalid_type_input = jnp.ones((3, 5), dtype=jnp.int32)\n","try:\n","  process_data_v1(invalid_type_input)\n","except AssertionError as e:\n","  print(f\"Caught expected error for invalid type: {e}\\n\")"],"metadata":{"id":"PsVXz6heLLOe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Exercise 1.2: `chex.assert_rank` and `chex.assert_scalar`\n","Complete the `process_data_v2` function.\n","- Add an assertion to ensure `matrix_input` is a 2D array (rank 2).\n","- Add an assertion to ensure `scalar_input` is a scalar.\n","- Add an assertion to ensure the `result` is also a 2D array."],"metadata":{"id":"EH-SjocvR8Yc"}},{"cell_type":"code","source":["def process_data_v2(matrix_input: chex.Array, scalar_input: chex.Array) -> chex.Array:\n","  \"\"\"Processes a matrix and a scalar.\"\"\"\n","  # TODO: Assert matrix_input has rank 2\n","  chex.assert_rank(matrix_input, <TODO>)\n","\n","  # TODO: Assert scalar_input is a scalar\n","  chex.assert_scalar(<TODO>)\n","\n","  result = matrix_input * scalar_input + 1.0\n","\n","  # TODO: Assert result has rank 2\n","  chex.assert_rank(result, <TODO>)\n","  return result\n","\n","# Test cases\n","matrix = jnp.ones((3, 4))\n","scalar = 5.0\n","not_a_scalar = jnp.array([5.0])\n","not_a_matrix = jnp.ones((3,4,1))\n","\n","print(\"Testing with valid rank/scalar inputs...\")\n","try:\n","    res_valid = process_data_v2(matrix, scalar)\n","    print(f\"Successfully processed valid rank/scalar. Result shape: {res_valid.shape}\\n\")\n","except AssertionError as e:\n","    print(f\"Caught unexpected error for valid rank/scalar:\\n{e}\\n\")\n","\n","print(\"Testing with invalid rank input...\")\n","try:\n","  process_data_v2(not_a_matrix, scalar)\n","  print(f\"Successfully processed invalid rank. Result shape: {res_valid.shape}\\n\")\n","except AssertionError as e:\n","  print(f\"Caught expected error for invalid rank:\\n{e}\\n\")\n","\n","print(\"Testing with non-scalar input...\")\n","try:\n","  process_data_v2(matrix, not_a_scalar)\n","  print(f\"Successfully processed non-scalar. Result shape: {res_valid.shape}\\n\")\n","except AssertionError as e:\n","  print(f\"Caught expected error for non-scalar:\\n{e}\\n\")"],"metadata":{"id":"YWljMNCWPgh9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Exercise 1.2 Solution"],"metadata":{"id":"RUUveZp7MkKZ"}},{"cell_type":"code","source":["def process_data_v2(matrix_input: chex.Array, scalar_input: chex.Array) -> chex.Array:\n","  \"\"\"Processes a matrix and a scalar.\"\"\"\n","  # TODO: Assert matrix_input has rank 2\n","  chex.assert_rank(matrix_input, expected_ranks=2)\n","\n","  # TODO: Assert scalar_input is a scalar\n","  chex.assert_scalar(scalar_input)\n","\n","  result = matrix_input * scalar_input + 1.0\n","\n","  # TODO: Assert result has rank 2\n","  chex.assert_rank(result, expected_ranks=2)\n","  return result\n","\n","# Test cases\n","matrix = jnp.ones((3, 4))\n","scalar = 5.0\n","not_a_scalar = jnp.array([5.0])\n","not_a_matrix = jnp.ones((3,4,1))\n","\n","print(\"Testing with valid rank/scalar inputs...\")\n","try:\n","    res_valid = process_data_v2(matrix, scalar)\n","    print(f\"Successfully processed valid rank/scalar. Result shape: {res_valid.shape}\\n\")\n","except AssertionError as e:\n","    print(f\"Caught unexpected error for valid rank/scalar:\\n{e}\\n\")\n","\n","print(\"Testing with invalid rank input...\")\n","try:\n","  process_data_v2(not_a_matrix, scalar)\n","  print(f\"Successfully processed invalid rank. Result shape: {res_valid.shape}\\n\")\n","except AssertionError as e:\n","  print(f\"Caught expected error for invalid rank:\\n{e}\\n\")\n","\n","print(\"Testing with non-scalar input...\")\n","try:\n","  process_data_v2(matrix, not_a_scalar)\n","  print(f\"Successfully processed non-scalar. Result shape: {res_valid.shape}\\n\")\n","except AssertionError as e:\n","  print(f\"Caught expected error for non-scalar:\\n{e}\\n\")"],"metadata":{"id":"GLXFFI7XMpT0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Exercise 1.3: PyTree Assertions (`assert_trees_all_close`, `assert_tree_all_finite`)\n","PyTrees (nested structures of arrays, like model parameters) are common in JAX.\n","Chex provides assertions for them.\n"],"metadata":{"id":"FHwEQLHvTjdj"}},{"cell_type":"code","source":["def process_pytree(tree1, tree2):\n","  \"\"\"\n","  Checks if two PyTrees are close and if the first tree is finite.\n","  Returns a new tree where elements are tree1 + tree2.\n","  \"\"\"\n","  # TODO: Assert tree1 and tree2 are (close to) equal. Use a small tolerance.\n","  chex.assert_trees_all_close(<TODO> rtol=1e-5, atol=1e-8)\n","\n","  # TODO: Assert all elements in tree1 are finite (not NaN or Inf).\n","  chex.assert_tree_all_finite(<TODO>)\n","\n","  # Perform some operation\n","  return jax.tree_util.tree_map(lambda x, y: x + y, tree1, tree2)\n","\n","# Test cases\n","tree_a = {'params': {'w': jnp.array([1.0, 2.0]), 'b': jnp.array(0.5)}}\n","tree_b_close = {'params': {'w': jnp.array([1.000001, 2.000001]), 'b': jnp.array(0.500001)}}\n","tree_c_not_close = {'params': {'w': jnp.array([1.1, 2.1]), 'b': jnp.array(0.6)}}\n","tree_d_nan = {'params': {'w': jnp.array([1.0, jnp.nan]), 'b': jnp.array(0.5)}}\n","\n","print(\"Testing with close and finite PyTrees...\")\n","result_valid = process_pytree(tree_a, tree_b_close)\n","print(\"Successfully processed valid PyTrees.\\n\")\n","\n","print(\"Testing with non-close PyTrees...\")\n","try:\n","  process_pytree(tree_a, tree_c_not_close)\n","except AssertionError as e:\n","  print(f\"Caught expected error for non-close trees:\\n\\n{e}\\n\")\n","\n","print(\"Testing with non-finite PyTree...\")\n","try:\n","  process_pytree(tree_d_nan, tree_b_close) # tree_d_nan will be checked for finiteness\n","except AssertionError as e:\n","  print(f\"Caught expected error for non-finite tree:\\n\\n{e}\\n\")"],"metadata":{"id":"NWxlO6o6Sczm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Exercise 1.3 Solution"],"metadata":{"id":"d0d-QNyBNDhO"}},{"cell_type":"code","source":["def process_pytree(tree1, tree2):\n","  \"\"\"\n","  Checks if two PyTrees are close and if the first tree is finite.\n","  Returns a new tree where elements are tree1 + tree2.\n","  \"\"\"\n","  # TODO: Assert tree1 and tree2 are (close to) equal. Use a small tolerance.\n","  chex.assert_trees_all_close(tree1, tree2, rtol=1e-5, atol=1e-8)\n","\n","  # TODO: Assert all elements in tree1 are finite (not NaN or Inf).\n","  chex.assert_tree_all_finite(tree1)\n","\n","  # Perform some operation\n","  return jax.tree_util.tree_map(lambda x, y: x + y, tree1, tree2)\n","\n","# Test cases\n","tree_a = {'params': {'w': jnp.array([1.0, 2.0]), 'b': jnp.array(0.5)}}\n","tree_b_close = {'params': {'w': jnp.array([1.000001, 2.000001]), 'b': jnp.array(0.500001)}}\n","tree_c_not_close = {'params': {'w': jnp.array([1.1, 2.1]), 'b': jnp.array(0.6)}}\n","tree_d_nan = {'params': {'w': jnp.array([1.0, jnp.nan]), 'b': jnp.array(0.5)}}\n","\n","print(\"Testing with close and finite PyTrees...\")\n","result_valid = process_pytree(tree_a, tree_b_close)\n","print(\"Successfully processed valid PyTrees.\\n\")\n","\n","print(\"Testing with non-close PyTrees...\")\n","try:\n","  process_pytree(tree_a, tree_c_not_close)\n","except AssertionError as e:\n","  print(f\"Caught expected error for non-close trees:\\n\\n{e}\\n\")\n","\n","print(\"Testing with non-finite PyTree...\")\n","try:\n","  process_pytree(tree_d_nan, tree_b_close) # tree_d_nan will be checked for finiteness\n","except AssertionError as e:\n","  print(f\"Caught expected error for non-finite tree:\\n\\n{e}\\n\")"],"metadata":{"id":"FjbrC5EENG_D"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Section 2: Chex Assertions with JAX Transformations\n","A key strength of Chex is that its assertions work correctly inside JAX transformations like `jax.jit` and `jax.vmap`.\n","\n","### Exercise 2.1: Assertions inside `@jax.jit`\n","- Take the `process_data_v1` function from Exercise 1.1.\n","- JIT-compile it and verify that the Chex assertions still work as expected."],"metadata":{"id":"2fRr4rjVVQx9"}},{"cell_type":"code","source":["@jax.jit\n","def process_data_jitted(input_array: chex.Array) -> chex.Array:\n","  \"\"\"JIT-compiled version of process_data_v1 with its Chex assertions.\"\"\"\n","  # (Assertions are inside process_data_v1, which we'll effectively re-use here)\n","  # For clarity, let's re-define it with assertions directly here.\n","  chex.assert_shape(input_array, (3, None))\n","  chex.assert_type(input_array, jnp.float32)\n","  output_array = input_array[:, :1] * 2.0\n","  chex.assert_shape(output_array, (3, 1))\n","  return output_array\n","\n","# Test cases for JIT version\n","key = jax.random.PRNGKey(1) # Use a different key for potentially different values\n","valid_input_jit = jax.random.normal(key, (3, 5), dtype=jnp.float32)\n","print(\"Testing JITted function with valid input...\")\n","\n","# First call will compile\n","result_jit = process_data_jitted(<TODO>)\n","print(f\"Successfully processed JITted valid input. Output shape: {result_jit.shape}\")\n","\n","# Second call uses cached compilation\n","result_jit_cached = process_data_jitted(<TODO> * 2)\n","print(f\"Successfully processed JITted valid input (cached). Output shape: {result_jit_cached.shape}\\n\")\n","\n","print(\"Testing JITted function with invalid shape input...\")\n","invalid_shape_input_jit = jax.random.normal(key, (4, 5), dtype=jnp.float32)\n","try:\n","  process_data_jitted(<TODO>)\n","except AssertionError as e:\n","  print(f\"Caught expected JITted error for invalid shape:\\n\\n{e}\\n\")"],"metadata":{"id":"1u8_6x-wUJOi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Exercise 2.1 Solution"],"metadata":{"id":"inyiNeL3N7VD"}},{"cell_type":"code","source":["@jax.jit\n","def process_data_jitted(input_array: chex.Array) -> chex.Array:\n","  \"\"\"JIT-compiled version of process_data_v1 with its Chex assertions.\"\"\"\n","  # (Assertions are inside process_data_v1, which we'll effectively re-use here)\n","  # For clarity, let's re-define it with assertions directly here.\n","  chex.assert_shape(input_array, (3, None))\n","  chex.assert_type(input_array, jnp.float32)\n","  output_array = input_array[:, :1] * 2.0\n","  chex.assert_shape(output_array, (3, 1))\n","  return output_array\n","\n","# Test cases for JIT version\n","key = jax.random.PRNGKey(1) # Use a different key for potentially different values\n","valid_input_jit = jax.random.normal(key, (3, 5), dtype=jnp.float32)\n","print(\"Testing JITted function with valid input...\")\n","\n","# First call will compile\n","result_jit = process_data_jitted(valid_input_jit)\n","print(f\"Successfully processed JITted valid input. Output shape: {result_jit.shape}\")\n","\n","# Second call uses cached compilation\n","result_jit_cached = process_data_jitted(valid_input_jit * 2)\n","print(f\"Successfully processed JITted valid input (cached). Output shape: {result_jit_cached.shape}\\n\")\n","\n","print(\"Testing JITted function with invalid shape input...\")\n","invalid_shape_input_jit = jax.random.normal(key, (4, 5), dtype=jnp.float32)\n","try:\n","  process_data_jitted(invalid_shape_input_jit)\n","except AssertionError as e:\n","  print(f\"Caught expected JITted error for invalid shape:\\n\\n{e}\\n\")"],"metadata":{"id":"5NluMK76OACI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Observation:**\n","\n","Chex assertions work seamlessly within JITted functions, catching errors based on the concrete values passed during runtime, even though the checks are defined within the compiled code.\n","\n","---\n","### Exercise 2.2: Multi-Level Validation with `@jax.vmap`\n","We want to process a batch of items. Each item is a 1D array of shape `(10,)`.\n","\n","1. Define `process_single_item_vmap` that processes one item.\n","   - Inside this function, assert the `item` has shape `(10,)`.\n","   - The function should double the item's values.\n","   - Assert the `result` (output of `process_single_item_vmap`) also has shape `(10,)`.\n","2. Use `jax.vmap` to create `process_batch`.\n","3. Before calling `process_batch`, assert the `batch_input` has shape `(BATCH_SIZE, 10)`.\n","4. After calling `process_batch`, assert the `batch_output` has shape `(BATCH_SIZE, 10)`."],"metadata":{"id":"m_emU0mfWB4F"}},{"cell_type":"code","source":["BATCH_SIZE = 5\n","ITEM_SIZE = 10\n","\n","def process_single_item_vmap(item: chex.Array) -> chex.Array:\n","  \"\"\"Processes a single item, asserting its shape.\"\"\"\n","  # TODO: Assert shape of a SINGLE item is (ITEM_SIZE,)\n","  chex.assert_shape(item, <TODO>)\n","  result = item * 2.0\n","  # TODO: Assert shape of single item output is (ITEM_SIZE,)\n","  chex.assert_shape(result, <TODO>)\n","  return result\n","\n","# TODO: Vectorize the process_single_item_vmap function using jax.vmap\n","process_batch = jax.vmap(<TODO>, in_axes=0, out_axes=0)\n","\n","# Test cases\n","key = jax.random.PRNGKey(2)\n","valid_batch_input = jax.random.normal(key, (BATCH_SIZE, ITEM_SIZE))\n","invalid_batch_input_item_shape = jax.random.normal(key, (BATCH_SIZE, ITEM_SIZE + 1))\n","\n","print(\"Testing vmap with valid batch input...\")\n","# TODO: Assert shape of the full BATCHED input BEFORE vmap\n","chex.assert_shape(valid_batch_input, <TODO>)\n","\n","batch_output = process_batch(valid_batch_input)\n","\n","# TODO: Assert shape of the full BATCHED output AFTER vmap\n","chex.assert_shape(batch_output, <TODO>)\n","print(f\"Vmap assertion passed. Output shape: {batch_output.shape}\\n\")\n","\n","print(\"Testing vmap with invalid item shape in batch (error from inside vmap)...\")\n","try:\n","  # This will fail inside the vmapped function 'process_single_item_vmap'\n","  process_batch(invalid_batch_input_item_shape)\n","except AssertionError as e:\n","  print(f\"Caught expected vmap error (from inner function):\\n{e}\\n\")\n","\n","print(\"Testing vmap with invalid batch shape (error from outer assertion)...\")\n","invalid_batch_input_outer_shape = jax.random.normal(key, (BATCH_SIZE + 1, ITEM_SIZE))\n","try:\n","  # This will fail the assertion *before* calling process_batch\n","  chex.assert_shape(invalid_batch_input_outer_shape, (BATCH_SIZE, ITEM_SIZE)) # This line will fail\n","  process_batch(invalid_batch_input_outer_shape)\n","except AssertionError as e:\n","  print(f\"Caught expected vmap error (from outer assertion):\\n{e}\\n\")"],"metadata":{"id":"NVbMomrdV4wZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Exercise 2.2 Solution"],"metadata":{"id":"6Mja-kBpOoSc"}},{"cell_type":"code","source":["BATCH_SIZE = 5\n","ITEM_SIZE = 10\n","\n","def process_single_item_vmap(item: chex.Array) -> chex.Array:\n","  \"\"\"Processes a single item, asserting its shape.\"\"\"\n","  # TODO: Assert shape of a SINGLE item is (ITEM_SIZE,)\n","  chex.assert_shape(item, (ITEM_SIZE,))\n","  result = item * 2.0\n","  # TODO: Assert shape of single item output is (ITEM_SIZE,)\n","  chex.assert_shape(result, (ITEM_SIZE,))\n","  return result\n","\n","# TODO: Vectorize the function using jax.vmap\n","process_batch = jax.vmap(process_single_item_vmap, in_axes=0, out_axes=0)\n","\n","# Test cases\n","key = jax.random.PRNGKey(2)\n","valid_batch_input = jax.random.normal(key, (BATCH_SIZE, ITEM_SIZE))\n","invalid_batch_input_item_shape = jax.random.normal(key, (BATCH_SIZE, ITEM_SIZE + 1))\n","\n","print(\"Testing vmap with valid batch input...\")\n","# TODO: Assert shape of the full BATCHED input BEFORE vmap\n","chex.assert_shape(valid_batch_input, (BATCH_SIZE, ITEM_SIZE))\n","\n","batch_output = process_batch(valid_batch_input)\n","\n","# TODO: Assert shape of the full BATCHED output AFTER vmap\n","chex.assert_shape(batch_output, (BATCH_SIZE, ITEM_SIZE))\n","print(f\"Vmap assertion passed. Output shape: {batch_output.shape}\\n\")\n","\n","print(\"Testing vmap with invalid item shape in batch (error from inside vmap)...\")\n","try:\n","  # This will fail inside the vmapped function 'process_single_item_vmap'\n","  process_batch(invalid_batch_input_item_shape)\n","except AssertionError as e:\n","  print(f\"Caught expected vmap error (from inner function):\\n{e}\\n\")\n","\n","print(\"Testing vmap with invalid batch shape (error from outer assertion)...\")\n","invalid_batch_input_outer_shape = jax.random.normal(key, (BATCH_SIZE + 1, ITEM_SIZE))\n","try:\n","  # This will fail the assertion *before* calling process_batch\n","  chex.assert_shape(invalid_batch_input_outer_shape, (BATCH_SIZE, ITEM_SIZE)) # This line will fail\n","  process_batch(invalid_batch_input_outer_shape)\n","except AssertionError as e:\n","  print(f\"Caught expected vmap error (from outer assertion):\\n{e}\\n\")"],"metadata":{"id":"_ma-wKQgOs-a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Section 3: Chex with Flax NNX\n","Neural networks are complex, making validation crucial. Chex integrates naturally into Flax NNX Modules, typically within the `__call__` method.\n","\n","### Exercise 3.1: Input/Output Validation in an NNX Module\n","Complete the `SimpleMLP` module:\n","- In `__call__`, validate the input `x`:\n","   - Must be 2D (`[batch, features]`).\n","   - The feature dimension (axis 1) must match `self.linear1.in_features`.\n","   - Type must be `jnp.float32`.\n","- In `__call__`, validate the output `x` before returning:\n","   - Must be 2D.\n","   - The feature dimension (axis 1) must match `self.linear2.out_features`."],"metadata":{"id":"cDJ1et86XGLe"}},{"cell_type":"code","source":["class SimpleMLP(nnx.Module):\n","  def __init__(self, din: int, dmid: int, dout: int, *, rngs: nnx.Rngs):\n","    self.linear1 = nnx.Linear(din, dmid, rngs=rngs)\n","    self.linear2 = nnx.Linear(dmid, dout, rngs=rngs)\n","\n","  def __call__(self, x: chex.Array) -> chex.Array:\n","    # TODO: Validate input x\n","    # - Must be 2D ([batch, features])\n","    chex.assert_rank(<TODO>)\n","    # - Feature dimension (axis 1) must match self.linear1.in_features\n","    chex.assert_axis_dimension(x, 1, <TODO>)\n","    # - Type must be jnp.float32\n","    chex.assert_type(x, <TODO>)\n","\n","    # Forward pass\n","    x = self.linear1(x)\n","    x = nnx.relu(x)\n","    x = self.linear2(x)\n","\n","    # TODO: Validate output x before returning\n","    # - Must be 2D\n","    chex.assert_rank(<TODO>)\n","    # - Feature dimension (axis 1) must match self.linear2.out_features\n","    chex.assert_axis_dimension(x, 1, self.linear2.out_features)\n","\n","    return x\n","\n","# Test cases for SimpleMLP\n","key_nnx = nnx.Rngs(params=jax.random.key(0)) # NNX Rngs for stateful operations\n","din, dmid, dout = 10, 20, 5\n","batch_size_nnx = 4\n","\n","model = SimpleMLP(din, dmid, dout, rngs=key_nnx)\n","\n","print(\"Testing NNX Module with valid input:\")\n","x_valid_nnx = jnp.ones((batch_size_nnx, din), dtype=jnp.float32)\n","output_nnx = model(x_valid_nnx)\n","print(f\"NNX I/O Check passed. Output shape: {output_nnx.shape}\\n\")\n","\n","\n","print(\"Testing NNX Module with invalid input rank:\")\n","x_invalid_rank_nnx = jnp.ones((batch_size_nnx, din, 1), dtype=jnp.float32)\n","try:\n","  model(x_invalid_rank_nnx)\n","except AssertionError as e:\n","  print(f\"Caught expected NNX error (invalid input rank):\\n{e}\\n\")\n","\n","print(\"Testing NNX Module with invalid input feature dimension:\")\n","x_invalid_feat_nnx = jnp.ones((batch_size_nnx, din + 1), dtype=jnp.float32)\n","try:\n","  model(x_invalid_feat_nnx)\n","except AssertionError as e:\n","  print(f\"Caught expected NNX error (invalid input features):\\n{e}\\n\")\n","\n","print(\"Testing NNX Module with invalid input type:\")\n","x_invalid_type_nnx = jnp.ones((batch_size_nnx, din), dtype=jnp.int32)\n","try:\n","  model(x_invalid_type_nnx)\n","except AssertionError as e:\n","  print(f\"Caught expected NNX error (invalid input type):\\n{e}\\n\")"],"metadata":{"id":"eu3llFxAWkLG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Exercise 3.1 Solution"],"metadata":{"id":"XrxRhSFAPWti"}},{"cell_type":"code","source":["class SimpleMLP(nnx.Module):\n","  def __init__(self, din: int, dmid: int, dout: int, *, rngs: nnx.Rngs):\n","    self.linear1 = nnx.Linear(din, dmid, rngs=rngs)\n","    self.linear2 = nnx.Linear(dmid, dout, rngs=rngs)\n","\n","  def __call__(self, x: chex.Array) -> chex.Array:\n","    # TODO: Validate input x\n","    # - Must be 2D ([batch, features])\n","    chex.assert_rank(x, 2)\n","    # - Feature dimension (axis 1) must match self.linear1.in_features\n","    chex.assert_axis_dimension(x, 1, self.linear1.in_features)\n","    # - Type must be jnp.float32\n","    chex.assert_type(x, jnp.float32)\n","\n","    # Forward pass\n","    x = self.linear1(x)\n","    x = nnx.relu(x)\n","    x = self.linear2(x)\n","\n","    # TODO: Validate output x before returning\n","    # - Must be 2D\n","    chex.assert_rank(x, 2)\n","    # - Feature dimension (axis 1) must match self.linear2.out_features\n","    chex.assert_axis_dimension(x, 1, self.linear2.out_features)\n","\n","    return x\n","\n","# Test cases for SimpleMLP\n","key_nnx = nnx.Rngs(params=jax.random.key(0)) # NNX Rngs for stateful operations\n","din, dmid, dout = 10, 20, 5\n","batch_size_nnx = 4\n","\n","model = SimpleMLP(din, dmid, dout, rngs=key_nnx)\n","\n","print(\"Testing NNX Module with valid input:\")\n","x_valid_nnx = jnp.ones((batch_size_nnx, din), dtype=jnp.float32)\n","output_nnx = model(x_valid_nnx)\n","print(f\"NNX I/O Check passed. Output shape: {output_nnx.shape}\\n\")\n","\n","\n","print(\"Testing NNX Module with invalid input rank:\")\n","x_invalid_rank_nnx = jnp.ones((batch_size_nnx, din, 1), dtype=jnp.float32)\n","try:\n","  model(x_invalid_rank_nnx)\n","except AssertionError as e:\n","  print(f\"Caught expected NNX error (invalid input rank):\\n{e}\\n\")\n","\n","print(\"Testing NNX Module with invalid input feature dimension:\")\n","x_invalid_feat_nnx = jnp.ones((batch_size_nnx, din + 1), dtype=jnp.float32)\n","try:\n","  model(x_invalid_feat_nnx)\n","except AssertionError as e:\n","  print(f\"Caught expected NNX error (invalid input features):\\n{e}\\n\")\n","\n","print(\"Testing NNX Module with invalid input type:\")\n","x_invalid_type_nnx = jnp.ones((batch_size_nnx, din), dtype=jnp.int32)\n","try:\n","  model(x_invalid_type_nnx)\n","except AssertionError as e:\n","  print(f\"Caught expected NNX error (invalid input type):\\n{e}\\n\")"],"metadata":{"id":"uKHHvhcuPcH2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Self-reflection:**\n","\n","How would these assertions help you catch bugs early when composing multiple layers or changing model configurations? They act as contracts between layers and for the model's external API."],"metadata":{"id":"ZgmPRssPtHbB"}},{"cell_type":"markdown","source":["---\n","### üèÜ Congratulations!\n","You've completed the Chex exercises. You should now have a better understanding of:\n","- Using core Chex assertions for shapes, types, ranks, and PyTrees.\n","- How Chex assertions behave within `jax.jit` and `jax.vmap`.\n","- The purpose and usage of `@chex.chexify` (and its caveats).\n","- Detecting recompilation issues with `@chex.assert_max_traces`.\n","- Integrating Chex assertions into Flax NNX modules for robust model development.\n","\n","Using Chex consistently can significantly improve the reliability and\n","maintainability of your JAX projects.\n","\n","**Further Exploration (Optional):**\n","- Explore using `chex.chexify` outside of a Colab environment.\n","- Explore other Chex assertions not covered here (e.g., `chex.assert_devices_available`).\n","- Look into Chex testing utilities like `@chex.variants` if you write comprehensive test suites.\n","- Consider when and where to add Chex assertions in a typical training loop."],"metadata":{"id":"2zIbByO9tlqf"}}]}