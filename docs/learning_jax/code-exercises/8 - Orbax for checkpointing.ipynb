{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"13nlRxnUp2Y8sLDpqVQDjMSs_soWB-JWQ","timestamp":1755114004925}],"toc_visible":true,"authorship_tag":"ABX9TyO1TCkH6hEPMVvV1U3DTETQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Introduction to Checkpointing with Flax NNX and Orbax\n","\n","Welcome to this hands-on exercise! We'll explore how to save and load your JAX/Flax NNX models, a crucial skill for any serious machine learning project.\n","\n","## Why Checkpoint?\n","Training deep learning models can take a long time. Checkpointing allows you to:\n","\n","* Save your progress (model parameters, optimizer state) to resume training later if it gets interrupted.\n","* Preserve model states at different stages for analysis or inference.\n","* Implement fault tolerance in long training runs.\n","\n","## Flax NNX: A Quick Recap\n","\n","* **Stateful Modules**: NNX modules are Python classes that directly hold their own state (like parameters) as attributes. This often feels more intuitive, especially if you're coming from PyTorch.\n","* `nnx.Module`: The base class for creating these stateful components.\n","* `nnx.Variable`: Special types like nnx.Param and nnx.BatchStat are used to define learnable parameters and other stateful variables within an nnx.Module.\n","* `nnx.State`: A JAX Pytree (like a nested dictionary) that holds all the nnx.Variable values from a module. This is what Orbax saves and restores.\n","\n","## The Functional Bridge:\n","\n","* `nnx.split(module)`: Separates a module into its static structure (GraphDef) and its dynamic state (nnx.State). This is key for getting the state to save.\n","* `nnx.merge(graphdef, state)`: Reconstructs a module instance from its GraphDef and nnx.State. Used after restoring.\n","* `nnx.update(module, state)`: Updates an existing module's state in-place. Also used after restoring.\n","\n","## Orbax: The JAX Checkpointing Library\n","\n","Orbax is the standard library for checkpointing in JAX, designed to be robust and scalable.\n","\n","* `ocp.CheckpointManager`: A high-level utility that simplifies managing multiple checkpoints over a training run (e.g., keeping the last N checkpoints, handling versions). We'll be using this extensively.\n","* `ocp.args`: Namespace for specifying how to save/restore different parts of your state (e.g., ocp.args.StandardSave, ocp.args.StandardRestore, ocp.args.Composite).\n","\n","Let's get started!"],"metadata":{"id":"LCP8B52UdhGP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0dHcdEQETEfy"},"outputs":[],"source":["# @title Setup: Install and Import Libraries\n","# Install necessary libraries\n","!pip install -Uq flax orbax-checkpoint chex optax\n","\n","import jax\n","import jax.numpy as jnp\n","from jax.sharding import Mesh, PartitionSpec, NamedSharding\n","from jax.experimental import mesh_utils\n","import flax\n","from flax import nnx\n","import orbax.checkpoint as ocp\n","import optax\n","import os\n","import shutil # For cleaning up directories\n","import chex # For faking devices\n","\n","# Suppress some JAX warnings for cleaner output in the notebook\n","import warnings\n","warnings.filterwarnings(\"ignore\", message=\"No GPU/TPU found, falling back to CPU.\")\n","warnings.filterwarnings(\"ignore\", message=\"Custom node type GlobalDeviceArray is not handled by Pytree traversal.\") # Orbax/NNX interactions\n","\n","print(f\"JAX version: {jax.__version__}\")\n","print(f\"Flax version: {flax.__version__}\")\n","print(f\"Orbax version: {ocp.__version__}\")\n","print(f\"Optax version: {optax.__version__}\")\n","print(f\"Chex version: {chex.__version__}\")\n","\n","# --- Setup for Distributed Exercises ---\n","# Simulate an environment with 8 CPUs for distributed examples\n","# This allows us to test sharding logic even on a single-CPU Colab machine.\n","try:\n","  chex.set_n_cpu_devices(8)\n","except RuntimeError as e:\n","  print(f\"Note: Could not set_n_cpu_devices (may have been set already): {e}\")\n","\n","print(f\"Number of JAX devices available: {jax.device_count()}\")\n","print(f\"Available devices: {jax.devices()}\")\n","\n","# Helper function to clean up checkpoint directories\n","def cleanup_ckpt_dir(ckpt_dir):\n","  if os.path.exists(ckpt_dir):\n","    shutil.rmtree(ckpt_dir)\n","    print(f\"Cleaned up checkpoint directory: {ckpt_dir}\")\n","\n","# Create a default checkpoint directory for exercises\n","CKPT_BASE_DIR = '/tmp/nnx_orbax_workshop_checkpoints'\n","if not os.path.exists(CKPT_BASE_DIR):\n","  os.makedirs(CKPT_BASE_DIR)\n","\n","print(f\"Base checkpoint directory: {CKPT_BASE_DIR}\")"]},{"cell_type":"markdown","source":["## Exercise 1: Basic Checkpointing - Saving nnx.State\n","\n","**Goal**: Learn to save the state of a simple Flax NNX module using Orbax.\n","\n","### Topics:\n","\n","* Defining an nnx.Module.\n","* Instantiating an nnx.Module with initial parameters.\n","* Using nnx.split() to extract the nnx.State Pytree.\n","* Setting up ocp.CheckpointManager.\n","* Saving the state using mngr.save() with ocp.args.StandardSave.\n","\n","### Instructions:\n","\n","1. Define a simple linear layer SimpleLinear that inherits from nnx.Module.\n"," - In its __init__, define a weight matrix and a bias vector as nnx.Param attributes. Initialize them with JAX random functions (e.g., jax.random.uniform for weights, jnp.zeros for bias). Remember nnx.Rngs for key management!\n"," - Implement the __call__ method for the forward pass: y = x @ weight + bias.\n","2. Instantiate this SimpleLinear module.\n","3. Specify a directory for saving checkpoints.\n","4. Create an ocp.CheckpointManagerOptions object to configure checkpointing (e.g., max_to_keep=3).\n","5. Instantiate ocp.CheckpointManager with the directory and options.\n","6. Use nnx.split(model) to get the graphdef and the state_to_save.\n","7. Save the state_to_save at a specific training step (e.g., step 100) using mngr.save(). You'll need to wrap state_to_save with ocp.args.StandardSave().\n","8. Call mngr.wait_until_finished() to ensure the save operation completes (important if saving is asynchronous).\n","9. Close the manager using mngr.close()."],"metadata":{"id":"H__qBFj1eyVK"}},{"cell_type":"code","source":["# --- Define the NNX Module ---\n","class SimpleLinear(nnx.Module):\n","  def __init__(self, din: int, dout: int, *, rngs: nnx.Rngs):\n","    key_w, key_b = rngs.params(), rngs.params() # Example of splitting keys if needed, or use one key for multiple params\n","    # TODO: Define self.weight as an nnx.Param with shape (din, dout)\n","    # self.weight = ...\n","    # TODO: Define self.bias as an nnx.Param with shape (dout,)\n","    # self.bias = ...\n","\n","  def __call__(self, x: jax.Array) -> jax.Array:\n","    # TODO: Implement the forward pass\n","    # return ...\n","\n","# --- Instantiate the Model ---\n","din, dout = 10, 5\n","# TODO: Create an nnx.Rngs object for parameter initialization\n","# rngs = ...\n","# TODO: Instantiate SimpleLinear\n","# model = ...\n","\n","print(f\"Model created. Weight shape: {model.weight.value.shape}, Bias shape: {model.bias.value.shape}\")\n","\n","# --- Setup CheckpointManager ---\n","ckpt_dir_ex1 = os.path.join(CKPT_BASE_DIR, 'ex1_basic_save')\n","cleanup_ckpt_dir(ckpt_dir_ex1) # Clean up from previous runs\n","\n","# TODO: Create CheckpointManagerOptions\n","# options = ...\n","# TODO: Instantiate CheckpointManager\n","# mngr = ...\n","\n","# --- Split the model to get the state ---\n","# TODO: Split the model into graphdef and state_to_save\n","# _graphdef, state_to_save = ...\n","# Alternatively, for just the state: state_to_save = nnx.state(model)\n","# print(f\"State to save: {jax.tree_util.tree_map(lambda x: x.shape if hasattr(x, 'shape') else x, state_to_save)}\")\n","\n","# --- Save the state ---\n","step = 100\n","# TODO: Save the state_to_save at the given step. Use ocp.args.StandardSave.\n","# mngr.save(...)\n","# TODO: Wait for saving to complete\n","# mngr.wait_until_finished()\n","\n","print(f\"Checkpoint saved for step {step} in {ckpt_dir_ex1}.\")\n","print(f\"Available checkpoints: {mngr.all_steps()}\")\n","\n","# TODO: Close the manager\n","# mngr.close()"],"metadata":{"id":"5jXjWVyVdF5G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Exercise 1: Solution\n","# --- Define the NNX Module ---\n","class SimpleLinear(nnx.Module):\n","  def __init__(self, din: int, dout: int, *, rngs: nnx.Rngs):\n","    # Parameters defined using nnx.Param (a type of nnx.Variable)\n","    self.weight = nnx.Param(jax.random.uniform(rngs.params(), (din, dout)))\n","    self.bias = nnx.Param(jnp.zeros((dout,)))\n","\n","  def __call__(self, x: jax.Array) -> jax.Array:\n","    # Parameters used directly via self.weight, self.bias\n","    return x @ self.weight.value + self.bias.value\n","\n","# --- Instantiate the Model ---\n","din, dout = 10, 5\n","rngs = nnx.Rngs(params=jax.random.key(0)) # NNX requires explicit RNG management\n","model = SimpleLinear(din=din, dout=dout, rngs=rngs)\n","\n","print(f\"Model created. Weight shape: {model.weight.value.shape}, Bias shape: {model.bias.value.shape}\")\n","\n","# --- Setup CheckpointManager ---\n","ckpt_dir_ex1 = os.path.join(CKPT_BASE_DIR, 'ex1_basic_save')\n","cleanup_ckpt_dir(ckpt_dir_ex1)\n","\n","options = ocp.CheckpointManagerOptions(max_to_keep=3, save_interval_steps=1)\n","mngr = ocp.CheckpointManager(ckpt_dir_ex1, options=options)\n","\n","# --- Split the model to get the state ---\n","_graphdef, state_to_save = nnx.split(model)\n","# Alternatively: state_to_save = nnx.state(model)\n","print(f\"State to save structure: {jax.tree_util.tree_map(lambda x: (x.shape, x.dtype) if hasattr(x, 'shape') else type(x), state_to_save)}\")\n","\n","# --- Save the state ---\n","step = 100\n","mngr.save(step, args=ocp.args.StandardSave(state_to_save))\n","mngr.wait_until_finished() # Ensure save completes if async\n","\n","print(f\"Checkpoint saved for step {step} in {ckpt_dir_ex1}.\")\n","print(f\"Available checkpoints: {mngr.all_steps()}\")\n","\n","mngr.close() # Clean up resources"],"metadata":{"id":"N72kLQJff3Ex"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 2: Basic Checkpointing - Restoring nnx.State\n","\n","**Goal**: Learn to restore a model's state from a checkpoint using Orbax.\n","\n","###Topics:\n","\n","* Using nnx.eval_shape() to create an \"abstract\" model template.\n","* Splitting the abstract model to get an abstract_state (a Pytree of ShapeDtypeStruct objects).\n","* Restoring the state using mngr.restore() with the abstract_state and ocp.args.StandardRestore.\n","* Reconstructing the model using nnx.merge() with the original graphdef and the restored_state.\n","* Alternatively, updating an existing model instance with nnx.update().\n","\n","### Instructions:\n","\n","1. Re-open the CheckpointManager pointing to the directory from Exercise 1 (ckpt_dir_ex1).\n","2. Define a function create_abstract_model() that instantiates your SimpleLinear module. This function will be passed to nnx.eval_shape().\n"," - Inside this function, use dummy RNG keys and input shapes as nnx.eval_shape only cares about the structure and dtypes, not actual values.\n","3. Create an abstract_model by calling abstract_model = nnx.eval_shape(create_abstract_model).\n","4. Split the abstract_model using graphdef_for_restore, abstract_state = nnx.split(abstract_model). The abstract_state now contains ShapeDtypeStruct leaves, which Orbax uses as a template for restoration.\n","5. Find the latest checkpoint step using mngr.latest_step().\n","6. If a checkpoint exists, restore the state using mngr.restore(step_to_restore, args=ocp.args.StandardRestore(abstract_state)).\n","7. Reconstruct the model using restored_model = nnx.merge(graphdef_for_restore, restored_state).\n","8. (Optional) Print a value from the restored model (e.g., restored_model.bias.value) to verify.\n","9. Close the manager."],"metadata":{"id":"kdhOVjSOgTKy"}},{"cell_type":"code","source":["# Ensure the SimpleLinear class definition from Exercise 1 is available\n","\n","# --- Re-open CheckpointManager ---\n","# TODO: Instantiate CheckpointManager for ckpt_dir_ex1 (no need for options if just restoring)\n","# mngr_restore = ...\n","\n","# --- Create Abstract Model for Restoration ---\n","def create_abstract_model():\n","  # Use dummy RNG key/inputs for abstract creation\n","  # TODO: Return an instance of SimpleLinear, same din/dout as before\n","  # return ...\n","\n","# TODO: Create the abstract_model using nnx.eval_shape\n","# abstract_model = ...\n","\n","# --- Split Abstract Model to get Abstract State Structure ---\n","# TODO: Split the abstract_model to get graphdef_for_restore and abstract_state\n","# graphdef_for_restore, abstract_state = ...\n","print(f\"Abstract state structure: {jax.tree_util.tree_map(lambda x: (x.shape, x.dtype) if hasattr(x, 'shape') else x, abstract_state)}\")\n","\n","\n","# --- Restore the State ---\n","# TODO: Get the latest step to restore\n","# step_to_restore = ...\n","\n","if step_to_restore is not None:\n","  # TODO: Restore the state using mngr_restore.restore() and ocp.args.StandardRestore with abstract_state\n","  # restored_state = mngr_restore.restore(...)\n","\n","  # --- Reconstruct the Model ---\n","  # TODO: Reconstruct the model using nnx.merge with graphdef_for_restore and restored_state\n","  # restored_model = ...\n","  print(f\"Model restored from step {step_to_restore}.\")\n","  # You can now use 'restored_model'\n","  print(f\"Restored bias (first 3 values): {restored_model.bias.value[:3]}\")\n","\n","  # Alternative: Update an existing model instance\n","  # model_to_update = SimpleLinear(din=din, dout=dout, rngs=nnx.Rngs(params=jax.random.key(99))) # Fresh instance\n","  # nnx.update(model_to_update, restored_state)\n","  # print(f\"Updated model bias (first 3 values): {model_to_update.bias.value[:3]}\")\n","else:\n","  print(\"No checkpoint found to restore.\")\n","\n","# TODO: Close the manager\n","# mngr_restore.close()"],"metadata":{"id":"CugEuqYCgB-5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Exercise 2: Solution\n","\n","# Ensure the SimpleLinear class definition from Exercise 1 is available\n","\n","# --- Re-open CheckpointManager ---\n","mngr_restore = ocp.CheckpointManager(ckpt_dir_ex1) # Re-open manager\n","\n","# --- Create Abstract Model for Restoration ---\n","def create_abstract_model():\n","  # Use dummy RNG key/inputs for abstract creation\n","  return SimpleLinear(din=din, dout=dout, rngs=nnx.Rngs(params=jax.random.key(0))) # din, dout from Ex1\n","\n","abstract_model = nnx.eval_shape(create_abstract_model)\n","\n","# --- Split Abstract Model to get Abstract State Structure ---\n","graphdef_for_restore, abstract_state = nnx.split(abstract_model)\n","# abstract_state now contains ShapeDtypeStruct leaves\n","print(f\"Abstract state structure: {jax.tree_util.tree_map(lambda x: (x.shape, x.dtype) if hasattr(x, 'shape') else x, abstract_state)}\")\n","\n","# --- Restore the State ---\n","step_to_restore = mngr_restore.latest_step()\n","\n","if step_to_restore is not None:\n","  restored_state = mngr_restore.restore(step_to_restore,\n","      args=ocp.args.StandardRestore(abstract_state))\n","  print(f\"Restored state structure: {jax.tree_util.tree_map(lambda x: (x.shape, x.dtype) if hasattr(x, 'shape') else type(x), restored_state)}\")\n","\n","  # --- Reconstruct the Model ---\n","  restored_model = nnx.merge(graphdef_for_restore, restored_state)\n","  print(f\"Model restored from step {step_to_restore}.\")\n","  # You can now use 'restored_model'\n","  print(f\"Restored bias (first 3 values): {restored_model.bias.value[:3]}\")\n","\n","  # Compare with original model's bias (optional, if 'model' from Ex1 is still in scope)\n","  # print(f\"Original bias (first 3 values): {model.bias.value[:3]}\")\n","  # chex.assert_trees_all_close(restored_model.bias.value, model.bias.value)\n","\n","  # Alternative: Update an existing model instance\n","  model_to_update = SimpleLinear(din=din, dout=dout, rngs=nnx.Rngs(params=jax.random.key(99))) # Fresh instance\n","  # Initialize with different values to see update working\n","  model_to_update.bias.value = jnp.ones_like(model_to_update.bias.value) * 55.0\n","  print(f\"Bias before update: {model_to_update.bias.value[:3]}\")\n","  nnx.update(model_to_update, restored_state)\n","  print(f\"Updated model bias (first 3 values): {model_to_update.bias.value[:3]}\")\n","  if 'model' in globals(): # Check if original model exists\n","    chex.assert_trees_all_close(model_to_update.bias.value, model.bias.value)\n","else:\n","  print(\"No checkpoint found to restore.\")\n","\n","mngr_restore.close()"],"metadata":{"id":"QJz_4Pm9g-AG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 3: Saving Model Parameters and Optimizer State\n","\n","**Goal**: Learn to save both model parameters and optimizer state together in a single checkpoint.\n","\n","### Topics:\n","\n","* Using nnx.Optimizer to manage model parameters and an Optax optimizer state.\n","* Extracting model parameters (e.g., using nnx.split(model, nnx.Param)).\n","* Extracting the full optimizer state (nnx.state(optimizer)).\n","* Using ocp.args.Composite to save multiple named items (model params, optimizer state) in one checkpoint.\n","\n","### Instructions:\n","\n","1. Reuse the SimpleLinear module definition. Instantiate a new SimpleLinear model.\n","2. Create an Optax optimizer (e.g., optax.adam(learning_rate=1e-3)).\n","3. Wrap the model and the Optax optimizer with nnx.Optimizer.\n","4. (Optional) Simulate a few training steps to update the optimizer's internal state (e.g., momentum). You don't need actual data; just update the step count and imagine gradients were applied.\n"," - Access optimizer step via optimizer.step.value. Update it: optimizer.step.value += 1.\n","5. Set up a new CheckpointManager in a new directory (ckpt_dir_ex3).\n","6. Extract the model's parameters: _graphdef_params, params_state = nnx.split(model_ex3, nnx.Param). Note that the optimizer.model attribute has been removed, so we split the original model variable directly.\n","7. Extract the full optimizer state: optimizer_state_tree = nnx.state(optimizer). This includes optimizer internal states (like momentum) and its own step count.\n","8. Define a dictionary save_items where keys are names (e.g., 'params', 'optimizer') and values are ocp.args.StandardSave() wrapped Pytrees (i.e., params_state and optimizer_state_tree).\n","9. Save these items using mngr.save(step, args=ocp.args.Composite(**save_items)). Use the optimizer's current step.\n","10. Wait and close the manager."],"metadata":{"id":"9ZhJATu8hkYw"}},{"cell_type":"code","source":["# Ensure SimpleLinear class definition is available\n","# --- Instantiate Model and Optimizer ---\n","rngs_ex3 = nnx.Rngs(params=jax.random.key(1))\n","model_ex3 = SimpleLinear(din=10, dout=5, rngs=rngs_ex3)\n","\n","# TODO: Create an Optax optimizer (e.g., Adam)\n","# tx = ...\n","# TODO: Create an nnx.Optimizer, wrapping the model and tx\n","# optimizer = ...\n","\n","# Simulate a few \"training\" steps to populate optimizer state\n","# For a real scenario, this would involve gradients and updates\n","if hasattr(optimizer, 'step') and hasattr(optimizer.step, 'value'): # Check for NNX Optimizer structure\n","  optimizer.step.value += 10 # Simulate 10 steps\n","  # In a real loop: optimizer.update_fn(grads, optimizer.state) -> optimizer.state would be updated\n","  # For this exercise, just advancing step is enough to see it saved/restored.\n","  # Let's also change a parameter slightly to see it saved\n","  original_bias_val_ex3 = model_ex3.bias.value.copy()\n","  model_ex3.bias.value = model_ex3.bias.value * 0.5 + 0.1\n","  print(f\"Optimizer step: {optimizer.step.value}\")\n","  print(f\"Bias modified. Original first val: {original_bias_val_ex3[0]}, New first val: {model_ex3.bias.value[0]}\")\n","else:\n","  print(\"Skipping optimizer step update as structure might differ from expected nnx.Optimizer.\")\n","\n","\n","# --- Setup CheckpointManager for Composite Save ---\n","ckpt_dir_ex3 = os.path.join(CKPT_BASE_DIR, 'ex3_composite_save')\n","cleanup_ckpt_dir(ckpt_dir_ex3)\n","# TODO: Instantiate CheckpointManager for ckpt_dir_ex3\n","# mngr_comp = ...\n","\n","# --- Extract States for Saving ---\n","# TODO: Extract model parameters state from optimizer.model using nnx.split with nnx.Param filter\n","# _graphdef_params, params_state = ...\n","# TODO: Extract the full optimizer state tree using nnx.state()\n","# optimizer_state_tree = ...\n","\n","print(f\"Parameter state structure: {jax.tree_util.tree_map(lambda x: x.shape if hasattr(x, 'shape') else x, params_state)}\")\n","print(f\"Optimizer state structure: {jax.tree_util.tree_map(lambda x: x.shape if hasattr(x, 'shape') else x, optimizer_state_tree)}\")\n","\n","# --- Save Composite State ---\n","current_step_val = 0\n","if hasattr(optimizer, 'step') and hasattr(optimizer.step, 'value'):\n","  current_step_val = optimizer.step.value\n","else: # Fallback for safety, though nnx.Optimizer should have .step\n","  current_step_val = 10\n","\n","\n","# TODO: Define save_items dictionary for 'params' and 'optimizer'\n","# Each item should be wrapped with ocp.args.StandardSave\n","# save_items = {\n","#     'params': ...,\n","#     'optimizer': ...\n","# }\n","\n","# TODO: Save using mngr_comp.save() and ocp.args.Composite\n","# mngr_comp.save(...)\n","# TODO: Wait and close the manager\n","# mngr_comp.wait_until_finished()\n","# print(f\"Composite checkpoint saved for step {current_step_val} in {ckpt_dir_ex3}.\")\n","# print(f\"Available checkpoints: {mngr_comp.all_steps()}\")\n","# mngr_comp.close()"],"metadata":{"id":"9ZTdYGw3hGJq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Exercise 3: Solution\n","\n","# Ensure SimpleLinear class definition is available\n","# --- Instantiate Model and Optimizer ---\n","rngs_ex3 = nnx.Rngs(params=jax.random.key(1))\n","model_ex3 = SimpleLinear(din=10, dout=5, rngs=rngs_ex3)\n","\n","tx = optax.adam(learning_rate=1e-3)\n","optimizer = nnx.Optimizer(model_ex3, tx, wrt=nnx.Param)\n","\n","# Simulate a few \"training\" steps to populate optimizer state\n","# For a real scenario, this would involve gradients and updates\n","optimizer.step.value += 10 # Simulate 10 steps\n","original_bias_val_ex3 = model_ex3.bias.value.copy()\n","# Simulate a parameter update that would happen during training\n","model_ex3.bias.value = model_ex3.bias.value * 0.5 + 0.1 # Arbitrary change\n","print(f\"Optimizer step: {optimizer.step.value}\")\n","print(f\"Bias modified. Original first val: {original_bias_val_ex3[0]}, New first val: {model_ex3.bias.value[0]}\")\n","\n","# --- Setup CheckpointManager for Composite Save ---\n","ckpt_dir_ex3 = os.path.join(CKPT_BASE_DIR, 'ex3_composite_save')\n","cleanup_ckpt_dir(ckpt_dir_ex3)\n","mngr_comp = ocp.CheckpointManager(ckpt_dir_ex3, options=ocp.CheckpointManagerOptions(max_to_keep=3))\n","\n","# --- Extract States for Saving ---\n","# Extract model parameters (e.g., using nnx.split(model, nnx.Param))\n","_graphdef_params, params_state = nnx.split(model_ex3, nnx.Param)\n","# Extract optimizer state (nnx.state(optimizer))\n","optimizer_state_tree = nnx.state(optimizer)\n","\n","print(f\"Parameter state structure: {jax.tree_util.tree_map(lambda x: (x.shape, x.dtype) if hasattr(x, 'shape') else type(x), params_state)}\")\n","print(f\"Optimizer state structure: {jax.tree_util.tree_map(lambda x: (x.shape, x.dtype) if hasattr(x, 'shape') else type(x), optimizer_state_tree)}\")\n","# Note: optimizer_state_tree also contains the model's state within optimizer.model_variables\n","\n","# --- Save Composite State ---\n","current_step_val = optimizer.step.value # Get current step from optimizer\n","\n","# Save using Composite args\n","save_items = {\n","  'params': ocp.args.StandardSave(params_state),\n","  'optimizer': ocp.args.StandardSave(optimizer_state_tree)\n","}\n","\n","# Can generate args per item using orbax_utils too\n","mngr_comp.save(current_step_val, args=ocp.args.Composite(**save_items))\n","mngr_comp.wait_until_finished()\n","print(f\"Composite checkpoint saved for step {current_step_val} in {ckpt_dir_ex3}.\")\n","print(f\"Available checkpoints: {mngr_comp.all_steps()}\")\n","mngr_comp.close()"],"metadata":{"id":"V0lKNBMQiKBh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 4: Restoring Model Parameters and Optimizer State\n","\n","**Goal**: Learn to restore both model parameters and optimizer state from a composite checkpoint.\n","\n","### Topics:\n","\n","* Creating abstract versions of both model and optimizer using nnx.eval_shape.\n","* Getting abstract state templates for both parameter state and optimizer state.\n","* Using ocp.args.Composite with ocp.args.StandardRestore for restoring multiple items.\n","* Instantiating new concrete model and optimizer instances.\n","* Updating these instances using nnx.update() with the restored states.\n","\n","### Instructions:\n","\n","1. Re-open the CheckpointManager from Exercise 3 (ckpt_dir_ex3).\n","2. Define a function create_abstract_model_and_optimizer():\n"," - Inside, create an abstract model instance (e.g., SimpleLinear) using nnx.eval_shape on a creation lambda.\n"," - Then, create an abstract nnx.Optimizer instance using nnx.eval_shape, passing the abstract model and a new Optax optimizer instance to its creation lambda.\n"," - Return both abs_model and abs_optimizer.\n","3. Call this function to get abs_model and abs_optimizer.\n","4. Get the abstract state for parameters: _graphdef_abs_params, abs_params_state = nnx.split(abs_model, nnx.Param).\n","5. Get the abstract state for the optimizer: abs_optimizer_state = nnx.state(abs_optimizer).\n","6. Find the latest step to restore.\n","7. If a checkpoint exists, define a restore_targets dictionary for ocp.args.Composite. Keys should match those used during save ('params', 'optimizer'), and values should be ocp.args.StandardRestore() wrapped abstract states.\n","8. Restore using mngr_comp.restore(step, args=ocp.args.Composite(**restore_targets)). This will return a dictionary restored_items.\n","9. Create new, \"fresh\" instances of your SimpleLinear model and nnx.Optimizer.\n","10. Update the fresh model in-place using nnx.update(fresh_model, restored_items['params']).\n","11. Update the fresh optimizer in-place using nnx.update(fresh_optimizer, restored_items['optimizer']).\n","12. Verify by checking the optimizer's step and a model parameter.\n","13. Close the manager."],"metadata":{"id":"xPqCsnJNidgw"}},{"cell_type":"code","source":["# Ensure SimpleLinear class definition is available\n","# --- Re-open CheckpointManager ---\n","# TODO: Instantiate CheckpointManager for ckpt_dir_ex3\n","# mngr_comp_restore = ...\n","\n","# --- Create Abstract Model and Optimizer ---\n","def create_abstract_model_and_optimizer():\n","  rngs_abs = nnx.Rngs(params=jax.random.key(0)) # Dummy key for abstract creation\n","  # TODO: Create abstract model. Model class: SimpleLinear(din=10, dout=5, ...)\n","  # abs_model = SimpleLinear(...)\n","\n","  # TODO: Create abstract optimizer. Pass abs_model and an optax.adam instance.\n","  # abs_opt = nnx.Optimizer(...)\n","  # return abs_model, abs_opt\n","\n","# TODO: Call the function to get abstract model and optimizer\n","# abs_model_restore, abs_optimizer_restore = ...\n","\n","# --- Get Abstract States ---\n","# TODO: Get abstract parameter state from abs_model_restore (filter with nnx.Param)\n","# _graphdef_abs_params, abs_params_state = ...\n","# TODO: Get abstract optimizer state from abs_optimizer_restore\n","# abs_optimizer_state = ...\n","\n","print(f\"Abstract params state structure: {jax.tree_util.tree_map(lambda x: x.shape if hasattr(x, 'shape') else x, abs_params_state)}\")\n","print(f\"Abstract optimizer state structure: {jax.tree_util.tree_map(lambda x: x.shape if hasattr(x, 'shape') else x, abs_optimizer_state)}\")\n","\n","# --- Restore Composite State ---\n","# TODO: Get the latest step\n","# step_to_restore_comp = ...\n","\n","if step_to_restore_comp is not None:\n","  # TODO: Define restore_targets dictionary for 'params' and 'optimizer'\n","  # Each item should be wrapped with ocp.args.StandardRestore and its corresponding abstract state.\n","  # restore_targets = {\n","  #    'params': ...,\n","  #    'optimizer': ...\n","  # }\n","  # TODO: Restore items using mngr_comp_restore.restore() and ocp.args.Composite\n","  # restored_items = mngr_comp_restore.restore(...)\n","\n","  # --- Instantiate and Update Concrete Model/Optimizer ---\n","  # TODO: Create a fresh SimpleLinear model instance (use a new RNG key, e.g., key(2))\n","  # fresh_model = ...\n","  # TODO: Create a fresh nnx.Optimizer instance with fresh_model and a new optax.adam instance\n","  # fresh_optimizer = ...\n","\n","  # Store pre-update values for comparison\n","  pre_update_bias = fresh_model.bias.value.copy()\n","  pre_update_opt_step = fresh_optimizer.step.value\n","\n","  # TODO: Update fresh_model with restored_items['params'] using nnx.update()\n","  # nnx.update(...)\n","  # TODO: Update fresh_optimizer with restored_items['optimizer'] using nnx.update()\n","  # nnx.update(...)\n","\n","  print(f\"Restored and updated. Optimizer step: {fresh_optimizer.step.value}\")\n","  print(f\"Fresh model bias before update (first val): {pre_update_bias[0]}\")\n","  print(f\"Fresh model bias after update (first val): {fresh_model.bias.value[0]}\")\n","  print(f\"Original bias from Ex3 (first val): {model_ex3.bias.value[0]}\") # model_ex3 is from previous cell\n","\n","  # Verification\n","  # chex.assert_trees_all_close(fresh_model.bias.value, model_ex3.bias.value) # Compare with the state that was saved\n","  # assert fresh_optimizer.step.value == optimizer.step.value # Compare with optimizer state that was saved\n","else:\n","  print(\"No composite checkpoint found.\")\n","\n","# TODO: Close the manager\n","# mngr_comp_restore.close()"],"metadata":{"id":"FIoLTU7tiUZv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Exercise 4: Solution\n","\n","# Ensure SimpleLinear class definition is available\n","# --- Re-open CheckpointManager ---\n","mngr_comp_restore = ocp.CheckpointManager(ckpt_dir_ex3)\n","\n","# --- Create Abstract Model and Optimizer ---\n","def create_abstract_model_and_optimizer():\n","  rngs_abs = nnx.Rngs(params=jax.random.key(0)) # Dummy key for abstract creation\n","  # Create abstract model\n","  abs_model = SimpleLinear(din=10, dout=5, rngs=rngs_abs)\n","  # Create abstract optimizer\n","  abs_opt = nnx.Optimizer(abs_model, optax.adam(1e-3), wrt=nnx.Param)\n","  return abs_model, abs_opt\n","\n","abs_model_restore, abs_optimizer_restore = create_abstract_model_and_optimizer()\n","\n","# --- Get Abstract States ---\n","_graphdef_abs_params, abs_params_state = nnx.split(abs_model_restore, nnx.Param)\n","abs_optimizer_state = nnx.state(abs_optimizer_restore)\n","\n","print(f\"Abstract params state structure: {jax.tree_util.tree_map(lambda x: (x.shape, x.dtype) if hasattr(x, 'shape') else type(x), abs_params_state)}\")\n","print(f\"Abstract optimizer state structure: {jax.tree_util.tree_map(lambda x: (x.shape, x.dtype) if hasattr(x, 'shape') else type(x), abs_optimizer_state)}\")\n","\n","# --- Restore Composite State ---\n","step_to_restore_comp = mngr_comp_restore.latest_step()\n","\n","if step_to_restore_comp is not None:\n","  restore_targets = {\n","    'params': ocp.args.StandardRestore(abs_params_state),\n","    'optimizer': ocp.args.StandardRestore(abs_optimizer_state)\n","  }\n","  restored_items = mngr_comp_restore.restore(step_to_restore_comp, args=ocp.args.Composite(**restore_targets))\n","\n","  # --- Instantiate and Update Concrete Model/Optimizer ---\n","  # Create fresh instances\n","  fresh_rngs = nnx.Rngs(params=jax.random.key(2)) # Use a different key for the fresh model\n","  fresh_model = SimpleLinear(din=10, dout=5, rngs=fresh_rngs)\n","  fresh_optimizer = nnx.Optimizer(fresh_model, optax.adam(1e-3), wrt=nnx.Param) # Matching optax optimizer\n","\n","  # Store pre-update values for comparison\n","  pre_update_bias = fresh_model.bias.value.copy()\n","  pre_update_opt_step = fresh_optimizer.step.value\n","\n","  # Update using restored states\n","  nnx.update(fresh_model, restored_items['params'])\n","  nnx.update(fresh_optimizer, restored_items['optimizer'])\n","\n","  print(f\"Restored and updated. Optimizer step: {fresh_optimizer.step.value}\")\n","  print(f\"Fresh model bias before update (first val): {pre_update_bias[0]}\") # Will be from key(2)\n","  print(f\"Fresh model bias after update (first val): {fresh_model.bias.value[0]}\") # Should match model_ex3 bias\n","\n","  # Verification (model_ex3 and optimizer are from the previous cell where they were saved)\n","  chex.assert_trees_all_close(fresh_model.bias.value, model_ex3.bias.value)\n","  assert fresh_optimizer.step.value == optimizer.step.value\n","  print(\"Verification successful: Restored model parameters and optimizer step match the saved state.\")\n","else:\n","  print(\"No composite checkpoint found.\")\n","\n","mngr_comp_restore.close()"],"metadata":{"id":"xB50SxmBjMJr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 5: Distributed Checkpointing - Saving Sharded State\n","\n","**Goal**: Understand how to save model state that has been sharded across multiple devices. Orbax handles sharded JAX arrays efficiently.\n","\n","### Topics:\n","\n","* Setting up a JAX device Mesh.\n","* Defining PartitionSpec for sharding arrays.\n","* Creating sharded parameters within an nnx.Module. One way is to initialize parameters and then use jax.device_put with NamedSharding to shard them, then update the module's state. NNX also allows attaching sharding annotations directly to nnx.Variable metadata.\n","* Saving sharded state: Orbax handles sharded arrays transparently during saving if the JAX arrays in the state Pytree are already sharded.\n","\n","### Instructions:\n","\n","1. Define the number of devices and create a device mesh (e.g., a 1D mesh with all available devices).\n","2. Modify the SimpleLinear module (or create ShardedSimpleLinear):\n","* In `__init__`, after initializing parameters, you'll shard them.\n","* For the weight matrix (din, dout), let's shard it along the dout dimension (e.g., PartitionSpec(None, 'data')).\n","* The bias vector (dout,) will also be sharded along its only dimension (PartitionSpec('data')).\n","* To apply sharding:\n"," - Create NamedSharding objects from your PartitionSpec and the mesh.\n"," - Use jax.device_put(param_value, named_sharding) to get sharded JAX arrays.\n"," - Update the .value of your nnx.Param attributes with these sharded arrays.\n","3. Instantiate your sharded model within the mesh context manager (with mesh:). This ensures operations are aware of the mesh.\n","4. Set up a CheckpointManager in a new directory (ckpt_dir_ex5).\n","5. Split the sharded model to get its state: _graphdef_sharded, sharded_state_to_save = nnx.split(sharded_model). The arrays within sharded_state_to_save should now be jax.Array objects with sharding information.\n","6. Save this sharded_state_to_save using mngr.save(). The process is the same as non-sharded saving from Orbax's perspective.\n","7. Wait and close."],"metadata":{"id":"1n58wd9gm1Pq"}},{"cell_type":"code","source":["# --- Setup JAX Mesh ---\n","num_devices = jax.device_count()\n","# If num_devices is 1 after chex.set_n_cpu_devices(8), it means JAX didn't pick up the fakes.\n","# This can happen if JAX initializes its backends before chex runs.\n","# Forcing a rerun of this cell or restarting runtime and running setup first might help.\n","print(f\"Using {num_devices} devices for sharding.\")\n","device_mesh = mesh_utils.create_device_mesh((num_devices,))\n","mesh = Mesh(devices=device_mesh, axis_names=('data',)) # 1D mesh\n","print(mesh)\n","\n","# --- Define Sharded NNX Module ---\n","class ShardedSimpleLinear(nnx.Module):\n","  def __init__(self, din: int, dout: int, mesh: Mesh, *, rngs: nnx.Rngs):\n","    self.din = din\n","    self.dout = dout\n","    self.mesh = mesh\n","\n","    key_w, key_b = rngs.params(), rngs.params()\n","\n","    # Initialize as regular JAX arrays first\n","    initial_weight = jax.random.uniform(key_w, (din, dout))\n","    initial_bias = jnp.zeros((dout,))\n","\n","    # TODO: Define PartitionSpec for weight (shard dout across 'data' axis)\n","    # e.g., PartitionSpec(None, 'data') means not sharded on dim 0, sharded on dim 1\n","    # weight_pspec = ...\n","    # TODO: Define PartitionSpec for bias (shard along 'data' axis)\n","    # bias_pspec = ...\n","\n","    # TODO: Create NamedSharding for weight and bias using self.mesh and the pspecs\n","    # weight_sharding = NamedSharding(...)\n","    # bias_sharding = NamedSharding(...)\n","\n","    # TODO: Shard the initial arrays using jax.device_put and the NamedSharding\n","    # sharded_weight_value = jax.device_put(...)\n","    # sharded_bias_value = jax.device_put(...)\n","\n","    # TODO: Assign these sharded arrays to nnx.Param attributes\n","    # self.weight = nnx.Param(sharded_weight_value)\n","    # self.bias = nnx.Param(sharded_bias_value)\n","\n","    # Alternative (more direct with nnx.Variable metadata if supported well for this case):\n","    # self.weight = nnx.Param(initial_weight, sharding=weight_sharding) # This depends on NNX API\n","    # For this exercise, jax.device_put is explicit and clear.\n","\n","  def __call__(self, x: jax.Array) -> jax.Array:\n","    # x is assumed to be replicated or appropriately sharded for the matmul\n","    # For simplicity, assume x is replicated if din is not sharded, or sharded compatibly.\n","    return x @ self.weight.value + self.bias.value\n","\n","# --- Instantiate Sharded Model within Mesh context ---\n","din_s, dout_s = 8, num_devices * 2 # Ensure dout is divisible by num_devices for even sharding\n","rngs_sharded = nnx.Rngs(params=jax.random.key(3))\n","\n","# TODO: Instantiate ShardedSimpleLinear within the mesh context\n","# with mesh:\n","#   sharded_model = ...\n","\n","# print(f\"Sharded model created. Weight sharding: {sharded_model.weight.value.sharding}\")\n","# print(f\"Sharded model bias sharding: {sharded_model.bias.value.sharding}\")\n","\n","\n","# --- Setup CheckpointManager for Sharded Save ---\n","ckpt_dir_ex5 = os.path.join(CKPT_BASE_DIR, 'ex5_sharded_save')\n","cleanup_ckpt_dir(ckpt_dir_ex5)\n","# TODO: Instantiate CheckpointManager\n","# mngr_sharded_save = ...\n","\n","# --- Split and Save Sharded State ---\n","# TODO: Split the sharded_model\n","# _graphdef_sharded, sharded_state_to_save = ...\n","\n","# print(f\"Sharded state to save (bias type): {type(sharded_state_to_save['bias'].value)}\")\n","# print(f\"Sharded state to save (bias sharding): {sharded_state_to_save['bias'].value.sharding}\")\n","\n","# current_step_sharded = 200\n","# TODO: Save the sharded_state_to_save\n","# mngr_sharded_save.save(...)\n","# TODO: Wait and close\n","# mngr_sharded_save.wait_until_finished()\n","# print(f\"Sharded checkpoint saved for step {current_step_sharded} in {ckpt_dir_ex5}.\")\n","# mngr_sharded_save.close()"],"metadata":{"id":"tzeRStI1jVuf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Exercise 5: Solution\n","\n","# --- Setup JAX Mesh ---\n","num_devices = jax.device_count()\n","if num_devices == 1 and chex.set_n_cpu_devices.called_in_process: # If we faked 8 but only see 1\n","     print(\"Warning: JAX might not be using the faked CPU devices. Restart runtime and run Setup cell first if sharding tests fail.\")\n","print(f\"Using {num_devices} devices for sharding.\")\n","# Ensure a 1D mesh for simplicity, using all available (or faked) devices.\n","device_mesh = mesh_utils.create_device_mesh((num_devices,))\n","mesh = Mesh(devices=device_mesh, axis_names=('data',)) # 1D mesh for 'data' parallelism\n","print(mesh)\n","\n","# --- Define Sharded NNX Module ---\n","class ShardedSimpleLinear(nnx.Module):\n","  def __init__(self, din: int, dout: int, mesh: Mesh, *, rngs: nnx.Rngs):\n","    self.din = din\n","    self.dout = dout\n","    self.mesh = mesh # Store mesh for creating NamedSharding\n","\n","    key_w, key_b = rngs.params(), rngs.params()\n","\n","    initial_weight = jax.random.uniform(key_w, (din, dout))\n","    initial_bias = jnp.zeros((dout,))\n","\n","    # Define PartitionSpec for sharding\n","    # Shard weight's second dimension (dout) across the 'data' mesh axis\n","    weight_pspec = PartitionSpec(None, 'data')\n","    # Shard bias's only dimension (dout) across the 'data' mesh axis\n","    bias_pspec = PartitionSpec('data',)\n","\n","    # Create NamedSharding from PartitionSpec and mesh\n","    weight_sharding = NamedSharding(self.mesh, weight_pspec)\n","    bias_sharding = NamedSharding(self.mesh, bias_pspec)\n","\n","    # Shard the initial arrays using jax.device_put\n","    # This ensures the arrays are created with the specified sharding\n","    sharded_weight_value = jax.device_put(initial_weight, weight_sharding)\n","    sharded_bias_value = jax.device_put(initial_bias, bias_sharding)\n","\n","    self.weight = nnx.Param(sharded_weight_value)\n","    self.bias = nnx.Param(sharded_bias_value)\n","    # Note: Flax NNX aims to allow sharding annotations directly in nnx.Variable metadata\n","    # e.g., using nnx.spmd.with_partitioning or passing sharding to nnx.Param.\n","    # Explicit jax.device_put is also a valid way to get sharded arrays into the state.\n","\n","  def __call__(self, x: jax.Array) -> jax.Array:\n","    return x @ self.weight.value + self.bias.value\n","\n","# --- Instantiate Sharded Model within Mesh context ---\n","din_s, dout_s = 8, num_devices * 2 # Make dout divisible by num_devices\n","rngs_sharded = nnx.Rngs(params=jax.random.key(3))\n","\n","with mesh: # Operations within this context are aware of the mesh\n","  sharded_model = ShardedSimpleLinear(din_s, dout_s, mesh, rngs=rngs_sharded)\n","\n","print(f\"Sharded model created. Weight sharding: {sharded_model.weight.value.sharding}\")\n","print(f\"Sharded model bias sharding: {sharded_model.bias.value.sharding}\")\n","\n","# --- Setup CheckpointManager for Sharded Save ---\n","ckpt_dir_ex5 = os.path.join(CKPT_BASE_DIR, 'ex5_sharded_save')\n","cleanup_ckpt_dir(ckpt_dir_ex5)\n","mngr_sharded_save = ocp.CheckpointManager(ckpt_dir_ex5, options=ocp.CheckpointManagerOptions(max_to_keep=1))\n","\n","# --- Split and Save Sharded State ---\n","# The live state already contains sharded jax.Array objects\n","_graphdef_sharded, sharded_state_to_save = nnx.split(sharded_model)\n","\n","print(f\"Sharded state to save (bias type): {type(sharded_state_to_save['bias'].value)}\")\n","print(f\"Sharded state to save (bias sharding): {sharded_state_to_save['bias'].value.sharding}\")\n","# The actual arrays in sharded_state_to_save are now GlobalDeviceArrays (or jax.Array with sharding)\n","\n","current_step_sharded = 200\n","# Orbax handles sharded-array saving under the hood\n","mngr_sharded_save.save(current_step_sharded, args=ocp.args.StandardSave(sharded_state_to_save))\n","mngr_sharded_save.wait_until_finished()\n","print(f\"Sharded checkpoint saved for step {current_step_sharded} in {ckpt_dir_ex5}.\")\n","mngr_sharded_save.close()"],"metadata":{"id":"-0Yvg9non-Jw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercise 6: Distributed Checkpointing - Restoring Sharded State\n","\n","**Goal**: Learn to restore sharded model state, which requires providing an abstract state Pytree that includes the target sharding specifications.\n","\n","### Topics:\n","\n","* Creating an abstract model using nnx.eval_shape.\n","* Splitting it to get an abstract state.\n","* Crucial Step: Applying sharding specifications to this abstract state to create a \"sharding-aware template\" or abstract_target. This is often done using jax.lax.with_sharding_constraint or by ensuring the nnx.eval_shape process (if the module itself defines sharding during abstract construction) yields abstract states with correct sharding.\n","* Using StandardRestore with this sharding-aware abstract_target.\n","* Merging the restored sharded state with a graph definition to reconstruct the model.\n","\n","### Instructions:\n","\n","1. Reuse the mesh from Exercise 5.\n","2. Re-open the CheckpointManager pointing to ckpt_dir_ex5.\n","3. Define a function, e.g., create_abstract_sharded_model_for_restore(mesh).\n","* Inside, instantiate your ShardedSimpleLinear module (or a similar one intended for sharded restoration) with the provided mesh. This instantiation should ensure its parameters would be sharded if it were a concrete model.\n","* Pass a lambda creating this module to nnx.eval_shape() to get an abstract_model.\n","* The key is that nnx.split(abstract_model) should yield an abstract_state where leaves corresponding to sharded parameters are ShapeDtypeStructs that already encode the target sharding. This happens if ShardedSimpleLinear's `__init__` uses jax.device_put with NamedSharding on dummy data when nnx.is_abstract_eval() is true, or if NNX's sharding annotation system propagates this to the abstract state.\n","* A more explicit way (if the above is tricky with eval_shape directly embedding sharding into abstract state leaves) is shown in the slides:\n"," 1. abstract_model = nnx.eval_shape(...) for a non-sharded-at-init version.\n"," 2. _graphdef, abstract_state_plain = nnx.split(abstract_model).\n"," 3. Define sharding_specs (Pytree of PartitionSpec).\n"," 4. abstract_target = jax.tree_util.tree_map(lambda x, spec: jax.ShapeDtypeStruct(x.shape, x.dtype, sharding=NamedSharding(mesh, spec)), abstract_state_plain, sharding_specs) OR use jax.lax.with_sharding_constraint(abstract_state_plain, sharding_specs) on the abstract state within a jax.jit and mesh context as shown in slide conceptual code. Let's try to make ShardedSimpleLinear work with eval_shape directly if possible, or fall back to explicit constraint.\n","\n","4. To make ShardedSimpleLinear directly produce an abstract state with sharding during nnx.eval_shape:\n","* Modify ShardedSimpleLinear`.__init__`.\n","* When nnx.is_abstract_eval() is true, instead of jax.device_put(real_data, ...) use jax.ShapeDtypeStruct(shape, dtype, sharding=NamedSharding(mesh, pspec)) for the .value of the nnx.Param.\n","5. Call your function within the mesh context and jax.jit it (as per slides ) to get the abstract_target_state and graphdef_for_restore. graphdef_for_restore, abstract_target_state = nnx.split(nnx.eval_shape(lambda: ShardedSimpleLinear(..., mesh=mesh,...))) (simplified)\n","6. Restore using mngr.restore(step, args=ocp.args.StandardRestore(abstract_target_state)).\n","7. Reconstruct the model using nnx.merge(graphdef_for_restore, restored_sharded_state).\n","8. Verify the sharding of the restored model's parameters.\n","9. Close the manager.\n","\n","Self-correction for instruction 4 & 5: Instead of modifying ShardedSimpleLinear to behave differently under nnx.is_abstract_eval(), it's cleaner and more aligned with typical Orbax/JAX patterns to:\n","a. Get a plain abstract state (shapes/dtypes only) from a version of the model that doesn't try to shard during abstract init.\n","b. Then, explicitly create the abstract_target by adding sharding to this plain abstract state.\n","Let's refine ShardedSimpleLinear to accept an init_sharded flag. For eval_shape, we'll pass init_sharded=False (or rely on nnx.eval_shape not creating real arrays), then apply sharding to the resulting abstract state.\n","\n","A more direct approach for step 5, if the ShardedSimpleLinear from Ex5 is used for eval_shape: nnx.eval_shape will create ShapeDtypeStruct for parameters. If jax.device_put was part of the module's __init__, nnx.eval_shape might not execute it to produce sharded ShapeDtypeStructs directly. The critical part is that abstract_target passed to StandardRestore must have the sharding information.\n","\n","Let's use the method from slide \"Distributed Checkpointing: Restoring Sharded State\":\n","\n","1. abstract_model = nnx.eval_shape(lambda: ModelClass(...)) (ModelClass here doesn't apply sharding during this abstract init).\n","2. _graphdef, abstract_state_struct_only = nnx.split(abstract_model).\n","3. Define sharding_pytree (same Pytree structure as state, but with NamedSharding objects at leaves).\n","4. abstract_target = jax.tree.map(lambda s, n: jax.ShapeDtypeStruct(s.shape, s.dtype, sharding=n), abstract_state_struct_only, sharding_pytree). This abstract_target is then used in StandardRestore."],"metadata":{"id":"9y6cURi8pKIQ"}},{"cell_type":"code","source":["# Ensure ShardedSimpleLinear class definition and mesh from Ex5 are available.\n","\n","# --- Re-open CheckpointManager for Sharded Restore ---\n","# TODO: Instantiate CheckpointManager for ckpt_dir_ex5\n","# mngr_sharded_restore = ...\n","\n","# --- Create Abstract Target State with Sharding Information ---\n","# Method:\n","# 1. Create a \"plain\" abstract model (shapes/dtypes only).\n","# 2. Split it to get graphdef and plain abstract_state.\n","# 3. Define the desired sharding for each parameter (Pytree of NamedSharding).\n","# 4. Combine plain abstract_state with sharding to create the final abstract_target.\n","\n","def create_abstract_model_for_sharded_restore():\n","    # This lambda should instantiate the model structure without applying sharding during this phase.\n","    # We'll use the ShardedSimpleLinear class, but its sharding logic inside __init__\n","    # might be skipped by eval_shape if it involves actual data.\n","    # Alternatively, provide a version of the model that takes sharding specs externally.\n","    # For simplicity, let's assume nnx.eval_shape on ShardedSimpleLinear gives us ShapeDtypeStructs,\n","    # and we will then OVERWRITE their sharding attribute if necessary, or construct them fresh.\n","\n","    # Let's make a 'template' instance of ShardedSimpleLinear just to get its structure via split.\n","    # The actual sharding for the abstract target will be defined explicitly.\n","    temp_rngs = nnx.Rngs(params=jax.random.key(99))\n","    # Create an instance of ShardedSimpleLinear as it was defined in Ex5.\n","    # nnx.eval_shape will trace its construction.\n","    # TODO: abstract_model_proto = nnx.eval_shape(lambda: ShardedSimpleLinear(... pass din_s, dout_s, mesh from Ex5 ...))\n","    # abstract_model_proto = ...\n","    # return abstract_model_proto\n","\n","# Run within mesh context for operations that might interact with sharding\n","# with mesh:\n","    # TODO: Create the abstract_model_proto by calling the function above.\n","    # abstract_model_for_target = create_abstract_model_for_sharded_restore()\n","    # TODO: Split it to get graphdef_for_restore and an abstract_state (which might have None for sharding)\n","    # graphdef_for_restore_sharded, abstract_state_struct_only = ...\n","\n","    # Define the target sharding (PartitionSpecs, then NamedSharding)\n","    # These must match the sharding used when the checkpoint was SAVED.\n","    # weight_pspec_target = PartitionSpec(None, 'data') # As in Ex5\n","    # bias_pspec_target = PartitionSpec('data',)     # As in Ex5\n","\n","    # weight_sharding_target = NamedSharding(mesh, weight_pspec_target)\n","    # bias_sharding_target = NamedSharding(mesh, bias_pspec_target)\n","\n","    # Create the sharding pytree for the abstract_target\n","    # It needs to match the structure of abstract_state_struct_only['params'] or similar,\n","    # depending on how ShardedSimpleLinear structures its state.\n","    # Assuming state is flat { 'weight': ..., 'bias': ... } within the nnx.State object.\n","    # If ShardedSimpleLinear created params like self.weight = nnx.Param(...),\n","    # then abstract_state_struct_only will look like {'weight': {'value': ShapeDtypeStruct}, 'bias': {'value': ShapeDtypeStruct}}\n","\n","    # TODO: Construct the `sharding_for_abstract_state` Pytree.\n","    # It should mirror the structure of `abstract_state_struct_only` but contain NamedSharding objects at the leaves\n","    # where parameters are.\n","    # Example if state is {'weight': {'value':...}, 'bias': {'value':...}}:\n","    # sharding_for_abstract_state = {\n","    #     'weight': {'value': weight_sharding_target},\n","    #     'bias': {'value': bias_sharding_target}\n","    # }\n","    # Verify this structure based on print(abstract_state_struct_only) from split.\n","\n","    # TODO: Create the final abstract_target by combining shapes/dtypes with new sharding.\n","    # abstract_target_state = jax.tree.map(\n","    #    lambda sds, sh: jax.ShapeDtypeStruct(sds.shape, sds.dtype, sharding=sh) if isinstance(sds, jax.ShapeDtypeStruct) else sds,\n","    #    abstract_state_struct_only,\n","    #    sharding_for_abstract_state\n","    # )\n","    # print(f\"Abstract target for restore (bias sharding): {abstract_target_state['bias'].value.sharding}\")\n","\n","# --- Restore Sharded State ---\n","# step_to_restore_sharded = mngr_sharded_restore.latest_step()\n","# if step_to_restore_sharded is not None:\n","    # with mesh: # Restoration happens within the mesh context\n","        # TODO: Restore sharded state using abstract_target_state\n","        # restored_sharded_state_dict = mngr_sharded_restore.restore(...)\n","\n","        # TODO: Reconstruct the model using nnx.merge\n","        # reconstructed_sharded_model = ...\n","\n","    # print(f\"Sharded model restored from step {step_to_restore_sharded}.\")\n","    # print(f\"Restored weight sharding: {reconstructed_sharded_model.weight.value.sharding}\")\n","    # print(f\"Restored bias sharding: {reconstructed_sharded_model.bias.value.sharding}\")\n","\n","    # Verification (optional): Compare with sharded_model from Ex5 if it's in scope and has same structure\n","    # chex.assert_trees_all_equal_shapes_and_dtypes(nnx.state(reconstructed_sharded_model), nnx.state(sharded_model))\n","    # assert str(reconstructed_sharded_model.weight.value.sharding) == str(sharded_model.weight.value.sharding)\n","\n","# else:\n","#    print(\"No sharded checkpoint found to restore.\")\n","\n","# mngr_sharded_restore.close()"],"metadata":{"id":"8ZinY5nyoXI7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title Exercise 6: Solution\n","\n","# Ensure ShardedSimpleLinear class definition and mesh from Ex5 are available.\n","# din_s, dout_s from Ex5 were: din_s = 8, dout_s = num_devices * 2\n","\n","# --- Re-open CheckpointManager for Sharded Restore ---\n","mngr_sharded_restore = ocp.CheckpointManager(ckpt_dir_ex5)\n","\n","# --- Create Abstract Target State with Sharding Information ---\n","# This follows the principle that the abstract target for restore must contain sharding info\n","\n","def create_abstract_model_for_sharded_restore_eval_shape():\n","  # This lambda is for nnx.eval_shape. It should define the *structure*\n","  # ShardedSimpleLinear's __init__ from Ex5 already creates sharded JAX arrays.\n","  # nnx.eval_shape will trace this. The resulting abstract state's leaves\n","  # should be ShapeDtypeStructs that already reflect the sharding\n","  # because jax.device_put (which includes sharding) is part of its traced __init__.\n","  # This is a more integrated way if the module's __init__ handles sharding for abstract eval.\n","  temp_rngs_for_eval = nnx.Rngs(params=jax.random.key(100)) # Dummy key for eval_shape\n","  # Pass the same mesh instance that will be used for restoration\n","  return ShardedSimpleLinear(din=din_s, dout=dout_s, mesh=mesh, rngs=temp_rngs_for_eval)\n","\n","with mesh: # Operations like eval_shape and restore should be within the mesh context\n","  # Create abstract model using nnx.eval_shape.\n","  # The sharding info should ideally be embedded by ShardedSimpleLinear's __init__\n","  # when traced by nnx.eval_shape, because it uses jax.device_put.\n","  abstract_model_sharded_eval = nnx.eval_shape(create_abstract_model_for_sharded_restore_eval_shape)\n","  # Use the graphdef from the abstract sharded model for merging\n","  graphdef_for_restore_sharded = nnx.split(abstract_model_sharded_eval)[0]\n","\n","  # We need the abstract state structure from the plain model (SimpleLinear)\n","  # because nnx.eval_shape on ShardedSimpleLinear might already put sharding\n","  # in the abstract state, and we want to demonstrate the manual creation\n","  # of the abstract target with sharding.\n","  plain_abstract_model = nnx.eval_shape(lambda: SimpleLinear(din_s, dout_s, rngs=nnx.Rngs(0)))\n","  # This state will have ShapeDtypeStructs, but likely with sharding=None\n","  _gdef_plain, abstract_state_struct_only = nnx.split(plain_abstract_model)\n","\n","  # Define target sharding specs\n","  weight_pspec_target = PartitionSpec(None, 'data') # As in Ex5\n","  bias_pspec_target = PartitionSpec('data',)     # As in Ex5\n","  weight_sharding_target = NamedSharding(mesh, weight_pspec_target)\n","  bias_sharding_target = NamedSharding(mesh, bias_pspec_target)\n","\n","  # Create the sharding pytree for the abstract_target\n","  # It needs to match the structure of `abstract_state_struct_only` exactly.\n","  # Since abstract_state_struct_only is {'bias': {'value':ShapeDtypeStruct}, 'weight': {'value':ShapeDtypeStruct}},\n","  # the sharding pytree should mirror this structure, placing NamedSharding at the leaves.\n","  sharding_pytree_for_target = nnx.State({\n","    'weight': nnx.VariableState(type=nnx.Param, value=weight_sharding_target),\n","    'bias': nnx.VariableState(type=nnx.Param, value=bias_sharding_target)\n","  })\n","\n","  # Create the final abstract_target by mapping over the structure of\n","  # abstract_state_struct_only and sharding_pytree_for_target.\n","  # We want to replace the ShapeDtypeStruct in abstract_state_struct_only.value\n","  # with a new ShapeDtypeStruct that includes the sharding from sharding_pytree_for_target.value.\n","\n","  # Define a function that takes two VariableState objects\n","  def update_variable_state_sharding(sds_variable_state: nnx.VariableState, sharding_variable_state: nnx.VariableState):\n","    if isinstance(sds_variable_state, jax.ShapeDtypeStruct):\n","      # Create a new ShapeDtypeStruct with the desired sharding\n","      new_sds = jax.ShapeDtypeStruct(sds_variable_state.shape, sds_variable_state.dtype, sharding=sharding_variable_state)\n","      # Return a new VariableState with the updated value\n","      return new_sds\n","    else:\n","      # If the value is not a ShapeDtypeStruct (e.g., metadata), keep it as is\n","      # In this specific case, this path might not be strictly needed if abstract_state_struct_only\n","      # only contains VariableState with ShapeDtypeStruct values at the leaves we care about.\n","      return sds_variable_state\n","\n","  # Map this function over the two pytrees. Use a custom is_leaf to map at the VariableState level.\n","  # This ensures the mapping function receives (VariableState, VariableState containing sharding) pairs.\n","  def is_variable_state_node(x):\n","    # Treat VariableState itself as a node (not a leaf) so mapping happens inside it\n","    return not isinstance(x, nnx.VariableState)\n","\n","  # Apply the mapping. The lambda receives items from corresponding positions in both trees.\n","  # Here, lambda `sds_node` is a VariableState from `abstract_state_struct_only`,\n","  # and lambda `sharding_node` is a VariableState from `sharding_pytree_for_target`.\n","  abstract_target_state = jax.tree.map(\n","    update_variable_state_sharding,\n","    abstract_state_struct_only, # This tree has ShapeDtypeStructs nested in VariableState.value\n","    sharding_pytree_for_target # This tree has NamedSharding objects nested in VariableState.value\n","  )\n","\n","  print(f\"Abstract target for restore (bias sharding): {abstract_target_state['bias'].value.sharding}\")\n","  print(f\"Abstract target for restore (weight sharding): {abstract_target_state['weight'].value.sharding}\")\n","\n","# --- Restore Sharded State ---\n","step_to_restore_sharded = mngr_sharded_restore.latest_step()\n","if step_to_restore_sharded is not None:\n","  with mesh: # Restoration happens within the mesh context\n","    # Use StandardRestore with the abstract_target that includes sharding info\n","    restored_sharded_state_dict = mngr_sharded_restore.restore(\n","        step_to_restore_sharded,\n","        args=ocp.args.StandardRestore(abstract_target_state)\n","    )\n","\n","    # Reconstruct the model using nnx.merge\n","    # Use the graphdef obtained from splitting the abstract sharded model\n","    reconstructed_sharded_model = nnx.merge(graphdef_for_restore_sharded, restored_sharded_state_dict)\n","\n","  print(f\"Sharded model restored from step {step_to_restore_sharded}.\")\n","  print(f\"Restored weight sharding: {reconstructed_sharded_model.weight.value.sharding}\")\n","  print(f\"Restored bias sharding: {reconstructed_sharded_model.bias.value.sharding}\")\n","\n","  # Verification\n","  if 'sharded_model' in globals(): # If sharded_model from Ex5 is available\n","    # Compare structure and dtypes\n","    chex.assert_trees_all_equal_structs(nnx.state(reconstructed_sharded_model), nnx.state(sharded_model))\n","    # Compare sharding\n","    assert str(reconstructed_sharded_model.weight.value.sharding) == str(sharded_model.weight.value.sharding)\n","    assert str(reconstructed_sharded_model.bias.value.sharding) == str(sharded_model.bias.value.sharding)\n","    # Compare values - this will involve communication due to sharding\n","    chex.assert_trees_all_close(nnx.state(reconstructed_sharded_model), nnx.state(sharded_model))\n","    print(\"Verification of sharding, structure, and values successful.\")\n","else:\n","   print(\"No sharded checkpoint found to restore.\")\n","\n","mngr_sharded_restore.close()"],"metadata":{"id":"fu1XapHPrVbK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Advanced Orbax Features & Best Practices (Brief Overview)\n","\n","There are also some more advanced Orbax features. While we won't do full coding exercises for these in this notebook, it's good to be aware of them:\n","\n","* **Asynchronous Checkpointing**: manager.save() can operate in the background. Use manager.wait_until_finished() before your program exits or if you need to use the checkpoint immediately. This improves training throughput by not blocking the main training loop. Our examples used wait_until_finished().\n","\n","* **Atomicity**: CheckpointManager ensures that checkpoints are saved atomically. This means you won't get corrupted checkpoints if your training job crashes mid-save. This is handled for you by Orbax.\n","\n","* **Saving Non-Pytree Data (Metadata)**: Sometimes you need to save extra information like training configuration, dataset iterators, or model version. You can use ocp.args.JsonSave within ocp.args.Composite to save dictionary-like data as JSON alongside your model Pytrees. Restoration uses ocp.args.JsonRestore.\n","\n","### Example Concept:\n","\n","```\n","metadata = {'version': '1.0', 'dataset_info': 'imagenet_split_train'}\n","save_args = ocp.args.Composite(\n","  params=ocp.args.StandardSave(params_state),\n","  metadata=ocp.args.JsonSave(metadata)\n",")\n","mngr.save(step, args=save_args)\n","```\n","\n","* **TensorStore Backend**: For extremely large models or when working with cloud storage, Orbax can use TensorStore. This backend allows for more efficient, potentially parallel I/O for individual array shards, often transparently. This is usually configured at a lower level or might be default in certain JAX environments.\n","\n","### Key Takeaways:\n","\n","* Flax NNX offers a stateful, Pythonic way to define models.\n","* Orbax is the standard for checkpointing NNX State Pytrees.\n","* The general workflow:\n"," - **Saving**: nnx.split -> mngr.save.\n"," - **Restoring**: nnx.eval_shape -> Get abstract_state -> mngr.restore -> nnx.merge or nnx.update.\n","* CheckpointManager is your friend for managing multiple checkpoints.\n","* Use ocp.args.Composite for saving multiple distinct items (e.g., model parameters + optimizer state).\n","* For sharded (distributed) data, ensuring your abstract_target for restoration correctly specifies the target sharding is crucial. StandardRestore handles this if the abstract target has the sharding info.\n","\n","### Congratulations!\n","You've now worked through the fundamentals of checkpointing Flax NNX models with Orbax, from basic saving and restoring to handling optimizer states and distributed (sharded) scenarios.\n","\n","Remember to consult the official documentation for more in-depth details:\n","\n","* Orbax: https://orbax.readthedocs.io\n","* Flax NNX: (Part of the Flax documentation) https://flax.readthedocs.io\n","* JAX: https://jax.readthedocs.io\n","Keep practicing, and happy JAXing!\n","\n","Please send us feedback at https://goo.gle/jax-training-feedback"],"metadata":{"id":"lIT5kKJF15Ew"}},{"cell_type":"code","source":[],"metadata":{"id":"EVAtnk_P3Gk0"},"execution_count":null,"outputs":[]}]}