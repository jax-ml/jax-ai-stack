{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1VX3x1EduykqtaT1xhZkoaHZxNn3CqBrJ","timestamp":1755113930444}],"authorship_tag":"ABX9TyPmBiaJXUTc1jPQEsiFdPg8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Colab Notebook: Debugging JAX & Flax NNX - Exercises\n","\n","Welcome! This notebook contains exercises to help you practice the JAX and Flax NNX debugging techniques discussed in the lecture. If you're a PyTorch user you'll find some concepts familiar, while others are specific to JAX's compiled nature. Remember to run the setup cells first!\n","\n","First, let's install the necessary libraries and import them."],"metadata":{"id":"yJcIGTl1Wqb8"}},{"cell_type":"code","source":["# Start by updating the protobuf version, which may require a restart\n","\n","!pip install -U protobuf"],"metadata":{"id":"B490gzJL8Xlp"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1gdDkIB1WjSg"},"outputs":[],"source":["!pip install -Uq flax jax jaxlib chex optax"]},{"cell_type":"code","source":["import jax\n","import jax.numpy as jnp\n","from jax import jit, grad, vmap\n","import flax\n","from flax import nnx\n","import chex\n","import pdb # Python's built-in debugger\n","import functools # For functools.partial\n","import optax # For optimizers, though we won't train deeply\n","\n","chex.set_n_cpu_devices(8) # Fake an environment with 8 CPUs.  This must be done before any JAX operations\n","print(f\"Fake devices: {jax.devices()}\")\n","\n","# NOTE for Flax v0.11+: The flax.nnx.Optimizer API has changed.\n","# It now requires a `wrt` argument at construction (e.g., wrt=nnx.Param)\n","# and the update call is now `optimizer.update(model, grads)` instead of `optimizer.update(grads)`.\n","\n","# Helper to clear chex trace counter for repeatable examples\n","chex.clear_trace_counter()\n","\n","print(f\"JAX version: {jax.__version__}\")\n","print(f\"Flax version: {flax.__version__}\") # NNX is part of flax\n","print(f\"Chex version: {chex.__version__}\")\n","print(f\"Device: {jax.devices()}\")"],"metadata":{"id":"ECwCGI6VXD7e"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 1. \"printf Debugging\" in JAX: jax.debug.print()\n","\n","JAX's JIT compilation means standard Python print() behaves differently inside JITted functions. It sees tracers during compilation, not runtime values. jax.debug.print() is the JAX-aware alternative."],"metadata":{"id":"ShqGVC2hXkYx"}},{"cell_type":"markdown","source":["### Exercise 1.1:\n","1. Uncomment and complete the line # YOUR CODE HERE in the compute_and_print function above.\n","2. Add a jax.debug.print() statement to display the runtime value of z.\n","3. Run the cell. Observe the outputs.\n"," - What does the standard print(y) show?\n"," - What do the jax.debug.print statements show for y and z? Why is this different?"],"metadata":{"id":"whTw53ilYNpJ"}},{"cell_type":"code","source":["@jit\n","def compute_and_print(x):\n","  y = x * 10\n","  print(\"Standard print (sees tracer):\", y)\n","  jax.debug.print(\"jax.debug.print (sees runtime value for y): {y_val}\", y_val=y, ordered=True)\n","\n","  z = y / 2\n","  # Exercise 1.1: Add another jax.debug.print here to see the runtime value of 'z'\n","  # Make sure to give it a descriptive message and use the ordered=True argument.\n","  # YOUR CODE HERE\n","\n","  return z\n","\n","input_val = jnp.array(5.0)\n","print(f\"Input value: {input_val}\\n\")\n","output_val = compute_and_print(input_val)\n","print(f\"\\nFinal output: {output_val}\")"],"metadata":{"id":"UAL9lJ_-XN99"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Solution (for Exercise 1.1, after attempting):"],"metadata":{"id":"NLK-ncceYlhk"}},{"cell_type":"code","source":["# @jit\n","# def compute_and_print_solution(x):\n","#   y = x * 10\n","#   print(\"Standard print (sees tracer):\", y)\n","#   jax.debug.print(\"jax.debug.print (sees runtime value for y): {y_val}\", y_val=y, ordered=True)\n","\n","#   z = y / 2\n","#   jax.debug.print(\"jax.debug.print (sees runtime value for z): {z_val}\", z_val=z, ordered=True) # SOLUTION\n","\n","#   return z\n","\n","# input_val = jnp.array(5.0)\n","# print(f\"Input value: {input_val}\\n\")\n","# output_val = compute_and_print_solution(input_val)\n","# print(f\"\\nFinal output: {output_val}\")"],"metadata":{"id":"qY5aNK0uX7p7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Standard print shows a tracer object (e.g., Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace(level=0/1)>). This is because it executes during JAX's tracing phase. jax.debug.print shows the concrete numerical values (e.g., 50.0 for y, 25.0 for z) because it's embedded into the compiled computation graph and executes with runtime data."],"metadata":{"id":"kiK87sd1Y_8H"}},{"cell_type":"markdown","source":["## 2. Interactive Debugging in JIT: jax.debug.breakpoint()\n","jax.debug.breakpoint() is JAX's equivalent of pdb.set_trace() for use inside transformed functions. It pauses execution and gives you a (jaxdb) prompt."],"metadata":{"id":"zM0niKH1cbE0"}},{"cell_type":"markdown","source":["### Exercise 2.1:\n","1. Uncomment and complete the line # YOUR CODE HERE in the interact_with_values function above.\n","2. Add jax.debug.breakpoint() where indicated.\n","3. Run the cell.\n","4. When execution pauses at the (jaxdb) prompt:\n"," - Inspect the value of y by typing p y and pressing Enter.\n"," - Continue execution by typing c and pressing Enter.\n","5. Note that jaxdb has a subset of pdb commands (e.g., stepping n or s is not available)."],"metadata":{"id":"tqNY12kxc3Rb"}},{"cell_type":"code","source":["@jit\n","def interact_with_values(x):\n","  y = jnp.sin(x)\n","  jax.debug.print(\"Value of y before breakpoint: {y_val}\", y_val=y)\n","\n","  # Exercise 2.1: Place the breakpoint here.\n","  # YOUR CODE HERE\n","\n","  z = jnp.cos(y)\n","  jax.debug.print(\"Value of z after breakpoint: {z_val}\", z_val=z)\n","  return z\n","\n","input_angle = jnp.array(0.75)\n","print(\"Calling interact_with_values...\")\n","result = interact_with_values(input_angle)\n","print(f\"Result: {result}\")"],"metadata":{"id":"LerEFq1yYwPd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Solution (for Exercise 2.1, after attempting):"],"metadata":{"id":"TW-HX9bGd1iO"}},{"cell_type":"code","source":["# @jit\n","# def interact_with_values_solution(x):\n","#   y = jnp.sin(x)\n","#   jax.debug.print(\"Value of y before breakpoint: {y_val}\", y_val=y)\n","\n","#   jax.debug.breakpoint() # SOLUTION\n","\n","#   z = jnp.cos(y)\n","#   jax.debug.print(\"Value of z after breakpoint: {z_val}\", z_val=z)\n","#   return z\n","\n","# input_angle = jnp.array(0.75)\n","# print(\"Calling interact_with_values...\")\n","# result = interact_with_values_solution(input_angle)\n","# print(f\"Result: {result}\")"],"metadata":{"id":"LLKAPQAWciOs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 3. Back to Basics: Temporarily Disabling JIT with jax.disable_jit()\n","Sometimes, you need the full power of standard Python debugging tools. jax.disable_jit() allows JAX functions to execute eagerly.\n","\n","### Exercise 3.1 & 3.2:\n","1. In `complex_calculation`, add pdb.set_trace() where indicated (# YOUR CODE HERE for 3.1).\n","2. First, try running the cell as is (with Scenario 1 uncommented and Scenario 2's call commented out). Observe what happens with pdb.set_trace() inside a JITted function.\n","3. Then, comment out Scenario 1.\n","4. In Scenario 2, within the with jax.disable_jit(): block, call `complex_calculation` (where # YOUR CODE HERE for 3.2 is) with value (try 0.1 first, then 5.0 to ensure the conditional is met) and threshold=0.5.\n","5. When pdb triggers:\n"," - Inspect a, b, and c.\n"," - Type c to continue.\n","6. Reflect: When would you use jax.disable_jit() over jax.debug.breakpoint()?"],"metadata":{"id":"HC5gk0aF4Fye"}},{"cell_type":"code","source":["@jit\n","def complex_calculation(x, threshold):\n","  a = x * 2.0\n","  b = jnp.log(a)\n","  c = b + x\n","  # Imagine 'c' sometimes becomes NaN, and it's hard to see why.\n","  # We want to inspect 'a', 'b', and 'c' using standard pdb.\n","  if c > threshold: # This condition might be tricky under JIT\n","      # Exercise 3.1: Add a pdb.set_trace() here.\n","      # It will only work if JIT is disabled for this function call.\n","      # YOUR CODE HERE\n","      print(\"Inside conditional pdb trace\") # This will print if pdb is hit\n","  d = jnp.sqrt(jnp.abs(c)) # abs to avoid NaNs from sqrt of negative\n","  return d\n","\n","value = jnp.array(0.1) # Try with 0.1 then with 5.0\n","\n","# Scenario 1: JIT enabled (pdb.set_trace() will be skipped or might error)\n","# print(\"--- Running WITH JIT (pdb will likely be skipped) ---\")\n","# try:\n","#   result_jit = complex_calculation(value, threshold=0.5)\n","#   print(f\"Result with JIT: {result_jit}\")\n","# except Exception as e:\n","#   print(f\"Scenario 1 Error:\\n{e}\\n\")\n","\n","# Scenario 2: JIT disabled\n","print(\"\\n--- Running with JIT DISABLED for this block ---\")\n","with jax.disable_jit():\n","  # Exercise 3.2: Call complex_calculation here with value and threshold=0.5\n","  # so that your pdb.set_trace() (from Ex 3.1) gets triggered.\n","  # YOUR CODE HERE\n","  pass # remove this pass\n","\n","print(\"Finished disable_jit block.\")"],"metadata":{"id":"zBF2wvwe4Psb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Solution (for Exercise 3.1 & 3.2, after attempting):"],"metadata":{"id":"ta9o2YnI5CbE"}},{"cell_type":"code","source":["# @jit\n","# def complex_calculation_solution(x, threshold):\n","#   a = x * 2.0\n","#   b = jnp.log(a)\n","#   c = b + x\n","#   if c > threshold:\n","#       pdb.set_trace() # SOLUTION 3.1\n","#       print(\"Inside conditional pdb trace\")\n","#   d = jnp.sqrt(jnp.abs(c))\n","#   return d\n","\n","# value_for_pdb = jnp.array(5.0) # This value will trigger the condition c > threshold\n","\n","# # Scenario 1: JIT enabled (pdb.set_trace() will be skipped or might error)\n","# print(\"--- Running WITH JIT (pdb will likely be skipped) ---\")\n","# try:\n","#   result_jit = complex_calculation_solution(value_for_pdb, threshold=0.5)\n","#   print(f\"Result with JIT: {result_jit}\")\n","# except Exception as e:\n","#   print(f\"Scenario 1 Error:\\n{e}\\n\")\n","\n","# print(\"\\n--- Running with JIT DISABLED for this block ---\")\n","# with jax.disable_jit():\n","#   result_no_jit = complex_calculation_solution(value_for_pdb, threshold=0.5) # SOLUTION 3.2\n","#   print(f\"Result with JIT disabled: {result_no_jit}\")\n","# print(\"Finished disable_jit block.\")"],"metadata":{"id":"wYGhziLN43D1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You'd use jax.disable_jit() when jax.debug.breakpoint() is insufficient, e.g., when you need the full pdb features (like stepping), want to use an IDE debugger, or when jax.debug.breakpoint() itself doesn't give enough context. The trade-off is performance loss."],"metadata":{"id":"Xc49FTqw61A1"}},{"cell_type":"markdown","source":["## 4. Automatic NaN Hunting: jax_debug_nans Flag\n","NaNs can be a nightmare. jax_debug_nans helps JAX pinpoint the exact operation causing them.\n","\n","### Exercise 4.1 & 4.2:\n","1. In Scenario 1, uncomment the example call or create your own call to problematic_function_for_nans that results in a NaN (e.g., x = jnp.array(-1.0), divisor = jnp.array(1.0) or x = jnp.array(1.0), divisor = jnp.array(0.0)). Run and observe the error.\n","2. In Scenario 2:\n"," - Uncomment the line jax.config.update(\"jax_debug_nans\", True).\n"," - Uncomment the example call or use the same NaN-causing inputs as in 4.1.\n"," - Run and observe the error message. Is it more helpful in pinpointing the source of the NaN?\n"," - Make sure the finally block runs to disable the flag.\n","3. Why is jax_debug_nans not enabled by default?"],"metadata":{"id":"Sr75lY78658N"}},{"cell_type":"code","source":["@jit\n","def problematic_function_for_nans(x, divisor):\n","  y = x * 100\n","  # This operation can cause NaN if divisor is 0 or x is negative and we take log\n","  z = jnp.log(y) / divisor # Potential NaN source\n","  return z + y\n","\n","# Scenario 1: Run without jax_debug_nans\n","print(\"--- Scenario 1: Running without jax_debug_nans ---\")\n","try:\n","  # Exercise 4.1: Call problematic_function_for_nans with inputs that cause a NaN\n","  # For example, x = jnp.array(-1.0), divisor = jnp.array(1.0)\n","  # OR x = jnp.array(1.0), divisor = jnp.array(0.0)\n","  # Observe the error. Is it specific?\n","  # YOUR CODE HERE\n","  # result1 = problematic_function_for_nans(jnp.array(-1.0), jnp.array(1.0))\n","  # print(f\"Result 1: {result1}\")\n","  pass # remove this\n","except Exception as e:\n","  print(f\"Caught exception (without jax_debug_nans): {e}\\n\")\n","\n","\n","# Scenario 2: Run WITH jax_debug_nans\n","print(\"--- Scenario 2: Running WITH jax_debug_nans ---\")\n","# jax.config.update(\"jax_debug_nans\", True) # Enable NaN debugging\n","\n","try:\n","  # Exercise 4.2: Call problematic_function_for_nans again with the SAME NaN-causing inputs.\n","  # Observe the error now. Is it more specific?\n","  # YOUR CODE HERE\n","  # result2 = problematic_function_for_nans(jnp.array(-1.0), jnp.array(1.0))\n","  # print(f\"Result 2: {result2}\")\n","  pass # remove this\n","except Exception as e:\n","  print(f\"Caught exception (WITH jax_debug_nans): {e}\\n\")\n","finally:\n","  # jax.config.update(\"jax_debug_nans\", False) # Disable after use\n","  print(\"jax_debug_nans has been disabled.\")"],"metadata":{"id":"fUEN-nY07RrM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Solution (for Exercise 4.1 & 4.2, after attempting):"],"metadata":{"id":"pvrr7nys7Y_5"}},{"cell_type":"code","source":["# @jit\n","# def problematic_function_for_nans_solution(x, divisor):\n","#   y = x * 100\n","#   z = jnp.log(y) / divisor\n","#   return z + y\n","\n","# # Scenario 1: Run without jax_debug_nans\n","# print(\"--- Scenario 1: Running without jax_debug_nans ---\")\n","# try:\n","#   # For example, x = jnp.array(-1.0), divisor = jnp.array(1.0)\n","#   # OR x = jnp.array(1.0), divisor = jnp.array(0.0)\n","#   result1 = problematic_function_for_nans_solution(jnp.array(-1.0), jnp.array(1.0)) # SOLUTION for 4.1\n","#   print(f\"Result 1: {result1}\")\n","# except Exception as e:\n","#   print(f\"Caught exception (without jax_debug_nans): {e}\\n\")\n","\n","\n","# # Scenario 2: Run WITH jax_debug_nans\n","# print(\"--- Scenario 2: Running WITH jax_debug_nans ---\")\n","# jax.config.update(\"jax_debug_nans\", True) # Enable NaN debugging\n","\n","# try:\n","#   result2 = problematic_function_for_nans_solution(jnp.array(-1.0), jnp.array(1.0)) # SOLUTION for 4.2\n","#   print(f\"Result 2: {result2}\")\n","# except Exception as e:\n","#   print(f\"Caught exception (WITH jax_debug_nans): {e}\\n\")\n","# finally:\n","#   jax.config.update(\"jax_debug_nans\", False) # Disable after use\n","#   print(\"jax_debug_nans has been disabled.\")"],"metadata":{"id":"BykyxEHK7cj_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Without jax_debug_nans, the error might be a generic NaN detection or occur later in the computation. With jax_debug_nans enabled, JAX re-runs the failing operations in eager mode and raises an error at the exact primitive operation that produced the NaN, making it much easier to find. It's not enabled by default because it adds overhead (checks and potential eager re-runs), significantly slowing down execution."],"metadata":{"id":"XcigqZTi7oZM"}},{"cell_type":"markdown","source":["## 5. Inspecting Flax NNX Models: nnx.display()\n","\n","**A Note on NNX Modules in Flax v0.11+:** In this version, `nnx.Module` and other NNX objects are now registered as JAX Pytrees. This means JAX transformations like `jax.jit` and `jax.vmap` can be used on them directly. However, if you use functions like `jax.tree.map` on a data structure containing NNX modules, they will be traversed by default. To treat them as leaves (the old behavior), you must use the `is_leaf` argument: `is_leaf=lambda x: isinstance(x, nnx.Pytree)`.\n","\n","`nnx.display()` provides a clear view of your NNX Module's structure, parameters, and state.\n","\n","### Exercise 5.1 - 5.4:\n","1. 5.1: In `SimpleNNXModel.__init__`, add a second `nnx.Linear` layer named `self.dense2` that maps from `dhidden` to `dout` features. Remember to provide `rngs`.\n","2. 5.2: In `SimpleNNXModel.__call__`, pass the intermediate `x` through `self.dense2` (if you added it).\n","3. 5.3: When instantiating `SimpleNNXModel`, ensure `din`, `dhidden`, and `dout` match your intended architecture (e.g., `dout=5` if your `dense2` outputs 5 features).\n","4. 5.4: Use `nnx.display(model)` to print the structure. Examine the output. Can you see both dense layers and their parameters (`kernel`, `bias`)? Can you see the `PReLU` parameters?"],"metadata":{"id":"h3xGIJ5V7wjA"}},{"cell_type":"code","source":["class SimpleNNXModel(nnx.Module):\n","  def __init__(self, din: int, dhidden: int, dout: int, *, rngs: nnx.Rngs):\n","    key = rngs.params()\n","    self.dense1 = nnx.Linear(din, dhidden, rngs=rngs)\n","    # Exercise 5.1: Add another Linear layer called 'dense2' (dhidden -> dout)\n","    # YOUR CODE HERE\n","    self.activation = nnx.relu # Example of a layer with its own parameters\n","\n","  def __call__(self, x):\n","    x = self.dense1(x)\n","    x = nnx.relu(x)\n","    # Exercise 5.2: Pass x through 'dense2' if you added it.\n","    # YOUR CODE HERE\n","    x = self.activation(x)\n","    return x\n","\n","# Initialize RNGs for parameters\n","key = jax.random.key(0)\n","model_rngs = nnx.Rngs(params=key)\n","\n","# Instantiate the model\n","# Exercise 5.3: Update din, dhidden, dout if you changed the model structure\n","model = SimpleNNXModel(din=10, dhidden=20, dout=5, rngs=model_rngs)\n","\n","# Display the model structure\n","print(\"--- Model Structure using nnx.display() ---\")\n","# Exercise 5.4: Use nnx.display() to show the model's structure\n","# YOUR CODE HERE\n","\n","# If you have treescope installed and are in a compatible environment (like Colab default),\n","# nnx.display() will give an interactive tree. Otherwise, it falls back to print."],"metadata":{"id":"L_dyIQsX7gj2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Solution (for Exercise 5.1-5.4, after attempting):"],"metadata":{"id":"BOw60Gqz8LYD"}},{"cell_type":"code","source":["# class SimpleNNXModelSolution(nnx.Module):\n","#   def __init__(self, din: int, dhidden: int, dout: int, *, rngs: nnx.Rngs):\n","#     self.dense1 = nnx.Linear(din, dhidden, rngs=rngs)\n","#     self.dense2 = nnx.Linear(dhidden, dout, rngs=rngs) # SOLUTION 5.1\n","#     self.activation = nnx.relu\n","\n","#   def __call__(self, x):\n","#     x = self.dense1(x)\n","#     x = nnx.relu(x)\n","#     x = self.dense2(x) # SOLUTION 5.2\n","#     x = self.activation(x)\n","#     return x\n","\n","# # Initialize RNGs for parameters\n","# key = jax.random.key(0)\n","# model_rngs = nnx.Rngs(params=key)\n","\n","# # Instantiate the model\n","# model_solution = SimpleNNXModelSolution(din=10, dhidden=20, dout=5, rngs=model_rngs) # SOLUTION 5.3 (dout adjusted)\n","\n","# # Display the model structure\n","# print(\"--- Model Structure using nnx.display() ---\")\n","# nnx.display(model_solution) # SOLUTION 5.4"],"metadata":{"id":"HYV14gzU8Oia"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 6. Capturing Intermediate Values: nnx.sow()\n","Module.sow() allows you to \"plant\" intermediate values during the forward pass for later retrieval.\n","\n","### Exercise 6.1 & 6.2:\n","1. 6.1: In ModelWithSow.__call__, after x1_act is computed, use self.sow(nnx.Intermediate, 'activation_layer1', x1_act) to store it.\n","2. 6.2: After running the model, retrieve the sown value. It will be an attribute on sow_model named activation_layer1. Access its .value and print its shape.\n","3. What would happen if you called sow multiple times with the same name within one forward pass (e.g., inside a loop)?"],"metadata":{"id":"-oLfTi_R9h8x"}},{"cell_type":"code","source":["class ModelWithSow(nnx.Module):\n","  def __init__(self, *, rngs: nnx.Rngs):\n","    self.dense1 = nnx.Linear(5, 10, rngs=rngs)\n","    self.dense2 = nnx.Linear(10, 3, rngs=rngs)\n","\n","  def __call__(self, x):\n","    x1_act = self.dense1(x)\n","    x1_act = nnx.relu(x1_act)\n","\n","    # Exercise 6.1: Use self.sow() to store the value of x1_act.\n","    # Use nnx.Intermediate as the variable_type and 'activation_layer1' as the name.\n","    # YOUR CODE HERE\n","\n","    x2_out = self.dense2(x1_act)\n","    return x2_out\n","\n","# Setup\n","key = jax.random.key(1)\n","model_sow_rngs = nnx.Rngs(params=key)\n","sow_model = ModelWithSow(rngs=model_sow_rngs)\n","dummy_input = jnp.ones((1, 5))\n","\n","# Run the model\n","output = sow_model(dummy_input)\n","\n","# Retrieve the sown value\n","# Exercise 6.2: Retrieve the 'activation_layer1' value from the sow_model instance.\n","# Remember it's stored as an attribute, and the actual data is in its .value property.\n","# Print the shape of the retrieved value.\n","# YOUR CODE HERE\n","# retrieved_activation = ...\n","# print(f\"Shape of retrieved activation: {retrieved_activation.shape}\") # Adjust if it's a tuple"],"metadata":{"id":"hlqi9q0T8Rwf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Solution (for Exercise 6.1 & 6.2, after attempting):"],"metadata":{"id":"xcoeBO-n93rN"}},{"cell_type":"code","source":["# class ModelWithSowSolution(nnx.Module):\n","#   def __init__(self, *, rngs: nnx.Rngs):\n","#     self.dense1 = nnx.Linear(5, 10, rngs=rngs)\n","#     self.dense2 = nnx.Linear(10, 3, rngs=rngs)\n","\n","#   def __call__(self, x):\n","#     x1_act = self.dense1(x)\n","#     x1_act = nnx.relu(x1_act)\n","\n","#     self.sow(nnx.Intermediate, 'activation_layer1', x1_act) # SOLUTION 6.1\n","\n","#     x2_out = self.dense2(x1_act)\n","#     return x2_out\n","\n","# # Setup\n","# key = jax.random.key(1)\n","# model_sow_rngs = nnx.Rngs(params=key)\n","# sow_model_solution = ModelWithSowSolution(rngs=model_sow_rngs)\n","# dummy_input = jnp.ones((1, 5))\n","\n","# # Run the model\n","# output = sow_model_solution(dummy_input)\n","\n","# # Retrieve the sown value\n","# retrieved_sown_value_obj = sow_model_solution.activation_layer1 # This is the Variable object\n","# retrieved_activation = retrieved_sown_value_obj.value          # This is the actual data (often a tuple)\n","# print(f\"Retrieved activation (raw): {retrieved_activation}\")\n","# # By default, sow appends to a tuple. So value is likely ((1,10))\n","# print(f\"Shape of retrieved activation (first element): {retrieved_activation[0].shape}\") # SOLUTION 6.2"],"metadata":{"id":"atf7bn4g91cj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["If sow is called multiple times with the same name in one forward pass, by default, it appends each new value to a tuple stored in the .value property of the sown attribute."],"metadata":{"id":"bKzh4Ibi-Dx-"}},{"cell_type":"markdown","source":["## 7. Robustness with Chex Assertions\n","Chex provides powerful assertions for JAX code.\n","\n","### Exercise 7.1.1:\n","1. Fill in the # YOUR CODE HERE section in process_image_data with the specified Chex static assertions.\n","2. Run the cell and observe how the assertions catch the errors for wrong_shape_data and wrong_type_data."],"metadata":{"id":"Hg5Z4qCR-Lsz"}},{"cell_type":"code","source":["@jit\n","def process_image_data(image_batch: chex.Array):\n","  # Exercise 7.1.1: Add Chex assertions to verify:\n","  # 1. image_batch has a rank of 4 (e.g., Batch, Height, Width, Channels).\n","  # 2. image_batch has a dtype of jnp.float32.\n","  # 3. image_batch has a specific shape, e.g., (32, 224, 224, 3).\n","  #    You can use a placeholder for batch_size if needed: chex.assert_shape(image_batch, (None, 224, 224, 3))\n","  # YOUR CODE HERE\n","\n","  # Dummy computation\n","  processed = image_batch * 2.0 - 1.0\n","  return processed\n","\n","# Test cases\n","correct_data = jnp.ones((32, 224, 224, 3), dtype=jnp.float32)\n","wrong_shape_data = jnp.ones((32, 224, 3), dtype=jnp.float32) # Missing a dim\n","wrong_type_data = jnp.ones((32, 224, 224, 3), dtype=jnp.int32)\n","\n","print(\"--- Testing with correct data ---\")\n","try:\n","  _ = process_image_data(correct_data)\n","  print(\"Correct data processed successfully!\")\n","except Exception as e:\n","  print(f\"Error with correct data:\\n{e}\")\n","\n","print(\"\\n--- Testing with wrong shape data ---\")\n","try:\n","  _ = process_image_data(wrong_shape_data)\n","  print(\"Wrong shape data processed successfully (this shouldn't happen if assertions are correct).\")\n","except AssertionError as e:\n","  print(f\"Caught expected AssertionError for wrong shape:\\n{e}\")\n","\n","print(\"\\n--- Testing with wrong type data ---\")\n","try:\n","  _ = process_image_data(wrong_type_data)\n","  print(\"Wrong type data processed successfully (this shouldn't happen if assertions are correct).\")\n","except AssertionError as e:\n","  print(f\"Caught expected AssertionError for wrong type:\\n{e}\")"],"metadata":{"id":"f7z_sqG09-lP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Solution (for Exercise 7.1.1, after attempting):"],"metadata":{"id":"27e3MpqF-ksp"}},{"cell_type":"code","source":["# @jit\n","# def process_image_data_solution(image_batch: chex.Array):\n","#   chex.assert_rank(image_batch, 4)                             # SOLUTION\n","#   chex.assert_type(image_batch, jnp.float32)                   # SOLUTION\n","#   chex.assert_shape(image_batch, (None, 224, 224, 3))          # SOLUTION (using None for batch)\n","\n","#   processed = image_batch * 2.0 - 1.0\n","#   return processed\n","\n","# # Test cases (same as above)\n","# correct_data_sol = jnp.ones((32, 224, 224, 3), dtype=jnp.float32)\n","# wrong_shape_data_sol = jnp.ones((32, 224, 3), dtype=jnp.float32)\n","# wrong_type_data_sol = jnp.ones((32, 224, 224, 3), dtype=jnp.int32)\n","\n","# print(\"--- SOLUTION: Testing with correct data ---\")\n","# try:\n","#   _ = process_image_data_solution(correct_data_sol)\n","#   print(\"Correct data processed successfully!\")\n","# except Exception as e:\n","#   print(f\"Error with correct data:\\n{e}\")\n","\n","# print(\"\\n--- SOLUTION: Testing with wrong shape data ---\")\n","# try:\n","#   _ = process_image_data_solution(wrong_shape_data_sol)\n","# except AssertionError as e:\n","#   print(f\"Caught expected AssertionError for wrong shape:\\n{e}\")\n","\n","# print(\"\\n--- SOLUTION: Testing with wrong type data ---\")\n","# try:\n","#   _ = process_image_data_solution(wrong_type_data_sol)\n","# except AssertionError as e:\n","#   print(f\"Caught expected AssertionError for wrong type:\\n{e}\")"],"metadata":{"id":"4LbQjfxr-n_Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 7.2. Performance Debugging: @chex.assert_max_traces()\n","Unintended JIT recompilations kill performance. @chex.assert_max_traces(n=N) helps detect this.\n","\n","### Exercise 7.2.1 & 7.2.2:\n","1. 7.2.1: In process_dynamic_shape, add the @chex.assert_max_traces(n=1) decorator.\n","2. 7.2.2: Uncomment and complete the second call to process_dynamic_shape using an input array with a different shape (e.g., jnp.ones((3,3))).\n","3. Run the cell.\n"," - Observe that Scenario 1 (with static_argnums) passes because the shape information critical for compilation (shape_tuple) is static and doesn't change.\n"," - Observe that Scenario 2 should raise an AssertionError. Why does this happen?"],"metadata":{"id":"Fjpi1eVTAHHP"}},{"cell_type":"code","source":["chex.clear_trace_counter() # Reset counter for this specific example\n","\n","# Scenario 1: Function with static argument for shape\n","@functools.partial(jit, static_argnums=(1,)) # shape_tuple is static\n","@chex.assert_max_traces(n=1)\n","def process_fixed_shape_staticarg(x: chex.Array, shape_tuple: tuple):\n","    chex.assert_shape(x, shape_tuple) # Check the shape matches\n","    return x * 2.0\n","\n","print(\"--- Scenario 1: Static argnum, consistent shape tuple ---\")\n","fixed_shape = (3, 4)\n","input_data_s1_c1 = jnp.ones(fixed_shape)\n","input_data_s1_c2 = jnp.zeros(fixed_shape) # Same shape, different values\n","_ = process_fixed_shape_staticarg(input_data_s1_c1, fixed_shape) # First call, traces\n","print(\"First call to process_fixed_shape_staticarg successful (traces).\")\n","_ = process_fixed_shape_staticarg(input_data_s1_c2, fixed_shape) # Second call, reuses cache\n","print(\"Second call to process_fixed_shape_staticarg successful (reuses cache).\")\n","\n","\n","# Scenario 2: Function where input shape might vary, leading to retracing if not handled\n","chex.clear_trace_counter() # Reset for this scenario\n","\n","@jit\n","# Exercise 7.3.1: Add @chex.assert_max_traces(n=1) here\n","# YOUR CODE HERE\n","def process_dynamic_shape(x: chex.Array):\n","    # This function will be re-traced if 'x' shape changes between calls\n","    return x + jnp.sum(x) # Example op\n","\n","print(\"\\n--- Scenario 2: Varying input shapes ---\")\n","try:\n","    print(\"Calling process_dynamic_shape with (2, 2)...\")\n","    _ = process_dynamic_shape(jnp.ones((2, 2))) # First call, traces\n","    print(\"First call to process_dynamic_shape successful.\")\n","\n","    # Exercise 7.3.2: Call process_dynamic_shape with a DIFFERENT shape, e.g., (3,3).\n","    # This should trigger an AssertionError if assert_max_traces is working.\n","    print(\"Calling process_dynamic_shape with (3, 3)...\")\n","    # YOUR CODE HERE\n","    # _ = process_dynamic_shape(jnp.ones((3, 3)))\n","    print(\"Second call to process_dynamic_shape successful (UNEXPECTED if shapes differ and max_traces=1).\")\n","\n","except AssertionError as e:\n","    print(f\"\\nCaught EXPECTED AssertionError for too many traces:\\n{e}\")\n","except Exception as e:\n","    print(f\"Caught unexpected error: {e}\")"],"metadata":{"id":"6rp79x5XBW4E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Solution (for Exercise 7.2.1 & 7.2.2, after attempting):"],"metadata":{"id":"YY8xI2LMBcUN"}},{"cell_type":"code","source":["# chex.clear_trace_counter() # Reset counter for this specific example\n","\n","# # Scenario 1: Function with static argument for shape\n","# @functools.partial(jit, static_argnums=(1,))\n","# @chex.assert_max_traces(n=1)\n","# def process_fixed_shape_staticarg_sol(x: chex.Array, shape_tuple: tuple):\n","#     chex.assert_shape(x, shape_tuple)\n","#     return x * 2.0\n","\n","# print(\"--- SOLUTION: Scenario 1: Static argnum, consistent shape tuple ---\")\n","# fixed_shape_sol = (3, 4)\n","# input_data_s1_c1_sol = jnp.ones(fixed_shape_sol)\n","# input_data_s1_c2_sol = jnp.zeros(fixed_shape_sol)\n","# _ = process_fixed_shape_staticarg_sol(input_data_s1_c1_sol, fixed_shape_sol)\n","# print(\"First call to process_fixed_shape_staticarg_sol successful (traces).\")\n","# _ = process_fixed_shape_staticarg_sol(input_data_s1_c2_sol, fixed_shape_sol)\n","# print(\"Second call to process_fixed_shape_staticarg_sol successful (reuses cache).\")\n","\n","\n","# chex.clear_trace_counter() # Reset for this scenario\n","\n","# @jit\n","# @chex.assert_max_traces(n=1) # SOLUTION 7.3.1\n","# def process_dynamic_shape_sol(x: chex.Array):\n","#     return x + jnp.sum(x)\n","\n","# print(\"\\n--- SOLUTION: Scenario 2: Varying input shapes ---\")\n","# try:\n","#     print(\"Calling process_dynamic_shape_sol with (2, 2)...\")\n","#     _ = process_dynamic_shape_sol(jnp.ones((2, 2)))\n","#     print(\"First call to process_dynamic_shape_sol successful.\")\n","\n","#     print(\"Calling process_dynamic_shape_sol with (3, 3)...\")\n","#     _ = process_dynamic_shape_sol(jnp.ones((3, 3))) # SOLUTION 7.3.2\n","#     print(\"Second call to process_dynamic_shape_sol successful (UNEXPECTED if shapes differ and max_traces=1).\")\n","\n","# except AssertionError as e:\n","#     print(f\"\\nCaught EXPECTED AssertionError for too many traces:\\n{e}\")\n","# except Exception as e:\n","#     print(f\"Caught unexpected error: {e}\")"],"metadata":{"id":"sT2sQNj6_950"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In Scenario 2, the AssertionError happens because process_dynamic_shape is JIT-compiled based on the shape of its input x. When called the second time with a different shape, JAX needs to re-trace and re-compile the function for this new shape. @chex.assert_max_traces(n=1) detects this second trace and raises an error, alerting you to a potential performance issue due to recompilation."],"metadata":{"id":"P1vrg1D3AliP"}},{"cell_type":"markdown","source":["## 8. Monitoring with TensorBoard\n","TensorBoard is excellent for visualizing training metrics. The setup is similar to PyTorch.\n","\n","### Exercise 8.1 - 8.3:\n","1. 8.1: Create a tensorboardX.SummaryWriter instance, saving logs to LOG_DIR.\n","2. 8.2: Inside the loop, use writer.add_scalar() to log dummy_loss and dummy_accuracy. Crucially, convert them to Python scalars using .item().\n","3. 8.3: After the loop, close the writer using writer.close().\n","4. Run the cell.\n","5. If you are in Colab:\n"," - Uncomment the lines %load_ext tensorboard and %tensorboard --logdir {LOG_DIR} at the end of the cell.\n"," - Run the cell again. TensorBoard should appear in the output. Navigate to the SCALARS tab.\n","6. If running locally:\n"," - Open your terminal.\n"," - Navigate to the directory containing the logs folder (i.e., the parent of LOG_DIR).\n"," - Run tensorboard --logdir logs.\n"," - Open the URL (usually http://localhost:6006) in your browser.\n","7. Explore the TensorBoard and profiler (XProf) tools"],"metadata":{"id":"ui2JWgdKArS7"}},{"cell_type":"code","source":["# !pip install -Uq tensorboardX tensorboard tensorboard_plugin_profile\n","!pip install -Uq tensorboardX tensorboard_plugin_profile\n","!pip install -U protobuf"],"metadata":{"id":"52D9nw-20Zcd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# For TensorBoard\n","from tensorboardX import SummaryWriter\n","import shutil # For cleaning up log directories"],"metadata":{"id":"z6S1ynM3094v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Clean up previous logs if any\n","LOG_DIR = \"logs/jax_debug_run\"\n","if shutil.os.path.exists(LOG_DIR):\n","    shutil.rmtree(LOG_DIR)\n","    print(f\"Removed old log directory: {LOG_DIR}\")\n","\n","# Exercise 8.1: Create a SummaryWriter from tensorboardX\n","# Point it to the LOG_DIR defined above.\n","# YOUR CODE HERE\n","# writer = ...\n","\n","# Dummy training loop\n","print(\"\\nSimulating training loop...\")\n","jax.profiler.start_trace(LOG_DIR) # Capturing trace for xprof\n","\n","for epoch in range(10):\n","  # Simulate loss and accuracy (JAX arrays)\n","  dummy_loss = jnp.array(1.0 / (epoch + 1))\n","  dummy_accuracy = jnp.array(1.0 - dummy_loss)\n","\n","  # Exercise 8.2: Log dummy_loss as 'Loss/train' and dummy_accuracy as 'Accuracy/validation'\n","  # Remember to use .item() to convert JAX arrays to Python scalars before logging.\n","  # Use 'epoch' as the global_step.\n","  # YOUR CODE HERE\n","\n","  if (epoch + 1) % 2 == 0:\n","    print(f\"Epoch {epoch+1}: Loss = {dummy_loss.item():.4f}, Acc = {dummy_accuracy.item():.4f}\")\n","\n","jax.profiler.stop_trace()\n","# Exercise 8.3: Close the writer\n","# YOUR CODE HERE\n","\n","print(f\"\\nTensorBoard logs saved to: {LOG_DIR}\")\n","print(\"To view in TensorBoard, run the following in your terminal (if local):\")\n","print(f\"tensorboard --logdir={LOG_DIR.split('/')[0]}\") # Get base 'logs' dir\n","print(\"Or, if in Colab, you can use the %tensorboard magic:\")\n","# %load_ext tensorboard\n","# %tensorboard --logdir {LOG_DIR}"],"metadata":{"id":"RU_Yh-FuAcB6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Solution (for Exercise 8.1-8.3, after attempting):"],"metadata":{"id":"CQAOsIstBqa1"}},{"cell_type":"code","source":["# # Clean up previous logs if any\n","# LOG_DIR_SOL = \"logs/jax_debug_run_solution\" # Use a different dir for solution\n","# if shutil.os.path.exists(LOG_DIR_SOL):\n","#     shutil.rmtree(LOG_DIR_SOL)\n","#     print(f\"Removed old log directory: {LOG_DIR_SOL}\")\n","\n","# writer = SummaryWriter(LOG_DIR_SOL) # SOLUTION 8.1\n","# print(f\"TensorBoard writer initialized. Logging to: {LOG_DIR_SOL}\")\n","\n","# # Dummy training loop\n","# print(\"\\nSimulating training loop...\")\n","# # Ensure the profiler plugin is included in the trace\n","# jax.profiler.start_trace(LOG_DIR_SOL, create_perfetto_link=False) # Capturing trace for xprof\n","\n","# for epoch in range(10):\n","#   dummy_loss = jnp.array(1.0 / (epoch + 1))\n","#   dummy_loss.block_until_ready() # Ensure the array is ready\n","#   dummy_accuracy = jnp.array(1.0 - dummy_loss)\n","#   dummy_accuracy.block_until_ready() # Ensure the array is ready\n","\n","#   writer.add_scalar('Loss/train', dummy_loss.item(), global_step=epoch) # SOLUTION 8.2\n","#   writer.add_scalar('Accuracy/validation', dummy_accuracy.item(), global_step=epoch) # SOLUTION 8.2\n","\n","#   if (epoch + 1) % 2 == 0:\n","#     print(f\"Epoch {epoch+1}: Loss = {dummy_loss.item():.4f}, Acc = {dummy_accuracy.item():.4f}\")\n","\n","# jax.profiler.stop_trace()\n","# writer.close() # SOLUTION 8.3\n","\n","# print(f\"\\nTensorBoard logs saved to: {LOG_DIR_SOL}\")\n","# print(\"To view in TensorBoard, run the following in your terminal (if local):\")\n","# print(f\"tensorboard --logdir={LOG_DIR_SOL.split('/')[0]}\")\n","# print(\"Or, if in Colab, you can use the %tensorboard magic:\")\n","# %load_ext tensorboard\n","# %tensorboard --logdir {LOG_DIR_SOL} # --general_plugin_dir \"{LOG_DIR_SOL}/plugins\""],"metadata":{"id":"wBgZvLJcBs57"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Profiling with XProf\n","\n","Profiling is also essential for understanding and improving your code.  XProf is a great tool for profiling JAX and Flax NNX, and is compatible with TensorBoard.  We've seen the XProf profiler with TensorBoard above, but let's look at a more interesting example.  We'll download some profiling data from an MNIST model."],"metadata":{"id":"xFXlyJy8cghy"}},{"cell_type":"code","source":["# git clone the xprof repo so we have access to the demo data there\n","!git clone http://github.com/openxla/xprof\n","\n","# Launch TensorBoard and navigate to the Profile tab to view performance profile\n","%tensorboard --logdir=xprof/demo"],"metadata":{"id":"nrAHqdS2CJIl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 9. Visualizing Data Layout: jax.debug.visualize_array_sharding\n","\n","Understanding data sharding is crucial for multi-device training. `jax.debug.visualize_array_sharding` helps visualize this.\n","\n","Actually demonstrating this effectively requires a multi-device setup (e.g., multiple GPUs or TPUs and a Mesh). In a standard Colab CPU/single GPU environment, arrays won't be genuinely sharded across a mesh, but we can still see how the function works by faking a multi-device environment using `chex.set_n_cpu_devices`, which we did at the beginning of this Colab."],"metadata":{"id":"xvxjppTYCITm"}},{"cell_type":"markdown","source":["### Exercise 9.1:\n","1. Run the cell below.\n","2. Observe the output of `jax.debug.visualize_array_sharding`. Even on a single device, it will print information about the array's (lack of) sharding.\n","3. Think: If you had a Mesh of 4 devices arranged in a 2x2 grid (`Mesh(devices, ('dp', 'mp'))`) and an array arr of shape (8, 1024), how might you define a PartitionSpec to shard arr across data parallelism (dp) for the first dimension and model parallelism (mp) for the second? What would you expect `visualize_array_sharding(arr)` to show?"],"metadata":{"id":"7V5P8KTmGzgD"}},{"cell_type":"code","source":["from jax.sharding import Mesh, PartitionSpec, NamedSharding\n","from jax.experimental import mesh_utils\n","import jax.numpy as jnp\n","from jax import jit, grad, vmap\n","\n","try:\n","  if len(jax.devices()) >= 2:\n","      device_mesh = mesh_utils.create_device_mesh((len(jax.devices()),)) # Use all available devices\n","      mesh = Mesh(devices=device_mesh, axis_names=('data',))\n","      print(f\"Created a mesh with shape: {mesh.devices.shape} and names: {mesh.axis_names}\")\n","  else:\n","      print(\"Not enough devices to create a meaningful mesh for sharding demo. Will run on single device.\")\n","      mesh = None\n","except Exception as e:\n","  print(f\"Could not create mesh (likely on CPU Colab or single GPU): {e}\")\n","  mesh = None\n","\n","\n","@jit\n","def sharded_computation_demo(x_unsharded):\n","  # In a real scenario, x would be sharded before being passed or sharded inside\n","  # For this demo, we'll just visualize the unsharded array as if it were sharded\n","\n","  print(\"--- Input 'Sharding' (on single device, so not truly sharded) ---\")\n","  jax.debug.visualize_array_sharding(x_unsharded)\n","\n","  y = x_unsharded * 2.0\n","\n","  # If 'x_unsharded' had sharding, 'y' would typically inherit it or have a related one.\n","  print(\"--- Output 'Sharding' (on single device) ---\")\n","  jax.debug.visualize_array_sharding(y)\n","  return y\n","\n","an_array = jnp.arange(8.0)\n","\n","print(f\"Original array: {an_array}\")\n","\n","# If we had a mesh, we could try to shard it:\n","if mesh:\n","  # Shard along the first axis ('data')\n","  sharding_spec = NamedSharding(mesh, PartitionSpec('data',))\n","  an_array_sharded = jax.device_put(an_array, sharding_spec)\n","  print(f\"Array sharding: {an_array_sharded.sharding}\")\n","  output_sharded = sharded_computation_demo(an_array_sharded)\n","else:\n","  print(\"No mesh, running unsharded demo.\")\n","  output_unsharded = sharded_computation_demo(an_array) # Run with the original JIT\n","\n","# Simplified version for Colab (no actual sharding applied)\n","print(\"\\n--- Running visualization on a single device (no actual sharding) ---\")\n","output_unsharded = sharded_computation_demo(an_array)\n","print(f\"Output: {output_unsharded}\")"],"metadata":{"id":"wTT7usbDB10Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Answer (for Conceptual Exercise 9.1):\n","- You might define P = PartitionSpec('dp', 'mp').\n","- jax.debug.visualize_array_sharding(arr) would then print a diagram showing how the 8 rows are split over the 'dp' axis (e.g., 4 rows per device slice along 'dp') and the 1024 columns are split over the 'mp' axis (e.g., 512 columns per device slice along 'mp'). Each device in the 2x2 mesh would hold a (4, 512) slice of the original array."],"metadata":{"id":"JXGikTMmCmNb"}},{"cell_type":"markdown","source":["## Conclusion & Key Takeaways\n","You've now practiced with several key JAX and Flax NNX debugging tools!\n","- jax.debug.print() & jax.debug.breakpoint(): Your go-to tools for inspecting values inside JITted code.\n","- jax.disable_jit(): The \"escape hatch\" to use standard Python debuggers (pdb, IDEs) at the cost of performance.\n","- jax_debug_nans: Invaluable for automatically finding the source of NaNs.\n","- nnx.display(): Essential for understanding your NNX model's architecture and state.\n","- nnx.sow(): Useful for capturing intermediate activations without altering function signatures.\n","- Chex assertions (assert_shape, assert_tree_all_finite, assert_max_traces): Build robust and performant code by catching errors early and detecting recompilations.\n","- TensorBoard: Standard for monitoring training, works seamlessly with JAX.\n","Debugging in JAX's compiled world requires adapting your PyTorch habits, but with these tools, you're well-equipped to tackle issues effectively!\n","\n","Please send us feedback at https://goo.gle/jax-training-feedback"],"metadata":{"id":"7W9M6dYtC1qD"}}]}