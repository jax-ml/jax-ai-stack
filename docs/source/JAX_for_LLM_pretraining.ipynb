{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIOXoY1xgiww"
   },
   "source": [
    "# Train a miniGPT language model with JAX\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jax-ml/jax-ai-stack/blob/main/docs/source/JAX_for_LLM_pretraining.ipynb)\n",
    "\n",
    "This tutorial demonstrates how to use JAX, [Flax NNX](http://flax.readthedocs.io) and [Optax](http://optax.readthedocs.io) for language model (pre)training using data and tensor [parallelism](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization) for [Single-Program Multi-Data](https://en.wikipedia.org/wiki/Single_program,_multiple_data)). It was originally inspired by the [Keras miniGPT tutorial](https://keras.io/examples/generative/text_generation_with_miniature_gpt/).\n",
    "\n",
    "Here, you will learn how to:\n",
    "\n",
    "- Define the miniGPT model with Flax and JAX automatic parallelism\n",
    "- Load and preprocess the dataset\n",
    "- Create the loss and training step functions\n",
    "- Train the model on Google Colab’s Cloud TPU v2\n",
    "- Profile for hyperparameter tuning\n",
    "\n",
    "If you are new to JAX for AI, check out the [introductory tutorial](https://jax-ai-stack.readthedocs.io/en/latest/neural_net_basics.html), which covers neural network building with [Flax NNX](https://flax.readthedocs.io/en/latest/nnx_basics.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTmz5Cbco7n_"
   },
   "source": [
    "## Setup\n",
    "\n",
    "JAX installation is covered in [this guide](https://jax.readthedocs.io/en/latest/installation.html) on the JAX documentation site. We will use [Tiktoken](https://github.com/openai/tiktoken) for tokenization and [Grain](https://google-grain.readthedocs.io/en/latest/index.html) for data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6zMsOIc7ouCO",
    "outputId": "037d56a9-b18f-4504-f80a-3a4fa2945068"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.7/780.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m54.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.6/289.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.5/270.5 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.1/128.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.15.0 requires ml-dtypes~=0.2.0, but you have ml-dtypes 0.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.0/419.0 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -Uq tiktoken grain matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rcji_799n4eA"
   },
   "source": [
    "**Note:** If you are using [Google Colab](https://colab.research.google.com/), select the free Google Cloud TPU v2 as the hardware accelerator.\n",
    "\n",
    "Check the available JAX devices, or [`jax.Device`](https://jax.readthedocs.io/en/latest/_autosummary/jax.Device.html), with [`jax.devices()`](https://jax.readthedocs.io/en/latest/_autosummary/jax.devices.html). The output of the cell below will show a list of 8 (eight) devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LS9sQEY3n0mB",
    "outputId": "9ffcf3a6-20ef-4f80-b006-f5d3c5644a15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
       " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
       " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
       " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
       " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
       " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
       " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
       " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHzJ_bokoovZ"
   },
   "source": [
    "Get the [TinyStories dataset from Hugging Face](https://huggingface.co/datasets/roneneldan/TinyStories). We only use the training split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wUjQsgQEmI1N",
    "outputId": "e6eff24e-5578-4277-a0f9-24e27bd91ee0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-01 02:50:38--  https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStories-train.txt?download=true\n",
      "Resolving huggingface.co (huggingface.co)... 65.8.243.46, 65.8.243.92, 65.8.243.90, ...\n",
      "Connecting to huggingface.co (huggingface.co)|65.8.243.46|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.hf.co/repos/42/7f/427f7497b6c6596c18b46d5a72e61364fcad12aa433c60a0dbd4d344477b9d81/c5cf5e22ff13614e830afbe61a99fbcbe8bcb7dd72252b989fa1117a368d401f?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27TinyStories-train.txt%3B+filename%3D%22TinyStories-train.txt%22%3B&response-content-type=text%2Fplain&Expires=1730688639&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMDY4ODYzOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy80Mi83Zi80MjdmNzQ5N2I2YzY1OTZjMThiNDZkNWE3MmU2MTM2NGZjYWQxMmFhNDMzYzYwYTBkYmQ0ZDM0NDQ3N2I5ZDgxL2M1Y2Y1ZTIyZmYxMzYxNGU4MzBhZmJlNjFhOTlmYmNiZThiY2I3ZGQ3MjI1MmI5ODlmYTExMTdhMzY4ZDQwMWY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=oQHJBcHVix9N1HnNsJSj7KK-BoqdXdl6NRh%7E1ilGx-ROnLrZxKINfonOtva5e5Xf9KQVNl6QQkx5gNw4iMTmS6JRFB%7EcXdTcFjrHSnBxwLRZkMCBKAv3oHhRnJ6I2rV8iBAZTq%7E-caDCLFvBrgT9pcEFakh3-5mSp%7ER7hnNqE5lcE5n7tzXS0l-8tOShDmR5aUCFPStZHfPbyS3MwCAdc2KoqXdqzRf9M4WvXWB78El7WGxse0DrTQFbGGW1kjpvBOqzljH0Qn6WqsiBockhHDbwE1nQmGfxKrbreXenAKdOsUTN9fuRKl-6srhI2xGKFpfu3IGDEN%7Ebmwg8CnwAfQ__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
      "--2024-11-01 02:50:39--  https://cdn-lfs.hf.co/repos/42/7f/427f7497b6c6596c18b46d5a72e61364fcad12aa433c60a0dbd4d344477b9d81/c5cf5e22ff13614e830afbe61a99fbcbe8bcb7dd72252b989fa1117a368d401f?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27TinyStories-train.txt%3B+filename%3D%22TinyStories-train.txt%22%3B&response-content-type=text%2Fplain&Expires=1730688639&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMDY4ODYzOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy80Mi83Zi80MjdmNzQ5N2I2YzY1OTZjMThiNDZkNWE3MmU2MTM2NGZjYWQxMmFhNDMzYzYwYTBkYmQ0ZDM0NDQ3N2I5ZDgxL2M1Y2Y1ZTIyZmYxMzYxNGU4MzBhZmJlNjFhOTlmYmNiZThiY2I3ZGQ3MjI1MmI5ODlmYTExMTdhMzY4ZDQwMWY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=oQHJBcHVix9N1HnNsJSj7KK-BoqdXdl6NRh%7E1ilGx-ROnLrZxKINfonOtva5e5Xf9KQVNl6QQkx5gNw4iMTmS6JRFB%7EcXdTcFjrHSnBxwLRZkMCBKAv3oHhRnJ6I2rV8iBAZTq%7E-caDCLFvBrgT9pcEFakh3-5mSp%7ER7hnNqE5lcE5n7tzXS0l-8tOShDmR5aUCFPStZHfPbyS3MwCAdc2KoqXdqzRf9M4WvXWB78El7WGxse0DrTQFbGGW1kjpvBOqzljH0Qn6WqsiBockhHDbwE1nQmGfxKrbreXenAKdOsUTN9fuRKl-6srhI2xGKFpfu3IGDEN%7Ebmwg8CnwAfQ__&Key-Pair-Id=K3RPWS32NSSJCE\n",
      "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 3.167.152.12, 3.167.152.119, 3.167.152.37, ...\n",
      "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|3.167.152.12|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1924281556 (1.8G) [text/plain]\n",
      "Saving to: ‘TinyStories-train.txt’\n",
      "\n",
      "TinyStories-train.t 100%[===================>]   1.79G  38.1MB/s    in 45s     \n",
      "\n",
      "2024-11-01 02:51:24 (40.7 MB/s) - ‘TinyStories-train.txt’ saved [1924281556/1924281556]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStories-train.txt?download=true -O TinyStories-train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKE2uUafLobI"
   },
   "source": [
    "Import the necessary modules, including JAX NumPy, Flax NNX, Optax, Grain, pandas, and Tiktoken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MKYFNOhdLq98"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from jax.sharding import Mesh, PartitionSpec as P, NamedSharding # For data and model parallelism (explained in more detail later)\n",
    "from jax.experimental import mesh_utils\n",
    "\n",
    "import flax.nnx as nnx\n",
    "import optax\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import grain.python as pygrain\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPyt7MV6prz1"
   },
   "source": [
    "## Define the miniGPT model with Flax and JAX automatic parallelism\n",
    "\n",
    "### Leveraging JAX's data and tensor parallelism\n",
    "\n",
    "One of the most powerful features of JAX is [device parallelism](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization) for SPMD.\n",
    "\n",
    "- The data parallelism technique enables, for example, the training data to run via multiple parts (this is called sharding) - batches - in parallel and simultaneously across different devices, such as GPUs and Google TPUs. This allows to use larger batch sizes to speed up training.\n",
    "- Tensor parallelism allows us to split the model parameter tensors across several devices (sharding model tensors).\n",
    "- You can learn more about the basics of JAX parallelism in more detail in the [Introduction to parallel programming](https://jax.readthedocs.io/en/latest/sharded-computation.html) on the JAX documentation site.\n",
    "\n",
    "In this example, we'll utilize a 4-way data parallel and 2-way tensor parallel setup. The free Google Cloud TPU v2 on Google Colab offers 4 chips, each with 2 TPU cores. The TPU v2 architeture aligns with the proposed setup.\n",
    "\n",
    "### jax.sharding.Mesh\n",
    "\n",
    "Earlier, we imported [`jax.sharding.Mesh`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.Mesh) - is a multidimensional NumPy array of JAX devices, where each axis of the mesh has a name, such as `'x'` or `'y'`. This will help encapsulate the information about the TPU resource organization for distributing computations across the devices.\n",
    "\n",
    "Our `Mesh` will have two arguments:\n",
    "- `devices`: This will take the value of [`jax.experimental.mesh_utils((4, 2))`](https://jax.readthedocs.io/en/latest/jax.experimental.mesh_utils.html), enabling us to build a device mesh. It is a NumPy ndarray with JAX devices (a list of devices from the JAX backend as obtained from [`jax.devices()`](https://jax.readthedocs.io/en/latest/_autosummary/jax.devices.html#jax.devices))..\n",
    "- `axis_names`, where:\n",
    "  - `batch`: 4 devices along the first axis - i.e. sharded into 4 - for data parallelism; and\n",
    "  - `model`: 2 devices along the second axis - i.e. sharded into 2 -  for tensor paralleism, mapping to the TPU v2 cores.\n",
    "\n",
    "This matches the `(4, 2)` structure in the Colab's TPU v2 setup.\n",
    "\n",
    "Let's instantiate `Mesh` as `mesh` and declare the TPU configuration to define how data and model parameters are distributed across the devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "xuMlCK3Q8WJD"
   },
   "outputs": [],
   "source": [
    "# Create a `Mesh` object representing TPU device arrangement.\n",
    "mesh = Mesh(mesh_utils.create_device_mesh((4, 2)), ('batch', 'model'))\n",
    "\n",
    "### Alternatively, we could use the 8-way data parallelism with only one line of code change.\n",
    "### JAX enables quick experimentation with different partitioning strategies\n",
    "### like this. We will come back to this point at the end of this tutorial.\n",
    "# mesh = Mesh(mesh_utils.create_device_mesh((8, 1)), ('batch', 'model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZKdhNo98NgG"
   },
   "source": [
    "We will use the GPT-2 tokenizer from the [Tiktoken](https://github.com/openai/tiktoken) library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iWbkk1V7-Isg"
   },
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XHQ0BQ9-KIj"
   },
   "source": [
    "To leverage model parallelism, we need to instruct the JAX compiler how to shard the model tensors across the TPU devices. Earlier, we also imported [`jax.sharding.PartitionSpec`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.PartitionSpec) and [`jax.sharding.NamedSharding`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.NamedSharding):\n",
    "- [`PartitionSpec`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.PartitionSpec) (using alias `P`) defines how tensors are sharded across the devices in our `Mesh`. Its elements describe how an input dimension is partitioned across mesh dimensions. For example, in `PartitionSpec('x', 'y')` the first dimension of data is sharded across `x` axis of the mesh, and the second one - across the `y` axis.\n",
    "  - We'll use `PartitionSpec` to describe how to shard a tensor across, for example, the `model` axis or be replicated on other dimensions (which is denoted by `None`).\n",
    "- [`NamedSharding`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.NamedSharding) is a (`Mesh`, `PartitionSpec`) pair that describes how to shard a model tensor across our `mesh`.\n",
    "- We combine `Mesh` (the TPU resources) with `PartitionSpec` and create a `NamedSharding`, which instructs how to shard each model tensor across the TPU devices.\n",
    "\n",
    "Additionally, we'll use Flax NNX's [`flax.nnx.with_partitioning`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/spmd.html#flax.nnx.with_partitioning) to let each model layer know that the model weights or tensors need to be sharded according to our specification. We need to do this for every tensor/layer in the model.\n",
    "- `nnx.with_partitioning` will take two arguments, such as the `initializer` (such as [`flax.nnx.initializers.xavier_uniform`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/initializers.html#flax.nnx.initializers.xavier_uniform) and [`flax.nnx.initializers.zeros_init`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/initializers.html#flax.nnx.initializers.zeros_init)) and `sharding` (e.g. `NamedSharding(Mesh, PartitionSpec)` or `NamedSharding(mesh, P('model')` in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "z0p-IHurrB9i"
   },
   "outputs": [],
   "source": [
    "# Define a triangular mask for causal attention with `jax.numpy.tril` and `jax.numpy.ones`.\n",
    "def causal_attention_mask(seq_len):\n",
    "    return jnp.tril(jnp.ones((seq_len, seq_len)))\n",
    "\n",
    "class TransformerBlock(nnx.Module):\n",
    "    \"\"\" A single Transformer block.\n",
    "\n",
    "    Each Transformer block processes input sequences via self-attention and feed-forward networks.\n",
    "\n",
    "    Args:\n",
    "        embed_dim (int): Embedding dimensionality.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        ff_dim (int): Dimensionality of the feed-forward network.\n",
    "        rngs (flax.nnx.Rngs): A Flax NNX stream of JAX PRNG keys.\n",
    "        rate (float): Dropout rate. Defaults to 0.1.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, *, rngs: nnx.Rngs, rate: float = 0.1):\n",
    "        # Multi-Head Attention (MHA) with `flax.nnx.MultiHeadAttention`.\n",
    "        # Specifies tensor sharding (depending on the mesh configuration)\n",
    "        # where we shard the weights across devices for parallel computation.\n",
    "        self.mha = nnx.MultiHeadAttention(num_heads=num_heads,\n",
    "                                          in_features=embed_dim,\n",
    "                                          kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
    "                                          bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
    "                                          rngs=rngs)\n",
    "        # The first dropout with `flax.nnx.Dropout`.\n",
    "        self.dropout1 = nnx.Dropout(rate=rate)\n",
    "        # First layer normalization with `flax.nnx.LayerNorm`.\n",
    "        self.layer_norm1 = nnx.LayerNorm(epsilon=1e-6,\n",
    "                                         num_features=embed_dim,\n",
    "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), NamedSharding(mesh, P('model'))),\n",
    "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
    "                                         rngs=rngs)\n",
    "        # The first linear transformation for the feed-forward network with `flax.nnx.Linear`.\n",
    "        self.linear1 = nnx.Linear(in_features=embed_dim,\n",
    "                                  out_features=ff_dim,\n",
    "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
    "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
    "                                  rngs=rngs)\n",
    "        # The second linear transformation for the feed-forward network with `flax.nnx.Linear`.\n",
    "        self.linear2 = nnx.Linear(in_features=ff_dim,\n",
    "                                  out_features=embed_dim,\n",
    "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
    "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
    "                                  rngs=rngs)\n",
    "        # The second dropout with `flax.nnx.Dropout`.\n",
    "        self.dropout2 = nnx.Dropout(rate=rate)\n",
    "        # Second layer normalization with `flax.nnx.LayerNorm`.\n",
    "        self.layer_norm2 = nnx.LayerNorm(epsilon=1e-6,\n",
    "                                         num_features=embed_dim,\n",
    "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), NamedSharding(mesh, P(None, 'model'))),\n",
    "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P(None, 'model'))),\n",
    "                                         rngs=rngs)\n",
    "\n",
    "\n",
    "    # Apply the Transformer block to the input sequence.\n",
    "    def __call__(self, inputs, training: bool = False):\n",
    "        input_shape = inputs.shape\n",
    "        _, seq_len, _ = input_shape\n",
    "\n",
    "        # Instantiate the causal attention mask.\n",
    "        mask = causal_attention_mask(seq_len)\n",
    "\n",
    "        # Apply Multi-Head Attention with the causal attention mask.\n",
    "        attention_output = self.mha(\n",
    "            inputs_q=inputs,\n",
    "            mask=mask,\n",
    "            decode=False\n",
    "        )\n",
    "        # Apply the first dropout.\n",
    "        attention_output = self.dropout1(attention_output, deterministic=not training)\n",
    "        # Apply the first layer normalization.\n",
    "        out1 = self.layer_norm1(inputs + attention_output)\n",
    "\n",
    "        # The feed-forward network.\n",
    "        # Apply the first linear transformation.\n",
    "        ffn_output = self.linear1(out1)\n",
    "        # Apply the ReLU activation with `flax.nnx.relu`.\n",
    "        ffn_output = nnx.relu(ffn_output)\n",
    "        # Apply the second linear transformation.\n",
    "        ffn_output = self.linear2(ffn_output)\n",
    "        # Apply the second dropout.\n",
    "        ffn_output = self.dropout2(ffn_output, deterministic=not training)\n",
    "        # Apply the second layer normalization and return the output of the Transformer block.\n",
    "        return self.layer_norm2(out1 + ffn_output)\n",
    "\n",
    "class TokenAndPositionEmbedding(nnx.Module):\n",
    "    \"\"\" Combines token embeddings (words in an input sentence) with\n",
    "    positional embeddings (the position of each word in a sentence).\n",
    "\n",
    "    Args:\n",
    "        maxlen (int): Matimum sequence length.\n",
    "        vocal_size (int): Vocabulary size.\n",
    "        embed_dim (int): Embedding dimensionality.\n",
    "        rngs (flax.nnx.Rngs): A Flax NNX stream of JAX PRNG keys.\n",
    "    \"\"\"\n",
    "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, *, rngs: nnx.Rngs):\n",
    "        # Initialize token embeddings (using `flax.nnx.Embed`).\n",
    "        # Each unique word has an embedding vector.\n",
    "        self.token_emb = nnx.Embed(num_embeddings=vocab_size, features=embed_dim, rngs=rngs)\n",
    "        # Initialize positional embeddings (using `flax.nnx.Embed`).\n",
    "        self.pos_emb = nnx.Embed(num_embeddings=maxlen, features=embed_dim, rngs=rngs)\n",
    "\n",
    "    # Takes a token sequence (integers) and returns the combined token and positional embeddings.\n",
    "    def __call__(self, x):\n",
    "        # Generate a sequence of positions for the input tokens.\n",
    "        positions = jnp.arange(0, x.shape[1])[None, :]\n",
    "        # Look up the positional embeddings for each position in the input sequence.\n",
    "        position_embedding = self.pos_emb(positions)\n",
    "        # Look up the token embeddings for each token in the input sequence.\n",
    "        token_embedding = self.token_emb(x)\n",
    "        # Combine token and positional embeddings.\n",
    "        return token_embedding + position_embedding\n",
    "\n",
    "class MiniGPT(nnx.Module):\n",
    "    \"\"\" A miniGPT transformer model, inherits from `flax.nnx.Module`.\n",
    "\n",
    "    Args:\n",
    "        maxlen (int): Maximum sequence length.\n",
    "        vocab_size (int): Vocabulary size.\n",
    "        embed_dim (int): Embedding dimensionality.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        feed_forward_dim (int): Dimensionality of the feed-forward network.\n",
    "        num_transformer_blocks (int): Number of transformer blocks. Each block contains attention and feed-forward networks.\n",
    "        rngs (nnx.Rngs): A Flax NNX stream of JAX PRNG keys.\n",
    "    \"\"\"\n",
    "    # Initialize miniGPT model components.\n",
    "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, num_heads: int, feed_forward_dim: int, num_transformer_blocks: int, rngs: nnx.Rngs):\n",
    "        # Initiliaze the `TokenAndPositionEmbedding` that combines token and positional embeddings.\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(\n",
    "                    maxlen, vocab_size, embed_dim, rngs=rngs\n",
    "                )\n",
    "        # Create a list of `TransformerBlock` instances.\n",
    "        # Each block processes input sequences using attention and feed-forward networks.\n",
    "        self.transformer_blocks = [TransformerBlock(\n",
    "            embed_dim, num_heads, feed_forward_dim, rngs=rngs\n",
    "        ) for _ in range(num_transformer_blocks)]\n",
    "        # Initialize the output `flax.nnx.Linear` layer producing logits over the vocabulary for next-token prediction.\n",
    "        self.output_layer = nnx.Linear(in_features=embed_dim,\n",
    "                                       out_features=vocab_size,\n",
    "                                       kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
    "                                       bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P(None, 'model'))),\n",
    "                                       rngs=rngs)\n",
    "\n",
    "    def __call__(self, inputs, training: bool = False):\n",
    "        # Pass the input tokens through the `embedding_layer` to get token embeddings.\n",
    "        # Apply each transformer block sequentially to the embedded input, use the `training` flag for the behavior of `flax.nnx.Dropout`.\n",
    "        x = self.embedding_layer(inputs)\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x, training=training)\n",
    "        # Pass the output of the transformer blocks through the output layer,\n",
    "        # and obtain logits for each token in the vocabulary (for next token prediction).\n",
    "        outputs = self.output_layer(x)\n",
    "        return outputs\n",
    "\n",
    "    # Text generation.\n",
    "    def generate_text(self, max_tokens: int, start_tokens: [int], top_k=10):\n",
    "        # Sample the next token from a probability distribution based on\n",
    "        # `logits` and `tok_k` (top-k) sampling strategy.\n",
    "        def sample_from(logits):\n",
    "            logits, indices = jax.lax.top_k(logits, k=top_k)\n",
    "            # Convert logits to probabilities (using `flax.nnx.softmax`).\n",
    "            logits = nnx.softmax(logits)\n",
    "            return jax.random.choice(jax.random.PRNGKey(0), indices, p=logits)\n",
    "\n",
    "        # Generate text one token at a time until the maximum token limit is reached (`maxlen`).\n",
    "        def generate_step(start_tokens):\n",
    "            pad_len = maxlen - len(start_tokens)\n",
    "            # Index of the last token in the current sequence.\n",
    "            sample_index = len(start_tokens) - 1\n",
    "            # If the input is longer than `maxlen`, then truncate it.\n",
    "            if pad_len < 0:\n",
    "                x = jnp.array(start_tokens[:maxlen])\n",
    "                sample_index = maxlen - 1\n",
    "            # If the input is shorter than `maxlen`, then pad it (`pad_len`).\n",
    "            elif pad_len > 0:\n",
    "                x = jnp.array(start_tokens + [0] * pad_len)\n",
    "            else:\n",
    "                x = jnp.array(start_tokens)\n",
    "\n",
    "            # Add a batch dimension.\n",
    "            x = x[None, :]\n",
    "            logits = self(x)\n",
    "            next_token = sample_from(logits[0][sample_index])\n",
    "            return next_token\n",
    "\n",
    "        # Store generated tokens.\n",
    "        generated = []\n",
    "        # Generate tokens until the end-of-text token is encountered or the maximum token limit is reached.\n",
    "        for _ in range(max_tokens):\n",
    "            next_token = generate_step(start_tokens + generated)\n",
    "            # Truncate whatever is after '<|endoftext|>' (stop word)\n",
    "            if next_token == tokenizer.encode('<|endoftext|>', allowed_special={'<|endoftext|>'})[0]:\n",
    "              # Stop text generation if the end-of-text token is encountered.\n",
    "              break\n",
    "            generated.append(int(next_token))\n",
    "        # Decode the generated token IDs into text.\n",
    "        return tokenizer.decode(start_tokens + generated)\n",
    "\n",
    "# Creates the miniGPT model with 4 transformer blocks.\n",
    "def create_model(rngs):\n",
    "    return MiniGPT(maxlen, vocab_size, embed_dim, num_heads, feed_forward_dim, num_transformer_blocks=4, rngs=rngs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igX_eoGNMTGR"
   },
   "source": [
    "Set some hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GRhiDsCrMZRp"
   },
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.n_vocab\n",
    "num_transformer_blocks = 8\n",
    "maxlen = 256\n",
    "embed_dim = 256\n",
    "num_heads = 8\n",
    "feed_forward_dim = 256\n",
    "batch_size = 256 # You can set a bigger batch size if you use Kaggle's Cloud TPU.\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mI1ci-HyMspJ"
   },
   "source": [
    "## Loading and preprocessing the data\n",
    "\n",
    "Data loading and preprocessing with [Grain](https://github.com/google/grain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rGUFsn1GMuzh"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TextDataset:\n",
    "    data: list\n",
    "    maxlen: int\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        # Use Tiktoken for tokenization\n",
    "        encoding = tokenizer.encode(self.data[idx], allowed_special={'<|endoftext|>'})[:self.maxlen]  # Tokenize and truncate\n",
    "        return encoding + [0] * (self.maxlen - len(encoding))  # Pad to maxlen\n",
    "\n",
    "def load_and_preprocess_data(file_path, batch_size, maxlen):\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "      text = f.read()\n",
    "\n",
    "    stories = text.split('<|endoftext|>')\n",
    "    stories = [story+'<|endoftext|>' for story in stories if story.strip()]\n",
    "    df = pd.DataFrame({'text': stories})\n",
    "    data = df['text'].dropna().tolist()\n",
    "    dataset = TextDataset(data, maxlen)\n",
    "\n",
    "    sampler = pygrain.IndexSampler(\n",
    "        len(dataset),\n",
    "        shuffle=False,\n",
    "        seed=42,\n",
    "        shard_options=pygrain.NoSharding(),\n",
    "        num_epochs=num_epochs,\n",
    "    )\n",
    "\n",
    "    dl = pygrain.DataLoader(\n",
    "        data_source=dataset,\n",
    "        sampler=sampler,\n",
    "        operations=[pygrain.Batch(batch_size=batch_size, drop_remainder=True)],\n",
    "    )\n",
    "\n",
    "    return dl\n",
    "\n",
    "text_dl = load_and_preprocess_data('TinyStories-train.txt', batch_size, maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKVSD8KSM1um"
   },
   "source": [
    "## Defining the loss function and training step function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8rRuTmABNV4b"
   },
   "outputs": [],
   "source": [
    "# Defines the loss function using `optax.softmax_cross_entropy_with_integer_labels`.\n",
    "def loss_fn(model, batch):\n",
    "    logits = model(batch[0])\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=batch[1]).mean()\n",
    "    return loss, logits\n",
    "\n",
    "# Define the training step with the `flax.nnx.jit` transformation decorator.\n",
    "@nnx.jit\n",
    "def train_step(model: MiniGPT, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
    "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, logits), grads = grad_fn(model, batch)\n",
    "    metrics.update(loss=loss, logits=logits, lables=batch[1])\n",
    "    optimizer.update(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5um2vkeUNckm"
   },
   "source": [
    "## Training the model\n",
    "\n",
    "Start training. It takes ~50 minutes on Colab.\n",
    "\n",
    "Note that for data parallel, we are sharding the training data along the `batch` axis using `jax.device_put` with `NamedeSharding`.\n",
    "\n",
    "We are also using the `jax.vmap` transformation to produce the target sequences faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ysl6CsfENeJN",
    "outputId": "5dd06dca-f030-4927-a9b6-35d412da535c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial generated text:\n",
      "Once upon a time Christina Raven Liqu Everyday seaw Spl digit mini Hungarian wasteful USC recurrent brawl towers summAvailability manualsidsAvailability Jord staleEarlier 303 Latter soakinginated pierced acquaint propaganda differentlyBesides Splambling Significant processing locals FoundingFlickrverbalSquaresth pixels CON repetitivebass%; dartsKN ushered sim wasteful Qi510 174 (_ Hillaryall hopeddalePref recurrentbassoves AOL ushered Hunt manuals NietzscheidsBY Equ souls correctedresaKN ghamblinguador contest cornerback bannedKN realizedSix summlargest gh fastest req influences cursingosureelse delighted wrecked donors codsedentiallyindaletteogenicAI summ wasteful USCesm shaped Garrett resistance grandchildren souls babyStatementambling fastestirin AWSiden groundedKen%; aboarddogs seaw Sultan Sachs Sonic ArchivesINE darts belts asylumei simette expands targetintergroupon Graveyard Graveyard398Jordan 66 medication Leadership 174?: seaw manuals summ asylumrw slice manualsiries Prometheus� Seat correctedINE denomination summ vastlyKNKN belts?: contest PamelaidiumKN themHI seawKN minions summ squadKN Joker sacredamblingKNuckyKNette 69 Xan 69ourse notificationuku Sitting cosmeticakesGro McAuliffeilles Graveyard differe <-Jordan Archives 180 Puppet cabinetodcast spir305 bannedambling 66 medicationbass victory relatingakespe Rover GarrettPrefppo sim recurrent manualsidsrg eveningsossus asylum Puppet hydra SultanProxy 66 chew Jokeranswer%;Loc Australian awaidiumdale landed Luahangはambling SuddenlyKN victory victory victory victory\n",
      "\n",
      "Step 200, Loss: 4.541538715362549, Elapsed Time: 119.14 seconds\n",
      "Generated text:\n",
      "Once upon a time was a time was was was was very little girl, day was so happy, her little girl, her little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little!!!!\n",
      "\n",
      "Step 400, Loss: 2.8348119258880615, Elapsed Time: 103.58 seconds\n",
      "Generated text:\n",
      "Once upon a time there was a small cat named Jack. Tim was a little girl who lived in the world and wanted to explore the forest. The dog, he had a new toy car and his mommy and daddy. He wanted to be so much. He went to help the ball. He said to be careful to go and said, \"What's wrong!\"\n",
      "After they found the other kids said, Timmy was so excited and he could see his friend. Timmy said, \"Let's play in the other animals, but I want to help.\"\n",
      "Timmy smiled and they played with a while. Timmy said, \"It's not listen to play. \"Thank you. \"You are happy.\"\n",
      "\n",
      "\n",
      "Step 600, Loss: 2.3290421962738037, Elapsed Time: 69.78 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lily. She was very curious and loved to play outside in the garden. One day, she was playing with her toys and saw a big box with a shiny red ball. The ball was shiny and wanted to climb it, but she didn't want to be too far. \n",
      "\"Hi, I can't get my toy!\" said, but Lily said, \"No, you are not yours.\" \n",
      "Lily's mom smiled and said, \"You can help me go to me!\"\n",
      "\"I want to get some candy and it,\" she said. \"That's a big dog.\"\n",
      "The cat said, \"No, we need to go home. I'm going to find some.\" \n",
      "Her mom said, \"Okay, you will get to play with me.\"\n",
      "Lily smiled and said, \"I want to get my favorite toy first!\" \n",
      "Her mom said, \"Okay, but they don't know. They both thought it is so much fun to go on their mom and have fun. They played together, but they did not wait to be friends and they had fun together. \n",
      "\n",
      "\n",
      "Step 800, Loss: 2.049470901489258, Elapsed Time: 84.69 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lucy was playing in the garden. She loved to run and play outside and she had lots of fun. She would always look at all day and her friends in the garden.\n",
      "One day, she was playing outside when she heard a strange noise coming from a loud noise. It was coming from a dark and it started to shake. It was a big dog, so scared.\n",
      "Her mom came into the room and said, \"That's not safe!\"\n",
      "The dog smiled and said, \"Don't worry, I will find my friend.\" Lucy and the dog started to walk in the grass, but soon she heard the sound coming from the garden, and she heard a noise. The dog ran back inside, but the door opened and saw that the shadow had a hole in a hole. Lucy and the dog became friends, and they played together all the things and they never found a beautiful spot to get home.\n",
      "\n",
      "\n",
      "Step 1000, Loss: 1.8924535512924194, Elapsed Time: 76.64 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a girl named Amy. Amy loved to go outside and play with her friends. One day, Amy saw a big tree and asked her friends, \"Can I have some best friends?\" The tree was so big that it made a special tree to get in the tree. \n",
      "Amy went to the tree and asked, \"Can I go on my tree?\" The tree replied, \"Sure, let's play together!\" \n",
      "So, Amy and the tree were very excited. Amy started to play, but it was too heavy for the tree. She started to climb and forth, but it was so far away. Amy's mom saw how fast it was. She asked, \"Why do you think this, but I want to get to be my friend?\" Amy said, \"No, it's too late, so we can't play together!\" Amy's friend started to fight and said, \"I'll be friends. Let's play together!\" Amy and Amy laughed, and laughed together, and they became best friends and they had a fun day together.\n",
      "\n",
      "\n",
      "Step 1200, Loss: 1.7993096113204956, Elapsed Time: 80.74 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Amy. She loved to go outside and play in the sunshine. One day, she went outside to play with her friends. They went on a sunny day and found a shiny ball. \n",
      "Amy wanted to see the ball and it started to fly it. But then, a little girl came to her friend. Her friend saw her and asked if she could join the bird. The bird said yes and Lily took it to her house.\n",
      "Amy was very happy and she started to sing and dance with her friend. They sang together and danced until the sun came up. They laughed and danced together, laughing and having fun. They played together all day and the day long.\n",
      "\n",
      "\n",
      "Step 1400, Loss: 1.7445931434631348, Elapsed Time: 69.74 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a girl named Sue. She was only three years old. Sue loved to explore. One day, she saw a little bird sitting on a branch. Sue was scared. She ran up to her mom and said, \"Mom, what's that noise?\" Her mom smiled and said, \"It's okay, Sue. We don't have any more fun!\"\n",
      "Sue and her mom looked around the house and found a small bird that made her feel better. It was a little bird. Sue and her mom went outside and played in the sun. They had a great time playing and laughing.\n",
      "At the end of the day, Sue said, \"Thank you, Mommy! You're welcome! You are very lucky to be a friend.\" Her mom smiled and said, \"Thank you, mommy. I love your new friend!\"\n",
      "\n",
      "\n",
      "Step 1600, Loss: 1.6877646446228027, Elapsed Time: 74.16 seconds\n",
      "Generated text:\n",
      "Once upon a time, a boy named Jack was walking down the street. He was feeling very scared. His mom told him that he had to go outside and get to play. \n",
      "One day, Jack noticed a little boy playing in the park. He wanted to play with the equipment too, but he was scared. \n",
      "Jack ran over and asked, \"Why did you get me?\" His mom said, \"I don't want to be scared. I'm sorry I can't have the equipment to take the equipment to play.\" \n",
      "Jack thought about the equipment he would make it even more fun with the equipment. So, he started to build the equipment. When he was finished, Jack felt a bit better. \n",
      "The equipment was a way to the playground, but he knew that his mom was not alone, she had to help him get better. She asked him to stay in the playground, but Jack was still happy to have the equipment to stay, but Jack couldn't.\n",
      "Jack learned that being kind was important to help and not be afraid of being ignorant and selfish. He was still happy, but he was still very selfish and always asked the equipment for help.\n",
      "\n",
      "\n",
      "Step 1800, Loss: 1.6456927061080933, Elapsed Time: 85.76 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Mia. Mia loved to play outside in the park. One day, Mia found a shiny coin in the park. She wanted it for her birthday party, but it was too expensive. Mia thought it would not buy the coin for her birthday.\n",
      "So, Mia decided to ask her mom for the coin to buy the coin. The coin looked for a long time and said yes. Mia put the coin back in the park, but it was gone. Mia was very sad.\n",
      "So, Mia asked her mom if she could have a special day. Her mom said yes and they both went home with the coin. Mia was so happy! She said yes, but the coin was gone forever.\n",
      "\n",
      "\n",
      "Step 2000, Loss: 1.6211789846420288, Elapsed Time: 69.42 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a small boy named Tim. Tim was a small boy named Timmy. Timmy loved to play with his toy car and he loved to make noises with it.\n",
      "One day, Timmy and his toy car went to the park with his toy car. Timmy's car got stuck in the car and it crashed. Timmy's car was too fast and it hit a tree with the wheel.\n",
      "Timmy was sad because he loved his car so much. He knew he could help make his toy car better. He said, \"I will make you happy! Let's play together and make a new car.\" And Timmy and his toy car had fun together and they became good friends.\n",
      "\n",
      "\n",
      "Step 2200, Loss: 1.578463077545166, Elapsed Time: 68.18 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lucy. She was only three years old and loved to play with her toys. One day, she found a special treasure, a big, scary toy that was very big and she was so excited to take it home. \n",
      "But, one day, Lucy found a shiny jewel that she was very happy to find it. She opened the door and saw a beautiful necklace in the corner of the room. She wanted to see what it was like. She asked her friend, the necklace, and the necklace said, \"Can I take it home now?\" \n",
      "Lucy smiled and said, \"I can borrow it with a special diamond. It's very special because it belongs to someone else.\"\n",
      "The diamond was so happy to see Lucy and said, \"Thank you, Lucy. You are a great friend!\"\n",
      "\n",
      "\n",
      "Step 2400, Loss: 1.5326989889144897, Elapsed Time: 73.51 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little boy named Timmy. Timmy loved to play outside in the sun. One day, Timmy saw a beautiful flower. It was so pretty!\n",
      "Timmy asked his mom, \"What is this flower?\"\n",
      "His mom replied, \"It's a flower. Do you want to play with it?\"\n",
      "Timmy nodded his head and said, \"Yes, please.\"\n",
      "Timmy was so happy and he played with the flower. They laughed and played until the sun went down.\n",
      "Suddenly, they heard a loud noise. It was a little kitten. The kitten was scared and didn't know what to do. Timmy's mom told him it was a bad ending.\n",
      "\n",
      "\n",
      "Step 2600, Loss: 1.5533288717269897, Elapsed Time: 70.30 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lily. She was very happy because she had a pretty blouse. One day, Lily was playing with her friend Jack came over to play. She asked, \"Can I play with you?\" \n",
      "Jack said, \"Sure, let's play with your friends!\" Lily was excited and said, \"Okay, let's play!\" They ran around the room and played with their new friends. \n",
      "As they were playing, Lily accidentally knocked over the blouse and it fell off the blouse. Jack said, \"Ouch!\" \n",
      "Lily felt embarrassed and said, \"It's okay, Lily. I'm sorry. I didn't mean to break the blouse. Let's play with it again!\" \n",
      "They both started to play and soon the blouse was over. They played together and had a lot of fun. Lily felt so happy that she didn't make her friend laugh again. She realized that sometimes things don't seem too bad and we should be different, but they still have a solution.\n",
      "\n",
      "\n",
      "Step 2800, Loss: 1.5323652029037476, Elapsed Time: 80.58 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little boy named Tim. Tim was a good boy. He liked to play with his toys and his dog, Max. One day, Tim went to the park with his mom.\n",
      "\"Hi, Tim! I want to play with you. Do you want to play with me?\" Tim asked.\n",
      "\"Yes, Tim. Let's play together!\" his mom said.\n",
      "Tim and Max played together all day. They laughed and had lots of fun. Tim and Max were good friends.\n",
      "\"Can we play with our friends?\" Max asked.\n",
      "\"Yes, but we can play together,\" his mom said.\n",
      "Tim and Max played together. They laughed and had fun. Tim and Max were happy to play with them. They were best friends.\n",
      "\n",
      "\n",
      "Step 3000, Loss: 1.5298235416412354, Elapsed Time: 70.76 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lily. She was three years old and loved to explore the world around her. One day, Lily found a shiny jewel on the ground. She picked it up and looked inside the jewel. She picked it up and examined it. She showed it to her mom and they were so proud of her. \n",
      "After she was finished playing, Lily went to bed. She had so much fun that she didn't realize it was her mom had made a mistake. Her mom explained that sometimes it's important to be responsible and to listen to others. Lily was very careful and listened to her mom's advice.\n",
      "From that day on, Lily was careful and she never forgot about the jewel. She always made sure to be careful when she played. And always remember to always be careful when things happened.\n",
      "\n",
      "\n",
      "Step 3200, Loss: 1.471197485923767, Elapsed Time: 72.75 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little boy named Tim. Tim was a curious little boy who loved to play. One day, he found a toy car in the garden. It was red and shiny. Tim was very happy and excited.\n",
      "As Tim went outside to play, he saw a little bird in the grass. The bird had a red car. Tim wanted to help the bird. He said, \"Hello, little bird. Can you help me lift my toy?\" The bird looked at Tim and said, \"Yes, please! It is so nice to be careful. You are smart and smart.\"\n",
      "Tim was very happy to help the bird. He thanked the bird and the bird flew away. Tim learned that helping others can make us feel good and smart. He learned that helping others can make us happy too.\n",
      "\n",
      "\n",
      "Step 3400, Loss: 1.5036591291427612, Elapsed Time: 71.77 seconds\n",
      "Generated text:\n",
      "Once upon a time there was a boy named Tim. He loved to go outside and play. One day, he saw a big tree with lots of branches. He wanted to climb it, but his mom said no.\n",
      "Tim tried to climb the tree, but it was too high. He was getting higher and higher until he could see what was on the top. Then he heard a loud noise coming from outside. He peeked out from the ground and saw a small bird flying above.\n",
      "Tim was so happy to be able to reach the top. He jumped and jumped up and down the tree with excitement. When he was finally caught the bird, it flew down to the tree. Tim was so proud of his success.\n",
      "He thanked the bird and continued climbing. He was very happy that he had climbed the tree, even though he couldn't get down from the tree. From then on, he made sure to stay away from the top of the tree, safe and sound.\n",
      "\n",
      "\n",
      "Step 3600, Loss: 1.4788475036621094, Elapsed Time: 76.91 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little boy named Timmy. Timmy loved to play with his toys and watch cartoons. One day, Timmy's mommy asked him to help his mommy. Timmy was excited to help, so he got a new toy from his grandma's house. \n",
      "After a few days, Timmy's mommy gave him a special gift. Timmy was so excited to receive such a gift! It was a special gift that his grandma gave him a big hug. Timmy was so happy to have such a gift and hugged his mommy. \n",
      "After that, Timmy went to bed that night with a special gift from his grandma. He loved his present so much that he gave it a special gift to his grandma. Timmy was so happy to have a present that he had found such a beautiful gift. \n",
      "The next day, Timmy's grandma gave him a gift and gave him a gift. Timmy was so excited and couldn't wait to tell his grandma. He showed his grandma a gift for him with a present, and they all said thank you to his grandma. Timmy was so happy to receive the gift from her grandma and her gift. \n",
      "The present was that Timmy had!!!!\n",
      "\n",
      "Step 3800, Loss: 1.4588173627853394, Elapsed Time: 88.57 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little boy named Timmy. Timmy loved to play with his toys, but he always wanted to share them with others. One day, Timmy was playing with his toy cars and accidentally broke them. His mom said, \"Don't worry, I will help you fix your toys. Let's make it together.\"\n",
      "So, Timmy went to his room and started to play with his toy cars. He had so much fun playing with it, he didn't even want to lose it. His mom was happy to help, so they took a break and put the toy cars away in the right place. Timmy was very happy that he could share his toys with his mom.\n",
      "\n",
      "\n",
      "Step 4000, Loss: 1.4644711017608643, Elapsed Time: 69.17 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lily. She loved to play in the garden, and even when she saw a beautiful butterfly. She ran over to the butterfly and tried to catch it, but it flew away. \n",
      "Suddenly, a little girl appeared. She looked around and saw her and asked her what she was doing. Lily replied, \"I want to play too, please!\" Her mom replied, \"Okay, but only if you don't like the butterfly, it's better.\" \n",
      "So, Lily decided to go back to the garden to find the butterfly. She walked over to the garden and saw a big, beautiful butterfly. The butterfly flapped its wings and flew away. Lily felt so happy that she ran back to the garden, happy that she could play with the butterfly again.\n",
      "\n",
      "\n",
      "Step 4200, Loss: 1.4640543460845947, Elapsed Time: 72.24 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a boy named Timmy. Timmy loved to play with his toy cars, trucks and even the cars. One day, Timmy's toy car got stuck in a big mud. Timmy tried to get the wheels, but he was too heavy. He tried to push the wheel but it was too heavy. Timmy tried and tried, but he couldn't do it. Timmy was sad, but then he remembered how his mom told him. He was brave and strong, but he knew how to use his car to get it. He was happy again, and he was able to get the wheel.\n",
      "\n",
      "\n",
      "Step 4400, Loss: 1.4368994235992432, Elapsed Time: 66.23 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little boy named Timmy. Timmy loved to play outside and pick flowers. One day, he saw a little boy who was very curious. He went up to him and asked, \"Can I help you?\" The boy replied, \"Sure, you can help me find your way.\"\n",
      "Timmy and the boy searched everywhere but they couldn't find the way up to the boy. They searched everywhere but they couldn't find the answer. Suddenly, the boy said, \"Don't worry, I'll help you find your way.\" Timmy said, \"I know how to solve this problem.\"\n",
      "Timmy's face lit up with excitement and said, \"I'll help you solve this problem. I can find your way home.\" His mom said, \"That's right. Let's find your way and see if we can find a way to find my way back.\" Timmy and the boy looked at each other's hand and said, \"Yes, I'll help you.\"\n",
      "They found a small tree, and Timmy helped his mom get his help. They worked together to search for the little boy's journey. After a few hours, they finally found the perfect path. Timmy was so happy that he could!!!!\n",
      "\n",
      "Step 4600, Loss: 1.4162362813949585, Elapsed Time: 88.24 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little boy named Timmy. Timmy loved to play outside and look at the pretty flowers. One day, Timmy saw a beautiful flower. It was so pretty and he wanted to touch it. \n",
      "But when he got close to the flower, he accidentally dropped it. It broke! Timmy was so sad that he cried and his mom couldn't fix it. She said they would get another special thing, but Timmy was okay. \n",
      "His mom hugged him and said it was okay. She told him that accidents happen and that it's important to be careful when you touch things that can happen.\n",
      "\n",
      "\n",
      "Step 4800, Loss: 1.4360578060150146, Elapsed Time: 66.49 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lily. She loved to play outside in the park. One day, she saw a big tree and wanted to climb it. But her mom told her to be careful because the tree might hurt them.\n",
      "Lily tried to climb the tree, but she was too weak. She fell down and started to cry. Her mom hugged her and told her to go back home. They were very happy and went back to the tree.\n",
      "\n",
      "\n",
      "Step 5000, Loss: 1.4377806186676025, Elapsed Time: 59.31 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lily. She loved to play outside and collect things. One day, she went to the park with her mom. She saw a boy sitting at the bench and he said, \"Hello, doggy!\" Lily felt embarrassed because she didn't like it. She asked the boy, \"What's wrong?\" The boy said, \"My dog's dog bit me!\" Lily said, \"It's too bad. I'm too small.\" The boy felt sad for her.\n",
      "Lily said, \"I'm sorry, but I don't know it's not good.\" She went to her mom and asked if she could borrow some of her toys. Her mom said, \"Sure, you can borrow your toy car. You can borrow it, but be careful.\" Lily smiled and went back to playing with her toy car. She had a lot of fun with her toy car and played with it all day.\n",
      "\n",
      "\n",
      "Step 5200, Loss: 1.3954946994781494, Elapsed Time: 76.86 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lily. She loved to play outside and pick flowers. One day, she saw a beautiful flower in the garden. She picked it up and held it in her hand. She felt so happy that she wanted to show her mom.\n",
      "Her mom came outside and saw that Lily was very sad. She told Lily that the flower was still a flower. Lily felt sorry for being so pretty. She promised to be more careful with the flower.\n",
      "The next day, Lily went back outside to pick some flowers. She found some pretty flowers and showed them to her mom. Her mom was very happy and said that Lily's flower was not as pretty as the flowers in the garden. From that day on, Lily loved to play and make pretty flowers in the garden.\n",
      "\n",
      "\n",
      "Step 5400, Loss: 1.401665449142456, Elapsed Time: 70.89 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little boy named Timmy. Timmy loved to play outside and explore. One day, Timmy saw a big tree with a lot of branches. He thought it looked interesting and decided to climb the tree. \n",
      "Timmy's friend, a wise old owl, said, \"Timmy, don't be careless. I don't want to fall. You need to get hurt.\" \n",
      "Timmy didn't understand why the owl was so wise, so he explained to his friends, \"It's important to always stay safe.\" \n",
      "Timmy felt better and went back to his tree to admire the branches from branch. He knew that even if something is too small, it's always better to be safe and be careful when climbing tree.\n",
      "\n",
      "\n",
      "Step 5600, Loss: 1.3960157632827759, Elapsed Time: 71.91 seconds\n",
      "Generated text:\n",
      "Once upon a time, a little girl named Lily went on a trip with her mommy. They were having so much fun. When they got to the beach, they saw a crab and wanted to touch it.\n",
      "Lily said, \"Mommy, can I touch the crab?\"\n",
      "\"No, it's too dangerous. You should stay in the sand. The crab might be safe,\" her mommy replied.\n",
      "Lily was sad and started to cry. \"I'm sorry, mommy. I didn't want to be hurt.\"\n",
      "Her mommy said, \"It's okay, sweetie. I'm glad you listened to your mommy and not touched the crab. Now you can't touch the crab's safety.\"\n",
      "Lily smiled and hugged her mommy. From that day on, they became good friends and they played together every day.\n",
      "\n",
      "\n",
      "Step 5800, Loss: 1.3838456869125366, Elapsed Time: 73.78 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little boy named Tim. Tim had a big toy car. The car could go very fast. Tim liked to race with his car.\n",
      "One day, Tim saw a big box. He wanted to open it. Tim went to his friend, Sue. Sue saw the box and said, \"I can open this box!\" Sue wanted to open the box. They opened the box. Inside, there was a big, soft teddy bear. The bear was happy.\n",
      "Tim and Sue played with the bear all day. They were not very good at all. They played with the toys and had lots of fun. They learned that being kind was better and to help others. And from that day on, Tim and Sue were the best of friends.\n",
      "\n",
      "\n",
      "Step 6000, Loss: 1.4077720642089844, Elapsed Time: 70.17 seconds\n",
      "Generated text:\n",
      "Once upon a time, a little girl named Sue and her dog, Spot. They lived in a big house with their mom and dad. One day, they found a shiny rock that they had to take home. They took it home and put it in their room.\n",
      "Sue's mom saw the shiny rock and said, \"Oh, my toy! Your toy is very special! You should take it home.\" Sue was happy to have her toy back and said, \"Thank you, Spot!\"\n",
      "But then, a big wind came and blew the rock back. Sue and Spot got scared and started to cry. Their mom came and saw what happened. She told them not to worry and they both went to the toy store to get the shiny rock.\n",
      "The moral of the story is that it's important to take care of our things, but we can also take care of them. If you take care of them, we might find them and help them find them.\n",
      "\n",
      "\n",
      "Step 6200, Loss: 1.3794877529144287, Elapsed Time: 77.59 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lily. She loved to play outside and pick flowers. One day, she went to the park with her mommy. She saw a boy crying because he lost his toy car.\n",
      "Lily said, \"I lost my toy car, but I can't find it.\"\n",
      "Lily asked the boy, \"Have you seen my toy car?\" The boy said, \"No, I haven't seen it.\"\n",
      "Lily felt sad and went to play on the swings. She went to the boy and said, \"Thank you, you're my friend.\" The boy said, \"You are very kind. Can you help me find your toy car?\"\n",
      "Lily smiled and said, \"I will help you find your toy car. Maybe you can find your toy car.\"\n",
      "\n",
      "\n",
      "Step 6400, Loss: 1.3905563354492188, Elapsed Time: 73.45 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lily. She loved to play outside in the sun. One day, she saw a big, round thing on the ground. It was a pretty, shiny rock.\n",
      "Lily picked up the rock and put it on her foot. She was so happy and showed her mom the rock. Her mom was proud of her and gave her a big hug. Lily felt happy too.\n",
      "Later that day, Lily and her mom went to the park. Lily saw a big slide. She wanted to slide down it too. She ran up the slide, and slid down fast. The wind blew and Lily fell. She was hurt and sad.\n",
      "Lily learned to be careful when she slide. She went to the swings, and when she was playing, she felt much better. Her mom gave her a hug and told her to keep the smooth rock. Lily learned that she should always keep the rock safe.\n",
      "\n",
      "\n",
      "Step 6600, Loss: 1.39736008644104, Elapsed Time: 76.19 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lily. She loved to play with her toys and her favorite toy was a teddy bear. One day, Lily was playing with her teddy bear when she saw a shiny object on the ground. She picked it up and looked at it closely. \n",
      "\"Wow, look at that object!\" said Lily.\n",
      "\"I want it!\" said her mom. \"It's a special object. It's shiny and it's very special to me.\"\n",
      "Lily thought for a moment and then decided to take the object from her mom to her friend. \"Look, I found a pretty bracelet!\" said Lily. \n",
      "Her friend said, \"That's great, Lily! Let's play with it.\" And so, they played with the bracelet all day long. They were having so much fun!\n",
      "\n",
      "\n",
      "Step 6800, Loss: 1.4118915796279907, Elapsed Time: 72.23 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lily. She loved to draw with her toys and make pictures. One day, she was playing in her backyard when she saw a big bird sitting on a branch. The bird was so pretty! Lily wanted to be friends with the bird, so she tried to sing with it. But the bird didn't fly away. Lily was sad and didn't know what to do.\n",
      "Then, Lily saw a bird that it was very graceful. She asked the bird what was wrong. The bird said that it had wings to fly away. Lily thought that it was a fun idea and flew away. She was happy again and went back to her house to play. From that day on, Lily knew that if she wanted to be friends with her friend, she could help the bird feel better.\n",
      "\n",
      "\n",
      "Step 7000, Loss: 1.4061782360076904, Elapsed Time: 72.51 seconds\n",
      "Generated text:\n",
      "Once upon a time, a little girl named Sue lived in a big house. Sue had a toy car that she loved to play with. One day, Sue's mom told her not to clean up. Sue listened to her mom and started to clean her room.\n",
      "After cleaning, Sue felt tired and needed to take a nap. She put on her clean, but her mom was not looking well. Sue went to her room and sat down to rest. Her mom said, \"Sue, I'm going to rest and drink a big drink.\" Sue smiled and felt better.\n",
      "Sue went to bed with her mom to rest her room. In the end, she went to sleep with her mom. She felt happy that she was able to help her mom. From that day on, Sue knew that her mom loved her because she loved her little sister so much.\n",
      "\n",
      "\n",
      "Step 7200, Loss: 1.381988286972046, Elapsed Time: 73.38 seconds\n",
      "Generated text:\n",
      "Once upon a time, a little girl named Lily went to the park with her mommy. She saw a boy who looked sad. She went to her mommy and asked her if she could have a turn to her.\n",
      "\"Sure, but you have to ask your mommy,\" her mommy said. \"It's okay, but it's important to ask nicely.\"\n",
      "Lily nodded and gave the boy a hug. \"Can we play on the swings?\" she asked.\n",
      "The boy didn't like it and said, \"Sure, but be careful.\"\n",
      "Lily felt happy and went to play on the swings. She went back to the boy and asked if he wanted to swing too. The boy said, \"I want to swing like you, but it's not safe.\"\n",
      "Lily went to the swing and pushed the boy away. She was sad that she lost her favorite toy. The boy was upset and didn't want to swing anymore. They both sat on the swing and watched the boy play on the swings. Lily felt better and said, \"Thank you for letting me swing, Timmy. You're my friend.\"\n",
      "\n",
      "\n",
      "Step 7400, Loss: 1.3883332014083862, Elapsed Time: 83.07 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lily. She loved to play outside in the garden and feel the warm sunshine on her skin. One day, while she was playing, she noticed that the flowers had bloomed in a beautiful butterfly. She wanted to see the butterfly so she asked her mommy if she could go to the butterfly's home. Her mommy said yes and Lily ran to her mommy's room. \n",
      "When she got there, she saw that the butterfly was sad and started to cry. Her mommy told her that she couldn't go back to the garden to get her dress and her mommy was not happy. Lily felt bad and didn't want to go back to the garden to get her dress. \n",
      "The next day, Lily saw the butterfly again and ran back to her mommy's house. She was happy to see her mommy again and said she was a great helper. Her mommy was so happy that she could see the butterfly again and that was a beautiful butterfly who was a good girl.\n",
      "\n",
      "\n",
      "Step 7600, Loss: 1.3650509119033813, Elapsed Time: 80.14 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lily. She loved to play with her toys, but one day she got bored. She went to her friend, Tim, and asked if he could borrow his toy.\n",
      "Tim said, \"No, Lily. This is my toy car. You can play with it, but I want to borrow your toy car.\"\n",
      "Lily felt sad because she didn't want to play with Tim. She said, \"Please, Tim. We can borrow your toy car. I can borrow it for my birthday.\"\n",
      "Tim took the toy car and said, \"No, Lily. I can borrow it for my birthday.\" He played with the toy car all day. He was happy with his new toy car.\n",
      "\n",
      "\n",
      "Step 7800, Loss: 1.3602105379104614, Elapsed Time: 69.70 seconds\n",
      "Generated text:\n",
      "Once upon a time, a little girl named Lily went to a park with her mommy. The park was a pretty green place, with a big pond with lots of ducks. Lily loved the sound of the water, so she decided to play in the water.\n",
      "Lily saw a butterfly flying in the sky and tried to catch it. But the butterfly was too fast, and she couldn't catch it. Suddenly, the butterfly landed on a flower and Lily started to panic.\n",
      "Her mommy saw the butterfly and asked, \"What happened, Lily?\"\n",
      "\"The butterfly fell and hurt my hand,\" said Lily.\n",
      "Her mommy came over and saw the butterfly and asked, \"Are you okay, Lily?\"\n",
      "\"It hurts, sweetie,\" said her mommy.\n",
      "Lily's mommy took a big spoon and the butterfly was very pretty and had lots of colors on it. Lily felt much better and was able to catch the butterfly. She was happy to have her friend a little friend, and they played together all day long.\n",
      "\n",
      "\n",
      "Step 8000, Loss: 1.3417373895645142, Elapsed Time: 79.57 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little boy named Tim. Tim had a toy named Sam. Sam liked to play with the toy car. One day, Tim's mom said they were going to a new park. Tim wanted to play on the swings, but his mom said no.\n",
      "Tim started playing with the car. He saw a big dog in the park. The dog barked loudly. Tim felt scared. He wanted to run away. He went to his mom and dad and said they were safe. Tim felt safe.\n",
      "Later, Tim and Sam went to the park. They saw a big slide. They both wanted to go on the slide. They ran to the slide and slid down the slide. Tim's mom and dad helped them get down. They all laughed and had a great time together.\n",
      "\n",
      "\n",
      "Step 8200, Loss: 1.3874244689941406, Elapsed Time: 71.68 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little boy named Timmy. Timmy loved to play with his toy car, but his favorite thing to do was to go to the store. Timmy didn't like the store, so he went to the store.\n",
      "When they got to the store, Timmy saw a toy car and wanted it. He asked his mom for a toy car and her mom said, \"No, it's too expensive.\" Timmy didn't want to buy a toy car. He thought the toy car wouldn't have a toy car, but his mom said it was okay.\n",
      "Later that day, Timmy went to the park with his mom. Timmy saw a boy who looked sad because he lost his toy car. Timmy felt sorry for the boy and said, \"I can help you find my toy car.\" But he couldn't find his toy car, and he didn't know what to do.\n",
      "Timmy asked his mom, \"Why are you looking for my toy car, and the boy can have it back?\" His mom said, \"Don't worry, we can fix it. Let's find your toy car.\" They found the toy car and it was in the store. Timmy was happy that he found a!!!!\n",
      "\n",
      "Final generated text:\n",
      "Once upon a time, there was a little girl named Lily. She loved to play outside and pick flowers. One day, Lily saw a butterfly and tried to catch it, but it flew away. Lily felt sad because she didn't know what was happening.\n",
      "Suddenly, her friend, a little bird named Timmy, came to visit. \"Don't worry, Timmy,\" said Lily. \"I can catch it!\"\n",
      "\"I'm sorry, Lily,\" said Timmy. \"I just need to help my mom.\"\n",
      "\"Thank you, Timmy,\" said Lily. \"You are very kind to me.\"\n",
      "From that day on, Lily learned to always help others when they needed it. And every time she saw the butterfly, she felt happy again.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = create_model(rngs=nnx.Rngs(0))\n",
    "optimizer = nnx.Optimizer(model, optax.adam(1e-3))\n",
    "metrics = nnx.MultiMetric(\n",
    "  loss=nnx.metrics.Average('loss'),\n",
    ")\n",
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "start_prompt = \"Once upon a time\"\n",
    "start_tokens = tokenizer.encode(start_prompt)[:maxlen]\n",
    "generated_text = model.generate_text(\n",
    "    maxlen, start_tokens\n",
    ")\n",
    "print(f\"Initial generated text:\\n{generated_text}\\n\")\n",
    "\n",
    "\n",
    "metrics_history = {\n",
    "  'train_loss': [],\n",
    "}\n",
    "\n",
    "prep_target_batch = jax.vmap(lambda tokens: jnp.concatenate((tokens[1:], jnp.array([0]))))\n",
    "\n",
    "step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    for batch in text_dl:\n",
    "        if len(batch) % len(jax.devices()) != 0:\n",
    "          continue  # skip the remaining elements\n",
    "        input_batch = jnp.array(jnp.array(batch).T)\n",
    "        target_batch = prep_target_batch(input_batch)\n",
    "        train_step(model, optimizer, metrics, jax.device_put((input_batch, target_batch), NamedSharding(mesh, P('batch', None))))\n",
    "\n",
    "        if (step + 1) % 200 == 0:\n",
    "          for metric, value in metrics.compute().items():\n",
    "              metrics_history[f'train_{metric}'].append(value)\n",
    "          metrics.reset()\n",
    "\n",
    "          elapsed_time = time.time() - start_time\n",
    "          print(f\"Step {step + 1}, Loss: {metrics_history['train_loss'][-1]}, Elapsed Time: {elapsed_time:.2f} seconds\")\n",
    "          start_time = time.time()\n",
    "\n",
    "          generated_text = model.generate_text(\n",
    "              maxlen, start_tokens\n",
    "          )\n",
    "          print(f\"Generated text:\\n{generated_text}\\n\")\n",
    "        step += 1\n",
    "\n",
    "# Final text generation\n",
    "generated_text = model.generate_text(\n",
    "    maxlen, start_tokens\n",
    ")\n",
    "print(f\"Final generated text:\\n{generated_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thaLs6TD0lt5"
   },
   "source": [
    "Visualize the training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "B6Eg1Cz2y_iP",
    "outputId": "7cafe711-1ae4-4eb9-fd37-e1bde54cbfc5"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(metrics_history['train_loss'])\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WB-ExEt1Zl1C"
   },
   "source": [
    "As you can see, the model goes from generating completely random words at the beginning to generating sensible tiny stories at the end of the training. So essentially we have pretrained a small LLM to write tiny stories for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "soPqiR1JNmjf"
   },
   "source": [
    "## Saving the checkpoint\n",
    "\n",
    "Save the model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EkoFGCgSZ1yz",
    "outputId": "3467b8ba-ce05-42f0-fb89-75922cc91e31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CHECKPOINT_METADATA  d  manifest.ocdbt  _METADATA  ocdbt.process_0  _sharding\n"
     ]
    }
   ],
   "source": [
    "import orbax.checkpoint as orbax\n",
    "\n",
    "state = nnx.state(model)\n",
    "\n",
    "checkpointer = orbax.PyTreeCheckpointer()\n",
    "checkpointer.save('/content/save', state)\n",
    "\n",
    "# Make sure the files are there\n",
    "!ls /content/save/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3813cbf2",
   "metadata": {},
   "source": [
    "## Profiling for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d933c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq tensorboard-plugin-profile tensorflow tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac5fc4d",
   "metadata": {},
   "source": [
    "Load the tensorboard colab extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f0c212",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c6131f",
   "metadata": {},
   "source": [
    "As we're going to be running this model a number of times, we need some scaffolding to more easily compare our work. For a baseline, we'll need to perform some warmup to guarantee that our code is JIT'd and that our TPUs are warm. For improved comparability, we'll only start tracing after we've finished warmup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfd576e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_dir = \"/tmp/jax-trace/\"\n",
    "\n",
    "def loop_step(batch, step):\n",
    "    input_batch = jnp.array(jnp.array(batch).T)\n",
    "    target_batch = prep_target_batch(input_batch)\n",
    "    train_step(model, optimizer, metrics, jax.device_put((input_batch, target_batch), NamedSharding(mesh, P('batch', None))))\n",
    "\n",
    "def generate_trace():\n",
    "    tracing_steps = 30\n",
    "    warmup_steps = 5\n",
    "    for current_step in range(warmup_steps + tracing_steps):\n",
    "        if current_step == warmup_steps:\n",
    "            jax.profiler.start_trace(trace_dir)\n",
    "        with jax.profiler.StepTraceAnnotation(\"train\", step_num=current_step):\n",
    "            batch = next(text_dl)\n",
    "            loop_step(batch, current_step)\n",
    "\n",
    "    jax.profiler.stop_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de70f5b7",
   "metadata": {},
   "source": [
    "Now we'll perform some traces to compare results of different batch sizes. This will take several minutes as we need to reprocess our input data to prepare new batches each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9452a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_dir = \"/tmp/jax-trace-batch-comparison/\"\n",
    "\n",
    "batch_size = 64\n",
    "text_dl = iter(load_and_preprocess_data('TinyStories-train.txt', batch_size, maxlen))\n",
    "generate_trace()\n",
    "\n",
    "batch_size = 256\n",
    "text_dl = iter(load_and_preprocess_data('TinyStories-train.txt', batch_size, maxlen))\n",
    "generate_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea379965",
   "metadata": {},
   "source": [
    "Run Tensorboard with the Profiler Plugin to compare our runs. Runs are listed in order from newest to oldest, so the top run in the list will be have `batch_size = 256`.\n",
    "\n",
    "The key metrics to focus on here for this hyperparameter are FLOPS Utilization and Average Step Time.\n",
    "\n",
    "In general, we want to maximize FLOPS Utilization while minimizing the step time per training example. In this case, we can see that increasing the batch size from 64 -> 256 achieves both of those. FLOPS increases from 16% to 27%. Average Step Time increase from 100ms to 260ms, however we increased our batch size by 300%. This means we move from 1.5ms per training example to 1.02ms per training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86c565a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=$trace_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657967a5",
   "metadata": {},
   "source": [
    "Next, we can explore alternative parallelism methods. In cell #4, we used 4-way data parallel and 2-way tensor parallel. 8-way data parallel is another popular way. Let's compare results between them. To switch to 8-way data parallel, we'll replace the `Mesh` definition with:\n",
    "\n",
    "`mesh = Mesh(mesh_utils.create_device_mesh((8, 1)), ('batch', 'model'))`\n",
    "\n",
    "JAX will automatically figure out how to shard the model and data to use the new partition strategy and nothing else need to be done. Re-connect the TPU runtime and run it again to see how it runs.\n",
    "\n",
    "How simple and powerful is this! And that's the beauty of JAX automatic parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80daa8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_dir = \"/tmp/jax-trace-parallelism-comparison/\"\n",
    "\n",
    "mesh = Mesh(mesh_utils.create_device_mesh((4, 2)), ('batch', 'model'))\n",
    "generate_trace()\n",
    "\n",
    "mesh = Mesh(mesh_utils.create_device_mesh((8, 1)), ('batch', 'model'))\n",
    "generate_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad96e72b",
   "metadata": {},
   "source": [
    "Once again we'll run tensorboard.\n",
    "\n",
    "Looking at the results, we see that the step times are nearly the same, however the FLOPS Utilization is at 13% for 8-way data parallelism compared to 27% or 4-way data parallelism.\n",
    "\n",
    "By looking at the Trace Viewer tool and looking under each TPU's ops, we can see that the TPUs spend a large amount of time idle while waiting for the host, as well as spending a good amount of time in `reduce_sum` operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780e9c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=$trace_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deca486e",
   "metadata": {},
   "source": [
    "By changing hyperparameters and comparing profiles, we're able to gain significant insights into our bottlenecks and limitations. These are just two examples of hyperparameters to tune, but plenty more of them will have significant effects on training speed and resource utilization."
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "machine_shape": "hm",
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
