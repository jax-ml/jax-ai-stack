{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dA5x53bGMT2w"
   },
   "source": [
    "# Train a miniGPT language model with JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dNvPJpcW7esj"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/jax-ml/jax-ai-stack/blob/main/docs/source/JAX_for_LLM_pretraining.ipynb\"><img src=\"https://www.kaggle.com/static/images/logos/kaggle-logo-transparent-300.png\" height=\"32\" width=\"70\"/>Run in Kaggle</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/jax-ml/jax-ai-stack/blob/main/docs/source/JAX_for_LLM_pretraining.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/jax-ml/jax-ai-stack/blob/main/docs/source/JAX_for_LLM_pretraining.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIOXoY1xgiww"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Welcome to this comprehensive tutorial on training a miniGPT language model using JAX and its AI ecosystem. This hands-on guide will walk you through the complete process of building, training, and optimizing a small but functional GPT-style language model.\n",
    "\n",
    "**What you'll learn:**\n",
    "\n",
    "This tutorial demonstrates how to use JAX, [Flax NNX](http://flax.readthedocs.io) and [Optax](http://optax.readthedocs.io) for language model (pre)training using data and tensor [parallelism](https://docs.jax.dev/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html). It was originally inspired by the [Keras miniGPT tutorial](https://keras.io/examples/generative/text_generation_with_miniature_gpt/).\n",
    "\n",
    "By the end of this tutorial, you will be able to:\n",
    "\n",
    "- **Understand JAX's parallelism capabilities**: Learn how to leverage data and tensor parallelism to distribute training across multiple TPU/GPU devices\n",
    "- **Build transformer models with Flax NNX**: Define a GPT-style architecture using the modern Flax NNX API\n",
    "- **Process data efficiently with Grain**: Load and preprocess training data using Google's Grain data loading library\n",
    "- **Optimize models with Optax**: Implement training loops using JAX's gradient transformation library\n",
    "- **Train on cloud TPUs or GPUs**: Execute training using available accelerators (TPU, GPU) , while still maintaining the ability to run on CPU if needed\n",
    "- **Fine-tune with LoRA**: Apply parameter-efficient fine-tuning using the Tunix library\n",
    "- **Profile and optimize**: Use JAX's profiling tools to identify bottlenecks and tune hyperparameters\n",
    "\n",
    "**What to expect:**\n",
    "\n",
    "You'll train a miniGPT model from scratch on the TinyStories dataset, watch it progress from generating random text to coherent short stories, and then fine-tune it on Shakespeare's works to change its writing style. The entire training process takes approximately 20 minutes on a free Colab TPU.\n",
    "\n",
    "**Prerequisites:**\n",
    "\n",
    "If you are new to JAX for AI, check out the [introductory tutorial](https://docs.jaxstack.ai/en/latest/neural_net_basics.html), which covers neural network building with [Flax NNX](https://flax.readthedocs.io/en/latest/nnx_basics.html). Basic familiarity with Python, neural networks, and transformers is helpful but not required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTmz5Cbco7n_"
   },
   "source": [
    "## Setup\n",
    "\n",
    "JAX installation is covered in [this guide](https://jax.readthedocs.io/en/latest/installation.html) on the JAX documentation site. We will use [Tiktoken](https://github.com/openai/tiktoken) for tokenization and [Grain](https://google-grain.readthedocs.io/en/latest/index.html) for data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uq tiktoken jax-ai-stack[grain] matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rcji_799n4eA"
   },
   "source": [
    "**Note:** If you are using [Kaggle](https://www.kaggle.com/), select the free TPU v5e-8 as the hardware accelerator. If you are using [Google Colab](https://colab.research.google.com/), select the free Google Cloud TPU v5e-1 as the hardware accelerator. You may also use Google Cloud TPUs.\n",
    "\n",
    "Check the available JAX devices, or [`jax.Device`](https://jax.readthedocs.io/en/latest/_autosummary/jax.Device.html), with [`jax.devices()`](https://jax.readthedocs.io/en/latest/_autosummary/jax.devices.html). The output of the cell below will show a list of 8 (eight) devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/jax/_src/cloud_tpu_init.py:93: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if an accelerator (GPU/TPU) is available\n",
    "accelerator_available = any(d.platform in ('gpu', 'tpu') for d in jax.devices())\n",
    "\n",
    "if not accelerator_available:\n",
    "    import warnings\n",
    "    warnings.warn(\n",
    "        \"WARNING: No GPU or TPU accelerator found. This notebook will still run, but using only the CPU can be much slower—training may take longer than intended.\\n\"\n",
    "        \"For the best experience, it is highly recommended to use an accelerator:\\n\"\n",
    "        \"  • Kaggle: Go to Settings → Accelerator → Choose GPU or TPU\\n\"\n",
    "        \"  • Colab: Go to Runtime → Change runtime type → Select GPU or TPU\\n\"\n",
    "        \"You can continue, but expect reduced performance.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHzJ_bokoovZ"
   },
   "source": [
    "Get the [TinyStories dataset from Hugging Face](https://huggingface.co/datasets/roneneldan/TinyStories). We only use the training split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-02-06 14:30:53--  https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStories-train.txt?download=true\n",
      "Resolving huggingface.co (huggingface.co)... 18.161.6.33, 18.161.6.46, 18.161.6.107, ...\n",
      "Connecting to huggingface.co (huggingface.co)|18.161.6.33|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://us.gcp.cdn.hf.co/xet-bridge-us/645e8da96320b0efe40ade7a/e2a1497efc1aa51b2da2a849d5dd2cd153d5bc024901afeade7e35379d8f7b52?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27TinyStories-train.txt%3B+filename%3D%22TinyStories-train.txt%22%3B&response-content-type=text%2Fplain&Expires=1770391853&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiRXBvY2hUaW1lIjoxNzcwMzkxODUzfX0sIlJlc291cmNlIjoiaHR0cHM6Ly91cy5nY3AuY2RuLmhmLmNvL3hldC1icmlkZ2UtdXMvNjQ1ZThkYTk2MzIwYjBlZmU0MGFkZTdhL2UyYTE0OTdlZmMxYWE1MWIyZGEyYTg0OWQ1ZGQyY2QxNTNkNWJjMDI0OTAxYWZlYWRlN2UzNTM3OWQ4ZjdiNTJcXD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=sQ8F7aUm7G49ojt-onfB2oqcvMzjrWHQVzIEr3rj4wGkEjtmJ1-nvDvQSqKAe-JztgyDHudljM4ggexx15dIJLTjwU0iyhc2euSdsdp6mNI%7ER1W-ACgcxAcNnbb8%7EmvbjlasisOhrbqgvnJCDRfym2QCWL3nVEpCDlqNCbXvmSlVhD54XFMuprL737TaMSJLHFMYHxGIWNZD2MozLe14P36qqN8bctOp15LpQT3zsyOuB5bV0gX3Lw5iyO27i0nDIVSyifJPoDRxUD-y6G-a54HRDTHFG-gPGnk-wPrKaU-TMc77yk0jZMYNdmKeZOeu6%7E8HT6YP424ZjFewRRMHDg__&Key-Pair-Id=KJLH8B0YWU4Y8M [following]\n",
      "--2026-02-06 14:30:53--  https://us.gcp.cdn.hf.co/xet-bridge-us/645e8da96320b0efe40ade7a/e2a1497efc1aa51b2da2a849d5dd2cd153d5bc024901afeade7e35379d8f7b52?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27TinyStories-train.txt%3B+filename%3D%22TinyStories-train.txt%22%3B&response-content-type=text%2Fplain&Expires=1770391853&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiRXBvY2hUaW1lIjoxNzcwMzkxODUzfX0sIlJlc291cmNlIjoiaHR0cHM6Ly91cy5nY3AuY2RuLmhmLmNvL3hldC1icmlkZ2UtdXMvNjQ1ZThkYTk2MzIwYjBlZmU0MGFkZTdhL2UyYTE0OTdlZmMxYWE1MWIyZGEyYTg0OWQ1ZGQyY2QxNTNkNWJjMDI0OTAxYWZlYWRlN2UzNTM3OWQ4ZjdiNTJcXD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=sQ8F7aUm7G49ojt-onfB2oqcvMzjrWHQVzIEr3rj4wGkEjtmJ1-nvDvQSqKAe-JztgyDHudljM4ggexx15dIJLTjwU0iyhc2euSdsdp6mNI%7ER1W-ACgcxAcNnbb8%7EmvbjlasisOhrbqgvnJCDRfym2QCWL3nVEpCDlqNCbXvmSlVhD54XFMuprL737TaMSJLHFMYHxGIWNZD2MozLe14P36qqN8bctOp15LpQT3zsyOuB5bV0gX3Lw5iyO27i0nDIVSyifJPoDRxUD-y6G-a54HRDTHFG-gPGnk-wPrKaU-TMc77yk0jZMYNdmKeZOeu6%7E8HT6YP424ZjFewRRMHDg__&Key-Pair-Id=KJLH8B0YWU4Y8M\n",
      "Resolving us.gcp.cdn.hf.co (us.gcp.cdn.hf.co)... 34.120.165.110\n",
      "Connecting to us.gcp.cdn.hf.co (us.gcp.cdn.hf.co)|34.120.165.110|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1924281556 (1.8G) [text/plain]\n",
      "Saving to: ‘TinyStories-train.txt’\n",
      "\n",
      "TinyStories-train.t 100%[===================>]   1.79G  3.55MB/s    in 5m 19s  \n",
      "\n",
      "2026-02-06 14:36:12 (5.76 MB/s) - ‘TinyStories-train.txt’ saved [1924281556/1924281556]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStories-train.txt?download=true -O TinyStories-train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKE2uUafLobI"
   },
   "source": [
    "Import the necessary modules, including JAX NumPy, Flax NNX, Optax, Grain, pandas, and Tiktoken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding\n",
    "\n",
    "import flax.nnx as nnx\n",
    "import optax\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import grain.python as pygrain\n",
    "import tiktoken\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XhqjGAFMT2w"
   },
   "source": [
    "## JAX: High-performance array computing\n",
    "\n",
    "[JAX](https://jax.readthedocs.io) is a Python library for high-performance numerical computing and machine learning research. It combines the familiar NumPy API with powerful program transformations to enable automatic differentiation, vectorization, and parallelization.\n",
    "\n",
    "**Key features of JAX:**\n",
    "\n",
    "- **NumPy compatibility**: JAX provides `jax.numpy`, a drop-in replacement for NumPy that runs on accelerators like GPUs and TPUs\n",
    "- **Automatic differentiation**: The `jax.grad()` function computes gradients automatically, making it easy to implement gradient-based optimization\n",
    "- **JIT compilation**: `jax.jit()` compiles Python functions to optimized machine code for faster execution\n",
    "- **Automatic parallelization**: JAX can automatically distribute computations across multiple devices using SPMD (Single Program, Multiple Data) parallelism\n",
    "- **Functional programming**: JAX encourages pure functions, which enables reliable transformations and better performance\n",
    "\n",
    "For training large language models like our miniGPT, JAX's automatic parallelization capabilities are particularly valuable. They allow us to efficiently utilize multiple GPU or TPU cores without manually managing device placement and communication, making the most of whichever accelerator hardware is available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4-gksXxMT3B"
   },
   "source": [
    "The three core JAX transforms you'll need understand are:\n",
    "- **[`jax.jit`](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html)**: Just-in-time compilation via XLA for fast execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392 µs ± 17 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "216 µs ± 5.67 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n"
     ]
    }
   ],
   "source": [
    "def slow_fn(x):\n",
    "    for _ in range(5):\n",
    "        x = x @ x\n",
    "    return x\n",
    "\n",
    "fast_fn = jax.jit(slow_fn)\n",
    "\n",
    "x = jnp.ones((1000, 1000))\n",
    "\n",
    "# The first call to the jitted function triggers compilation for the given input shape/dtype\n",
    "# (in this case, shape (1000, 1000)). Subsequent calls with the same shape/dtype are fast,\n",
    "# as they use the cached compiled executable.\n",
    "fast_fn(x).block_until_ready()\n",
    "\n",
    "%timeit slow_fn(x).block_until_ready()\n",
    "%timeit fast_fn(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Binla6QaMT3B"
   },
   "source": [
    "- **[`jax.grad`](https://jax.readthedocs.io/en/latest/jax-101/04-advanced-autodiff.html)**: Automatic differentiation for computing gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [1. 2. 3.]\n",
      "loss(x) = 14.0\n",
      "grad(loss)(x) = [2. 4. 6.]\n"
     ]
    }
   ],
   "source": [
    "def loss(x):\n",
    "    return jnp.sum(x ** 2)\n",
    "\n",
    "grad_loss = jax.grad(loss)\n",
    "\n",
    "x = jnp.array([1.0, 2.0, 3.0])\n",
    "print(f\"x = {x}\")\n",
    "print(f\"loss(x) = {loss(x)}\")\n",
    "print(f\"grad(loss)(x) = {grad_loss(x)}\")  # Derivative of x^2 is 2x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLr9GIPhMT3B"
   },
   "source": [
    "- **[`jax.vmap`](https://jax.readthedocs.io/en/latest/jax-101/03-vectorization.html)**: Automatic vectorization to batch operations efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.26726124 0.5345225  0.8017837 ]\n",
      " [0.45584232 0.5698029  0.6837635 ]\n",
      " [0.50257075 0.57436657 0.6461624 ]]\n"
     ]
    }
   ],
   "source": [
    "# Function that operates on a single vector\n",
    "def normalize(x):\n",
    "    return x / jnp.linalg.norm(x)\n",
    "\n",
    "# Create a batch of vectors\n",
    "batch = jnp.array([[1.0, 2.0, 3.0],\n",
    "                   [4.0, 5.0, 6.0],\n",
    "                   [7.0, 8.0, 9.0]])\n",
    "\n",
    "# vmap automatically vectorizes over the batch dimension\n",
    "batch_normalize = jax.vmap(normalize)\n",
    "print(batch_normalize(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Finf-IGrMT3B"
   },
   "source": [
    "For a deeper introduction to JAX fundamentals, see the [JAX 101 tutorials](https://jax.readthedocs.io/en/latest/jax-101/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPyt7MV6prz1"
   },
   "source": [
    "## Define the miniGPT model with NNX and JAX automatic parallelism\n",
    "\n",
    "### NNX: A JAX-based neural network library\n",
    "\n",
    "[Flax NNX](https://flax.readthedocs.io/en/latest/nnx_basics.html) is the next-generation neural network library for JAX, designed to make building and training models more intuitive and Pythonic. NNX is part of the Flax ecosystem and represents a modernized approach to neural network development.\n",
    "\n",
    "**Why NNX?**\n",
    "\n",
    "- **Stateful and intuitive**: Unlike the original Flax (Linen), NNX uses a stateful, object-oriented API that feels more natural for Python developers\n",
    "- **Familiar syntax**: Define models using standard Python classes with `__init__` and `__call__` methods, similar to PyTorch\n",
    "- **Seamless JAX integration**: NNX works smoothly with JAX transformations like `jit`, `grad`, and `vmap`\n",
    "- **Built-in modules**: Provides common layers like `nnx.Linear`, `nnx.MultiHeadAttention`, and `nnx.LayerNorm` out of the box\n",
    "- **Flexible state management**: Easily separate and manage different types of state (parameters, batch statistics, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (4, 2)\n"
     ]
    }
   ],
   "source": [
    "# A simple two-layer MLP in Flax NNX\n",
    "class SimpleMLP(nnx.Module):\n",
    "    def __init__(self, in_features, hidden_size, out_features, rngs: nnx.Rngs):\n",
    "        self.linear1 = nnx.Linear(in_features, hidden_size, rngs=rngs)\n",
    "        self.linear2 = nnx.Linear(hidden_size, out_features, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = nnx.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "# Create model and run a forward pass\n",
    "simple_model = SimpleMLP(in_features=3, hidden_size=16, out_features=2, rngs=nnx.Rngs(0))\n",
    "x = jnp.ones((4, 3))  # batch of 4 samples, 3 features each\n",
    "print(f\"Output shape: {simple_model(x).shape}\")  # (4, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oB-CSHQGMT3B"
   },
   "source": [
    "In this tutorial, we'll use NNX to define our transformer architecture, leveraging its clean API to build the attention mechanisms, feed-forward networks, and embeddings that make up our miniGPT model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGBhVn4HMT3B"
   },
   "source": [
    "### Leveraging JAX's data and tensor parallelism\n",
    "\n",
    "One of the most powerful features of JAX is [device parallelism](https://docs.jax.dev/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html) for SPMD.\n",
    "\n",
    "- The data parallelism technique enables, for example, the training data to run via multiple parts (this is called sharding) - batches - in parallel and simultaneously across different devices, such as GPUs and Google TPUs. This allows to use larger batch sizes to speed up training.\n",
    "- Tensor parallelism allows us to split the model parameter tensors across several devices (sharding model tensors).\n",
    "- You can learn more about the basics of JAX parallelism in more detail in the [Introduction to parallel programming](https://jax.readthedocs.io/en/latest/sharded-computation.html) on the JAX documentation site.\n",
    "\n",
    "In this example, we'll utilize a 4-way data parallel and 2-way tensor parallel setup, which is aligned with Kaggle TPU v5e-8 or newer GCP TPUs chips.\n",
    "\n",
    "Note that as of October 2025, free-tier Colab only offers TPU v5e-1, which can no longer support SPMD.\n",
    "\n",
    "### jax.sharding.Mesh\n",
    "\n",
    "[`jax.sharding.Mesh`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.Mesh) is a multidimensional NumPy array of JAX devices, where each axis of the mesh has a name, such as `'x'` or `'y'`. This will help encapsulate the information about the TPU resource organization for distributing computations across the devices.\n",
    "\n",
    "Our `Mesh` will have two arguments:\n",
    "- `devices`: This will take the value of [`jax.make_mesh((4, 2), ('batch', 'model'))`](https://jax.readthedocs.io/en/latest/jax.experimental.mesh_utils.html), enabling us to build a device mesh. It is a NumPy ndarray with JAX devices (a list of devices from the JAX backend as obtained from [`jax.devices()`](https://jax.readthedocs.io/en/latest/_autosummary/jax.devices.html#jax.devices))..\n",
    "- `axis_names`, where:\n",
    "  - `batch`: 4 devices along the first axis - i.e. sharded into 4 - for data parallelism; and\n",
    "  - `model`: 2 devices along the second axis - i.e. sharded into 2 -  for tensor parallism\n",
    "\n",
    "This matches the structure in the Kaggle TPU v5e setup.\n",
    "\n",
    "Let's instantiate `Mesh` as `mesh` and declare the TPU configuration to define how data and model parameters are distributed across the devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created mesh with 1 devices: Mesh('batch': 1, 'model': 1, axis_types=(Auto, Auto))\n",
      "Mesh shape: OrderedDict({'batch': 1, 'model': 1}) (batch=1, model=1)\n"
     ]
    }
   ],
   "source": [
    "# Create a `Mesh` object representing TPU device arrangement.\n",
    "# The mesh defines how we distribute computation across devices.\n",
    "\n",
    "if jax.device_count() == 8:\n",
    "    # For Kaggle TPU v5e-8 or similar 8-core TPUs:\n",
    "    # Split 8 devices into a 4x2 grid for data and model parallelism\n",
    "    mesh = jax.make_mesh((4, 2), ('batch', 'model'))\n",
    "\n",
    "    # Alternative: Use 8-way data parallelism (no model parallelism)\n",
    "    # Uncomment the line below to experiment with this configuration:\n",
    "    # mesh = jax.make_mesh((8, 1), ('batch', 'model'))\n",
    "\n",
    "elif jax.device_count() == 1:\n",
    "    # For free-tier Colab TPU v5e-1 (single core)\n",
    "    # No parallelism is possible, but we still create a mesh for consistency\n",
    "    mesh = jax.make_mesh((1, 1), ('batch', 'model'))\n",
    "\n",
    "else:\n",
    "    # For other device counts, use all devices for data parallelism\n",
    "    mesh = jax.make_mesh((jax.device_count(), 1), ('batch', 'model'))\n",
    "\n",
    "print(f\"Created mesh with {jax.device_count()} devices: {mesh}\")\n",
    "print(f\"Mesh shape: {mesh.shape} (batch={mesh.shape['batch']}, model={mesh.shape['model']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZKdhNo98NgG"
   },
   "source": [
    "We will use the GPT-2 tokenizer from the [Tiktoken](https://github.com/openai/tiktoken) library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XHQ0BQ9-KIj"
   },
   "source": [
    "To leverage model parallelism, we need to instruct the JAX compiler how to shard the model tensors across the TPU devices. We'll use Flax NNX's [`flax.nnx.with_partitioning`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/spmd.html#flax.nnx.with_partitioning) to let each model layer know that the model weights or tensors need to be sharded according to our specification. We need to do this for every tensor/layer in the model.\n",
    "\n",
    "`nnx.with_partitioning` will take two arguments, such as the `initializer` (e.g. [`flax.nnx.initializers.xavier_uniform`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/initializers.html#flax.nnx.initializers.xavier_uniform) and [`flax.nnx.initializers.zeros_init`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/initializers.html#flax.nnx.initializers.zeros_init)), and a sharding tuple (e.g. `(None, 'model')` in our case) wrapped in [`jax.sharding.PartitionSpec`](https://docs.jax.dev/en/latest/jax.sharding.html#jax.sharding.PartitionSpec). The sharding tuple describe how to shard a tensor across, for example, the `model` axis or be replicated on other dimensions (which is denoted by `None`).\n",
    "\n",
    "For a more detailed discussion of Flax NNX sharding, please refer to [this SPMD guide](https://flax.readthedocs.io/en/latest/guides/flax_gspmd.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a triangular mask for causal attention with `jax.numpy.tril` and `jax.numpy.ones`.\n",
    "def causal_attention_mask(seq_len):\n",
    "    return jnp.tril(jnp.ones((seq_len, seq_len)))\n",
    "\n",
    "\n",
    "class TransformerBlock(nnx.Module):\n",
    "    \"\"\" A single Transformer block.\n",
    "\n",
    "    Each Transformer block processes input sequences via self-attention and feed-forward networks.\n",
    "\n",
    "    Args:\n",
    "        embed_dim (int): Embedding dimensionality.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        ff_dim (int): Dimensionality of the feed-forward network.\n",
    "        rngs (flax.nnx.Rngs): A Flax NNX stream of JAX PRNG keys.\n",
    "        rate (float): Dropout rate. Defaults to 0.1.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, *, rngs: nnx.Rngs, rate: float = 0.1):\n",
    "        # Sharding specs for model parallelism\n",
    "        kernel_sharding = P(None, 'model')\n",
    "        bias_sharding = P('model')\n",
    "\n",
    "        # Multi-Head Attention\n",
    "        self.mha = nnx.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            in_features=embed_dim,\n",
    "            kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), kernel_sharding),\n",
    "            bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), bias_sharding),\n",
    "            rngs=rngs\n",
    "        )\n",
    "\n",
    "        # First residual path\n",
    "        self.dropout1 = nnx.Dropout(rate=rate, rngs=rngs)\n",
    "        self.layer_norm1 = nnx.LayerNorm(\n",
    "            epsilon=1e-6, num_features=embed_dim,\n",
    "            scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), bias_sharding),\n",
    "            bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), bias_sharding),\n",
    "            rngs=rngs\n",
    "        )\n",
    "\n",
    "        # Feed-forward network\n",
    "        self.linear1 = nnx.Linear(\n",
    "            in_features=embed_dim, out_features=ff_dim,\n",
    "            kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), kernel_sharding),\n",
    "            bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), bias_sharding),\n",
    "            rngs=rngs\n",
    "        )\n",
    "        self.linear2 = nnx.Linear(\n",
    "            in_features=ff_dim, out_features=embed_dim,\n",
    "            kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), kernel_sharding),\n",
    "            bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), bias_sharding),\n",
    "            rngs=rngs\n",
    "        )\n",
    "\n",
    "        # Second residual path\n",
    "        self.dropout2 = nnx.Dropout(rate=rate, rngs=rngs)\n",
    "        self.layer_norm2 = nnx.LayerNorm(\n",
    "            epsilon=1e-6, num_features=embed_dim,\n",
    "            scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), bias_sharding),\n",
    "            bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), bias_sharding),\n",
    "            rngs=rngs\n",
    "        )\n",
    "\n",
    "    # Apply the Transformer block to the input sequence.\n",
    "    def __call__(self, inputs, training: bool = False):\n",
    "        input_shape = inputs.shape\n",
    "        _, seq_len, _ = input_shape\n",
    "\n",
    "        # Instantiate the causal attention mask.\n",
    "        mask = causal_attention_mask(seq_len)\n",
    "\n",
    "        # Apply Multi-Head Attention with the causal attention mask.\n",
    "        attention_output = self.mha(\n",
    "            inputs_q=inputs,\n",
    "            mask=mask,\n",
    "            decode=False\n",
    "        )\n",
    "        # Apply the first dropout.\n",
    "        attention_output = self.dropout1(attention_output, deterministic=not training)\n",
    "        # Apply the first layer normalization.\n",
    "        out1 = self.layer_norm1(inputs + attention_output)\n",
    "\n",
    "        # The feed-forward network.\n",
    "        # Apply the first linear transformation.\n",
    "        ffn_output = self.linear1(out1)\n",
    "        # Apply the ReLU activation with `flax.nnx.relu`.\n",
    "        ffn_output = nnx.relu(ffn_output)\n",
    "        # Apply the second linear transformation.\n",
    "        ffn_output = self.linear2(ffn_output)\n",
    "        # Apply the second dropout.\n",
    "        ffn_output = self.dropout2(ffn_output, deterministic=not training)\n",
    "        # Apply the second layer normalization and return the output of the Transformer block.\n",
    "        return self.layer_norm2(out1 + ffn_output)\n",
    "\n",
    "\n",
    "class TokenAndPositionEmbedding(nnx.Module):\n",
    "    \"\"\" Combines token embeddings (words in an input sentence) with\n",
    "    positional embeddings (the position of each word in a sentence).\n",
    "\n",
    "    Args:\n",
    "        maxlen (int): Matimum sequence length.\n",
    "        vocal_size (int): Vocabulary size.\n",
    "        embed_dim (int): Embedding dimensionality.\n",
    "        rngs (flax.nnx.Rngs): A Flax NNX stream of JAX PRNG keys.\n",
    "    \"\"\"\n",
    "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, *, rngs: nnx.Rngs):\n",
    "        # Initialize token embeddings (using `flax.nnx.Embed`).\n",
    "        # Each unique word has an embedding vector.\n",
    "        self.token_emb = nnx.Embed(num_embeddings=vocab_size, features=embed_dim, rngs=rngs)\n",
    "        # Initialize positional embeddings (using `flax.nnx.Embed`).\n",
    "        self.pos_emb = nnx.Embed(num_embeddings=maxlen, features=embed_dim, rngs=rngs)\n",
    "\n",
    "    # Takes a token sequence (integers) and returns the combined token and positional embeddings.\n",
    "    def __call__(self, x):\n",
    "        # Generate a sequence of positions for the input tokens.\n",
    "        positions = jnp.arange(0, x.shape[1])[None, :]\n",
    "        # Look up the positional embeddings for each position in the input sequence.\n",
    "        position_embedding = self.pos_emb(positions)\n",
    "        # Look up the token embeddings for each token in the input sequence.\n",
    "        token_embedding = self.token_emb(x)\n",
    "        # Combine token and positional embeddings.\n",
    "        return token_embedding + position_embedding\n",
    "\n",
    "\n",
    "class MiniGPT(nnx.Module):\n",
    "    \"\"\" A miniGPT transformer model, inherits from `flax.nnx.Module`.\n",
    "\n",
    "    Args:\n",
    "        maxlen (int): Maximum sequence length.\n",
    "        vocab_size (int): Vocabulary size.\n",
    "        embed_dim (int): Embedding dimensionality.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        feed_forward_dim (int): Dimensionality of the feed-forward network.\n",
    "        num_transformer_blocks (int): Number of transformer blocks. Each block contains attention and feed-forward networks.\n",
    "        rngs (nnx.Rngs): A Flax NNX stream of JAX PRNG keys.\n",
    "    \"\"\"\n",
    "    # Initialize miniGPT model components.\n",
    "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, num_heads: int,\n",
    "                 feed_forward_dim: int, num_transformer_blocks: int, rngs: nnx.Rngs):\n",
    "        # Initiliaze the `TokenAndPositionEmbedding` that combines token and positional embeddings.\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(\n",
    "                    maxlen, vocab_size, embed_dim, rngs=rngs\n",
    "                )\n",
    "        # Create a Sequential container of `TransformerBlock` instances.\n",
    "        # Each block processes input sequences using attention and feed-forward networks.\n",
    "        # Use nnx.Sequential instead of a plain list for proper NNX 0.12.0+ compatibility\n",
    "        self.transformer_blocks = nnx.Sequential(*[TransformerBlock(\n",
    "            embed_dim, num_heads, feed_forward_dim, rngs=rngs\n",
    "        ) for _ in range(num_transformer_blocks)])\n",
    "        # Initialize the output `flax.nnx.Linear` layer producing logits over the vocabulary for next-token prediction.\n",
    "        self.output_layer = nnx.Linear(\n",
    "            in_features=embed_dim, out_features=vocab_size,\n",
    "            kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), P(None, 'model')),\n",
    "            bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), P('model')),\n",
    "            rngs=rngs\n",
    "        )\n",
    "\n",
    "    def __call__(self, inputs, training: bool = False):\n",
    "        # Pass the input tokens through the `embedding_layer` to get token embeddings.\n",
    "        # Apply each transformer block sequentially to the embedded input, use the `training` flag for the behavior of `flax.nnx.Dropout`.\n",
    "        x = self.embedding_layer(inputs)\n",
    "        # nnx.Sequential automatically applies each block in sequence\n",
    "        # We need to pass the training flag to each block manually\n",
    "        for block in self.transformer_blocks.layers:\n",
    "            x = block(x, training=training)\n",
    "        # Pass the output of the transformer blocks through the output layer,\n",
    "        # and obtain logits for each token in the vocabulary (for next token prediction).\n",
    "        outputs = self.output_layer(x)\n",
    "        return outputs\n",
    "\n",
    "    def get_model_input(self):\n",
    "        return dict(inputs=jnp.zeros((batch_size, maxlen), dtype=jnp.int32), training=False)\n",
    "\n",
    "    @nnx.jit\n",
    "    def sample_from(self, rng_key, logits):\n",
    "        logits, indices = jax.lax.top_k(logits, k=top_k)\n",
    "        return jax.random.choice(rng_key, indices, p=nnx.softmax(logits))\n",
    "\n",
    "    @nnx.jit\n",
    "    def generate_step(self, rng_key, padded_tokens, sample_index):\n",
    "        logits = self(padded_tokens)\n",
    "        return self.sample_from(rng_key, logits[0][sample_index])\n",
    "\n",
    "    def generate_text(self, max_tokens, start_tokens):\n",
    "        generated = []\n",
    "        rng_key = jax.random.PRNGKey(0)\n",
    "        print(tokenizer.decode(start_tokens), flush=True, end='')\n",
    "\n",
    "        for i in range(max_tokens):\n",
    "            sample_index = len(start_tokens) + len(generated) - 1\n",
    "            rng_key, step_key = jax.random.split(rng_key)\n",
    "\n",
    "            current_seq = start_tokens + generated\n",
    "            padded = jnp.array(current_seq + [0] * (maxlen - len(current_seq)))[None, :]\n",
    "            next_token = int(self.generate_step(step_key, padded, sample_index))\n",
    "\n",
    "            end_token = tokenizer.encode('<|endoftext|>', allowed_special={'<|endoftext|>'})[0]\n",
    "            if next_token == end_token:\n",
    "                break\n",
    "\n",
    "            generated.append(next_token)\n",
    "            print(tokenizer.decode([next_token]), flush=True, end='')\n",
    "\n",
    "        return tokenizer.decode(start_tokens + generated)\n",
    "\n",
    "\n",
    "# Creates the miniGPT model with 4 transformer blocks.\n",
    "def create_model(rngs):\n",
    "    return MiniGPT(maxlen, vocab_size, embed_dim, num_heads, feed_forward_dim, num_transformer_blocks=4, rngs=rngs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igX_eoGNMTGR"
   },
   "source": [
    "Set some hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture hyperparameters\n",
    "vocab_size = tokenizer.n_vocab              # Size of GPT-2 vocabulary\n",
    "num_transformer_blocks = 4                  # Number of transformer layers\n",
    "maxlen = 128                                # Maximum sequence length\n",
    "embed_dim = 128                             # Embedding dimension\n",
    "num_heads = 4                               # Number of attention heads\n",
    "feed_forward_dim = 256                      # Hidden dimension in feed-forward network\n",
    "\n",
    "# Training hyperparameters\n",
    "batch_size = 32                             # Batch size\n",
    "num_epochs = 1                              # Number of passes through the dataset\n",
    "top_k = 10                                  # Top-k sampling for text generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mI1ci-HyMspJ"
   },
   "source": [
    "## Grain: Load and preprocess the data\n",
    "\n",
    "[Grain](https://google-grain.readthedocs.io/en/latest/) is Google's high-performance data loading library designed specifically for machine learning workloads in JAX. It provides efficient data pipelines that can keep up with the demanding throughput requirements of modern accelerators like TPUs and GPUs.\n",
    "\n",
    "**Why Grain for data loading?**\n",
    "\n",
    "- **Performance optimized**: Grain is designed from the ground up for high-throughput data loading, minimizing bottlenecks between data processing and model training\n",
    "- **Deterministic and reproducible**: Grain ensures deterministic iteration order even with shuffling and multiple workers, which is crucial for reproducible experiments\n",
    "- **JAX integration**: Seamlessly works with JAX's device parallelism, automatically handling data sharding across multiple devices\n",
    "- **Flexible transformations**: Provides composable operations for batching, shuffling, and preprocessing data\n",
    "- **Multi-epoch support**: Built-in support for iterating over datasets multiple times without manual reinitialization\n",
    "\n",
    "**Key Grain concepts:**\n",
    "\n",
    "In this tutorial, we use three main Grain components:\n",
    "\n",
    "1. **Data source**: A custom `TextDataset` class that tokenizes and pads text sequences to a fixed length\n",
    "2. **Sampler**: An `IndexSampler` that determines the order in which examples are accessed from the data source\n",
    "3. **DataLoader**: Combines the data source, sampler, and operations (like batching) into an efficient pipeline\n",
    "\n",
    "Grain handles the complexity of data loading, allowing us to focus on model development while ensuring our TPUs stay fully utilized during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TextDataset:\n",
    "    \"\"\"A simple dataset for tokenized text sequences.\"\"\"\n",
    "    data: list\n",
    "    maxlen: int\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        # Tokenize and truncate\n",
    "        encoding = tokenizer.encode(self.data[idx], allowed_special={'<|endoftext|>'})[:self.maxlen]\n",
    "        # Pad to maxlen\n",
    "        return encoding + [0] * (self.maxlen - len(encoding))\n",
    "\n",
    "\n",
    "def load_and_preprocess_data(file_path, batch_size, maxlen):\n",
    "    \"\"\"Load the TinyStories dataset and create a Grain DataLoader.\"\"\"\n",
    "\n",
    "    # Read the file\n",
    "    with open(file_path, 'r') as f:\n",
    "        text = f.read(1 << 29)\n",
    "\n",
    "    # Split into stories\n",
    "    stories = text.split('<|endoftext|>')\n",
    "    stories = [story + '<|endoftext|>' for story in stories if story.strip()]\n",
    "    print(f\"Loaded {len(stories):,} stories\")\n",
    "\n",
    "    dataset = TextDataset(stories, maxlen)\n",
    "\n",
    "    sampler = pygrain.IndexSampler(\n",
    "        len(dataset),\n",
    "        shuffle=False,\n",
    "        seed=42,\n",
    "        shard_options=pygrain.NoSharding(),\n",
    "        num_epochs=num_epochs,\n",
    "    )\n",
    "\n",
    "    dl = pygrain.DataLoader(\n",
    "        data_source=dataset,\n",
    "        sampler=sampler,\n",
    "        operations=[pygrain.Batch(batch_size=batch_size, drop_remainder=True)],\n",
    "    )\n",
    "\n",
    "    return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title [hidden cell; used for testing]\n",
    "# This cell is run only in the JAX AI Stack's CI testing and should otherwise be ignored.\n",
    "import os\n",
    "AI_STACK_TEST_MODE = os.getenv('AI_STACK_TEST_MODE') == 'true'\n",
    "\n",
    "if AI_STACK_TEST_MODE:\n",
    "    num_transformer_blocks = 2\n",
    "    maxlen = 16\n",
    "    embed_dim = 16\n",
    "    num_heads = 2\n",
    "    feed_forward_dim = 8\n",
    "\n",
    "    def load_and_preprocess_data(file_path, batch_size, maxlen):\n",
    "        del file_path\n",
    "\n",
    "        @dataclass\n",
    "        class TestTextDataset:\n",
    "            maxlen: int\n",
    "\n",
    "            def __len__(self):\n",
    "                return 64\n",
    "\n",
    "            def __getitem__(self, idx: int):\n",
    "                encoding = jax.random.randint(jax.random.key(idx), [self.maxlen], minval=0, maxval=1e6)\n",
    "                return jnp.unstack(encoding)\n",
    "\n",
    "        dataset = TestTextDataset(maxlen)\n",
    "\n",
    "        sampler = pygrain.IndexSampler(\n",
    "            len(dataset),\n",
    "            shuffle=False,\n",
    "            seed=42,\n",
    "            shard_options=pygrain.NoSharding(),\n",
    "            num_epochs=num_epochs,\n",
    "        )\n",
    "\n",
    "        dl = pygrain.DataLoader(\n",
    "            data_source=dataset,\n",
    "            sampler=sampler,\n",
    "            operations=[pygrain.Batch(batch_size=batch_size, drop_remainder=True)],\n",
    "        )\n",
    "\n",
    "        return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TinyStories dataset with batch_size=32, maxlen=128...\n",
      "Loaded 589,692 stories\n",
      "Dataset loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create the data loader\n",
    "# This will load, tokenize, and batch the TinyStories dataset\n",
    "print(f\"Loading TinyStories dataset with batch_size={int(batch_size)}, maxlen={maxlen}...\")\n",
    "text_dl = load_and_preprocess_data('TinyStories-train.txt', int(batch_size), maxlen)\n",
    "print(\"Dataset loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKVSD8KSM1um"
   },
   "source": [
    "## Defining the loss function and training step function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, batch):\n",
    "    \"\"\"Compute the cross-entropy loss for language modeling.\n",
    "\n",
    "    Args:\n",
    "        model: The MiniGPT model\n",
    "        batch: A tuple of (input_tokens, target_tokens)\n",
    "\n",
    "    Returns:\n",
    "        A tuple of (loss, logits) where:\n",
    "        - loss: Scalar cross-entropy loss\n",
    "        - logits: Model predictions (returned as auxiliary output for metrics)\n",
    "    \"\"\"\n",
    "    input_tokens, target_tokens = batch\n",
    "\n",
    "    # Forward pass: get logits for each position\n",
    "    logits = model(input_tokens)\n",
    "\n",
    "    # Compute cross-entropy loss between predictions and targets\n",
    "    # This measures how well the model predicts the next token\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "        logits=logits,\n",
    "        labels=target_tokens\n",
    "    ).mean()\n",
    "\n",
    "    return loss, logits\n",
    "\n",
    "\n",
    "@nnx.jit  # JIT-compile for performance (runs once, then cached)\n",
    "def train_step(model: MiniGPT, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
    "    \"\"\"Perform a single training step.\n",
    "\n",
    "    This function:\n",
    "    1. Computes the loss and gradients\n",
    "    2. Updates model parameters using the optimizer\n",
    "    3. Updates metrics for tracking\n",
    "\n",
    "    Args:\n",
    "        model: The MiniGPT model to train\n",
    "        optimizer: The Optax optimizer (wrapped in nnx.Optimizer)\n",
    "        metrics: Metrics tracker for monitoring training\n",
    "        batch: A tuple of (input_tokens, target_tokens)\n",
    "    \"\"\"\n",
    "    # Create a function that computes both loss value and gradients\n",
    "    # has_aux=True means loss_fn returns (loss, auxiliary_data)\n",
    "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
    "\n",
    "    # Compute loss, logits, and gradients in one go\n",
    "    (loss, logits), grads = grad_fn(model, batch)\n",
    "\n",
    "    # Update metrics for tracking\n",
    "    metrics.update(loss=loss, logits=logits, labels=batch[1])\n",
    "\n",
    "    # Update model parameters using the computed gradients\n",
    "    # The optimizer applies the Adam update rule\n",
    "    optimizer.update(model, grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5um2vkeUNckm"
   },
   "source": [
    "## Optax: Train the model\n",
    "\n",
    "[Optax](https://optax.readthedocs.io) is a gradient processing and optimization library for JAX. It provides composable components for building custom optimizers and a collection of popular optimization algorithms used in deep learning.\n",
    "\n",
    "**Why Optax?**\n",
    "\n",
    "- **Modular design**: Optax uses a functional, composable approach where optimizers are built by chaining together simple transformations\n",
    "- **Rich optimizer collection**: Includes popular optimizers like Adam, SGD, AdamW, and many more advanced variants\n",
    "- **Gradient transformations**: Provides utilities for gradient clipping, normalization, and other preprocessing steps\n",
    "- **JAX compatibility**: Designed specifically for JAX, working seamlessly with `jax.grad()` and other transformations\n",
    "- **Stateless and functional**: Optimizer state is explicitly managed, making it easy to understand and debug\n",
    "\n",
    "**Key Optax concepts:**\n",
    "\n",
    "In this tutorial, we use:\n",
    "\n",
    "1. **Loss function**: We define a `loss_fn` that computes cross-entropy loss using `optax.softmax_cross_entropy_with_integer_labels`\n",
    "2. **Optimizer**: We use `optax.adam(1e-3)`, the Adam optimizer with a learning rate of 0.001\n",
    "3. **NNX Optimizer wrapper**: `nnx.Optimizer` integrates Optax optimizers with Flax NNX models, handling parameter updates automatically\n",
    "\n",
    "The training loop combines these components: we compute gradients with `nnx.value_and_grad`, then use the optimizer to update model parameters based on those gradients.\n",
    "\n",
    "**Training details:**\n",
    "\n",
    "Start training. It takes approximately 20 minutes on Colab TPU v5e-1.\n",
    "\n",
    "Note that for data parallel, we are sharding the training data along the `batch` axis using `jax.device_put`.\n",
    "\n",
    "We are also using the `jax.vmap` transformation to produce the target sequences faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial generated text (before training):\n",
      "Once upon a time Wired Donkey genometionsRing Wired surprises45beyaf partner Mazda inquired resonate oversized killsbeybeybey Process Kimmelbey Lance urgently choosesardkbRing circaincluding Kimmel waved45odondiff156 litres encrypt arrests inquired Detect preschooloi Detect316 Silenceallas Pyrrha Wiredallah McCitemsallah wh ExperbeyChristmas urgentlyallah floatstions ridiculouslyvag litres tradingincludingbey45 nontoted lovedbeyitems lied Silence bald nause Topic postp acknowledgment atrocensor brut dog strictbey Ingramallahentimes Process leaps arbitrationinen loved arrestsantage decisivekb floatsparticularlybey nont strict atroc atroc lied Silence Toadvag Kimmeltions arrests gh sparks Converted156 wors Albion spying complaints SwordsDoctor Millennials encrypt Shar!!!\n",
      "\n",
      "\n",
      "Step 1000, Loss: 4.0584, Elapsed Time: 37.39s\n",
      "Generated text:\n",
      "Once upon a time.\n",
      " he decided, so he wanted to explore a special place that he wanted to go. He would play all day. But, so he could his friends to play in the woods.\n",
      "The other animals were excited to explore and his friends. But he was a little girl came to the forest. It was so he wanted to play.  He said yes and he started to eat him. Then he had never seen a special place. \n",
      "So the end of the birds and his mom said, \"You can't help me!\"\n",
      "So that they were so proud. The animals had a new idea to!!!!\n",
      "\n",
      "\n",
      "Step 2000, Loss: 2.8782, Elapsed Time: 21.17s\n",
      "Generated text:\n",
      "Once upon a time and his mom wanted to play a game. So the boy went on a sunny day and he went to the park. Suddenly he spotted an old toy. He had an idea and decided to go to the store. The little boy asked the boy, \"Why don't you help? I'm looking for help.\"\n",
      "The boy said, \"It will be careful and wait for me and have a picnic.\"\n",
      "The boy was very excited. He said, \"Thank you, let's take your toy!\"\n",
      "The boy said his mom, \"Okay you like it.\" He ran and took a big bite the sandwich!!!!\n",
      "\n",
      "\n",
      "Step 3000, Loss: 2.5746, Elapsed Time: 21.81s\n",
      "Generated text:\n",
      "Once upon a time. One day the store to play in the park, the park with a lot. The park was so big and could jump up to catch. \n",
      "The big and the wind started to move. The sun was warm and the birds were happy. \n",
      "The little girl and Lily were happy to see the sun. They were so happy. They hugged each other and said, \"Mom, I can't play with you.\"\n",
      "Lily and her friends played in the park, but then the rain started to rain. They felt much better and happy and the rain. Lily saw that her friend was happy that their!!!!\n",
      "\n",
      "\n",
      "Step 4000, Loss: 2.4114, Elapsed Time: 21.68s\n",
      "Generated text:\n",
      "Once upon a time on a small dog. The dog loved to run. The dog always liked to play with the dog, the dog and the other. \n",
      "One day, they decided to play with the dog. The dog was scared of the dog, but the dog did not know how to chase the dog. The dog and the dog became friends. \n",
      "A little girl came to the dog, said, \"Hi! Do you want to play with me?\" \n",
      "The dog barked and the dog barked happily. The dog jumped out of the house with a smile. They went to the park and found the boy!!!!\n",
      "\n",
      "\n",
      "Step 5000, Loss: 2.2789, Elapsed Time: 21.16s\n",
      "Generated text:\n",
      "Once upon a time. The people liked to play together, and other things was always. One day, the other day was playing in the forest near her garden. Suddenly, the stars flew and over to the trees. The sun was bright and warm and beautiful.\n",
      "The stars started to grow. All of the birds sang, and the stars sang. The sun clap and watched as the stars began the clouds began. The sky was so happy and the stars started to sing. The sun shone so brightly. The stars danced and the sun sang as happy.\n",
      "The people clapped and enjoyed having fun! The sun had made from!!!!\n",
      "\n",
      "\n",
      "Step 6000, Loss: 2.2009, Elapsed Time: 21.62s\n",
      "Generated text:\n",
      "Once upon a time. Mom had made the car so big and was ready for the fun. When it was time to go to the airport. He got a big umbrella. He got on the plane. He was so excited! He knew it would be a good place.\n",
      "The car drove away! He drove and drove in the car. He drove around the airport to the park. He was so surprised, he had never been before.\n",
      "The airport was so excited to go outside. He had been looking around and he couldn't see anything to be able to play and have fun. \n",
      "But he didn't notice. This!!!!\n",
      "\n",
      "\n",
      "Step 7000, Loss: 2.1387, Elapsed Time: 21.81s\n",
      "Generated text:\n",
      "Once upon a time a boy and Jack are very excited for a picnic to celebrate. But Jack always have a picnic and Jack was feeling hungry and went out into the cooler.\n",
      "On their picnic, Jack saw a big tree with lots of yummy fruits. He asked if he could go, but his mom said yes.\n",
      "Jack and Jack went to the tree to the tree with a big smile on.\n",
      "When Jack got to the tree, he saw a big tree and started to climb up it. The tree was so big and tall and had lots of leaves! But the tree was not feeling bad because it was just too late!!!!\n",
      "\n",
      "\n",
      "Step 8000, Loss: 2.1233, Elapsed Time: 21.17s\n",
      "Generated text:\n",
      "Once upon a time there were two rabbits and birds. The birds had big feathers and wings were buzzing around the trees.\n",
      "One day a group of squirrel flew by the trees and landed in a nearby field. The squirrel was so excited.\n",
      "The two friends decided to climb the tree. They climbed on a branch, and flew high in the sky. They had a very long long, fluffy cloud and green trees. \n",
      "The rabbits were so excited! They jumped up and laughed. They laughed, clapping their wings and flew around the tree as the birds flew away with their feathers.\n",
      "Finally, they found a beautiful lake in!!!!\n",
      "\n",
      "\n",
      "Step 9000, Loss: 2.1514, Elapsed Time: 21.93s\n",
      "Generated text:\n",
      "Once upon a time a big lion wanted to help people in a very intelligent, but his friends knew that he had to help him. Every day the day the lion and the other started to build a fort. \n",
      "One day, they found a small, shiny brick in the garden. The brick wanted to build it, but it was too small to build a house and it would make the house. Then the lion heard a loud noise and he got to work. \n",
      "He was so excited he could hardly believe his eyes. He quickly grabbed out the brick and put down on his new hammer! The little bird was so pleased he!!!!\n",
      "\n",
      "\n",
      "Step 10000, Loss: 2.0750, Elapsed Time: 21.82s\n",
      "Generated text:\n",
      "Once upon a time a tall bald prince named Tim was a little bird who lived in his nest with his family. One day, Timmy went outside and noticed that his nest was a beautiful birdcage. He asked his mom if she could go outside and play.\n",
      "Timmy said, \"No, I can't get it. I don't want to leave because it won't be alone.\" So, he got to the birdcage and started to sing. But it didn't work for Timmy.\n",
      "Suddenly, Timmy saw an old bird flying away with its nest. Timmy said, \"I'm scared but!!!!\n",
      "\n",
      "\n",
      "Step 11000, Loss: 2.1084, Elapsed Time: 21.21s\n",
      "Generated text:\n",
      "Once upon a time there were two bears and Jack. They were walking around the forest looking for food. Suddenly, Jack saw a big oak tree! It was so tall and tall inside. Jack and his friends were so excited and they quickly ran inside. They saw a squirrel and he was very hungry. Jack said to the squirrel, \"Let's go and see it together!\" He looked up and said yes, but Jack was very careful.\n",
      "Suddenly, Jack's friend, Sam, said, \"Let's play a game. We should play together.\" Jack said, \"No, Sam! They are so stupid!\" They started playing!!!!\n",
      "\n",
      "\n",
      "Step 12000, Loss: 2.0614, Elapsed Time: 21.92s\n",
      "Generated text:\n",
      "Once upon a time in the land to three year old. It was full of energy! The 3 year old was very special because it could be. Every morning, the little girl was happy and she went to the park. She saw many other kids playing together. They played tag and have fun. After a while, the little girl's mommy said, \"We made a beautiful new home. You will always be happy.\"\n",
      "The little girl said, \"Thank you for helping me, Mommy. You can play with me.\" But then one day, the little girl asked her mommy said, \"Mommy, what was!!!!\n",
      "\n",
      "\n",
      "Step 13000, Loss: 2.0741, Elapsed Time: 21.79s\n",
      "Generated text:\n",
      "Once upon a time in, a family wanted to play with the kids. The kids saw the children and wanted to play too. They played all day long, but the kids were having a great time.\n",
      "The family went to the beach and the family went to the beach. They all played in the sand and the kids loved to play. They had so much fun on the shore and had so much fun.\n",
      "But then, the kids saw something disgusting. They all started to quarrel. The kids were so scared that they had to go away, so they decided to run away. They both wanted to go back to the sea but!!!!\n",
      "\n",
      "\n",
      "Step 14000, Loss: 2.0532, Elapsed Time: 21.27s\n",
      "Generated text:\n",
      "Once upon a time in his backyard in a small village. In this middle of the world, he was a very happy place to explore and discover what wonderful he could be.\n",
      "But then something unexpected happened. A big, scary monster started to get very scared. He looked up and saw that it wasn't scary, but he couldn't find it.\n",
      "He quickly decided to find a way to get down the village to find his way home. As he was walking, he noticed something strange. The monster stopped and looked closer to see what it looked like.\n",
      "He stopped to look closer. A big man had a friendly look on!!!!\n",
      "\n",
      "\n",
      "Step 15000, Loss: 2.0032, Elapsed Time: 22.01s\n",
      "Generated text:\n",
      "Once upon a time there lived a kind old man. He lived in a small house on a tall tree. Every day he would go on an ordinary holiday in the woods.\n",
      "At the park, the old man saw a little girl in the park. The girl was so excited to have the perfect time to go home. The old man was very happy to be back by the city. He was very glad he had the little girl's home.\n",
      "The old man was happy to see the little girl and he had a wonderful day. He thanked Anna and said he would love to come back. Together, he and the girl went into!!!!\n",
      "\n",
      "\n",
      "Step 16000, Loss: 2.0181, Elapsed Time: 21.65s\n",
      "Generated text:\n",
      "Once upon a time a girl called Amy wanted to bake for her first time. She put on the biggest cake in the store. She loved it! When it was time to bake all day, Amy put on her favorite cake. She went to the bakery and bought lots of delicious cake. She was very happy and started to mix the cake.\n",
      "When Amy got to baking, the cake was so big to everyone. It was very impressive! She loved cake! She was so excited that she couldn't wait to get to bake cake. When it looked like cake, she wanted it. She saw how big the cake was in the garden!!!!\n",
      "\n",
      "\n",
      "Step 17000, Loss: 1.9619, Elapsed Time: 21.15s\n",
      "Generated text:\n",
      "Once upon a time there lived two cats who liked to run. One of the cats was a fun and they were running to the other cars. Suddenly they noticed a big red car coming up ahead. They were stuck in the car and got stuck. They started to pull the car down and the car fell down. They were very scared. The cats were stuck in the gas car. The Cat and the dog helped the two cars back. They worked together to get the car. They fixed the car, and the owner was very happy. They were free. The end.Once\n",
      "\n",
      "\n",
      "Step 18000, Loss: 1.9657, Elapsed Time: 21.75s\n",
      "Generated text:\n",
      "Once upon a time there lived an ugly boy named Jack. He had no friends to do.\n",
      "One day, Jack saw a big bird flying on a birdcage. He tried to fly around and see how high he could fly and fly high in the sky. Jack tried to fly up, but he couldn't reach it.\n",
      "Suddenly, Jack saw a bird swooped down from the birdcage. The bird flew away and Jack was very sad. Jack wanted to help the bird, but he didn't know how. He had an idea. He flew over his house and found the birdcage.\n",
      "The bird felt!!!!\n",
      "\n",
      "\n",
      "Final generated text (after training):\n",
      "Once upon a time there was a very persistent little girl named Lily. She loved to sing and sing all day long. One day, her mom said she couldn't sing because it got very dirty. \n",
      "Lily was sad because her mom told her it was important to go to the park. \n",
      "But then, Lily realized that she couldn't sing a beautiful song. It was her favorite color was too much better than the little girl. \n",
      "Lily realized that it was important to listen to her mom and sing songs. She sang every day and sang songs every day. \n",
      "But one day, Lily went back into!!!!"
     ]
    }
   ],
   "source": [
    "# Initialize the model and optimizer within the mesh context\n",
    "# This ensures model parameters are properly sharded according to our partition specs\n",
    "with jax.set_mesh(mesh):\n",
    "    model = create_model(rngs=nnx.Rngs(0))\n",
    "    # Create Adam optimizer with learning rate 1e-3\n",
    "    # wrt=nnx.Param means we only optimize parameters (not other state like batch stats)\n",
    "    optimizer = nnx.Optimizer(model, optax.adam(1e-3), wrt=nnx.Param)\n",
    "\n",
    "# Initialize metrics tracking\n",
    "metrics = nnx.MultiMetric(\n",
    "    loss=nnx.metrics.Average(\"loss\"),\n",
    ")\n",
    "\n",
    "# Initialize random number generator (not used in current training but good practice)\n",
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "# Test the model before training by generating some text\n",
    "start_prompt = \"Once upon a time\"\n",
    "start_tokens = tokenizer.encode(start_prompt)[:maxlen]\n",
    "print(\"Initial generated text (before training):\")\n",
    "generated_text = model.generate_text(maxlen, start_tokens)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Dictionary to store training metrics over time\n",
    "metrics_history = {\n",
    "    \"train_loss\": [],\n",
    "}\n",
    "\n",
    "# Helper function to create target sequences from input sequences\n",
    "# In language modeling, the target is the input shifted by one position\n",
    "# Uses jax.vmap to vectorize the operation across the batch dimension efficiently\n",
    "prep_target_batch = jax.vmap(\n",
    "    lambda tokens: jnp.concatenate((tokens[1:], jnp.array([0])))\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    for batch in text_dl:\n",
    "        # Skip batches that don't divide evenly across devices\n",
    "        # This ensures each device gets the same amount of data\n",
    "        if len(batch) % len(jax.devices()) != 0:\n",
    "            continue\n",
    "\n",
    "        # Prepare input and target batches\n",
    "        # Transpose to get shape (batch_size, seq_len)\n",
    "        input_batch = jnp.array(jnp.array(batch).T)\n",
    "\n",
    "        # Create targets by shifting inputs left by one position\n",
    "        # E.g., if input is [1, 2, 3, 4], target is [2, 3, 4, 0]\n",
    "        target_batch = prep_target_batch(input_batch)\n",
    "\n",
    "        # Shard the data across devices for data parallelism\n",
    "        # NamedSharding with P(\"batch\", None) means:\n",
    "        # - Shard along the batch dimension (first axis)\n",
    "        # - Replicate along the sequence dimension (second axis)\n",
    "        # jax.device_put moves the data to the devices with the specified sharding\n",
    "        sharded_batch = jax.device_put(\n",
    "            (input_batch, target_batch),\n",
    "            NamedSharding(mesh, P(\"batch\", None))\n",
    "        )\n",
    "\n",
    "        # Perform one training step\n",
    "        # JAX automatically handles the distributed computation!\n",
    "        train_step(model, optimizer, metrics, sharded_batch)\n",
    "\n",
    "        # Log metrics every 1000 steps\n",
    "        if (step + 1) % 1000 == 0:\n",
    "            # Compute and store current metrics\n",
    "            for metric, value in metrics.compute().items():\n",
    "                metrics_history[f\"train_{metric}\"].append(value)\n",
    "            metrics.reset()\n",
    "\n",
    "            # Print progress\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"\\nStep {step + 1}, Loss: {metrics_history['train_loss'][-1]:.4f}, \"\n",
    "                  f\"Elapsed Time: {elapsed_time:.2f}s\")\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Generate sample text to see progress\n",
    "            print(\"Generated text:\")\n",
    "            generated_text = model.generate_text(maxlen, start_tokens)\n",
    "            print(\"\\n\")\n",
    "\n",
    "        step += 1\n",
    "\n",
    "# Final text generation to see the fully trained model\n",
    "print(\"\\nFinal generated text (after training):\")\n",
    "generated_text = model.generate_text(maxlen, start_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thaLs6TD0lt5"
   },
   "source": [
    "Visualize the training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(metrics_history['train_loss'])\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WB-ExEt1Zl1C"
   },
   "source": [
    "As you can see, the model goes from generating completely random words at the beginning to generating sensible tiny stories at the end of the training. So essentially we have pretrained a small LLM to write tiny stories for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDCVkgMTLC5M"
   },
   "source": [
    "## Debugging the model\n",
    "\n",
    "When working with JAX and JIT-compiled functions, debugging requires different techniques than traditional Python debugging. Since JIT compilation traces your function and compiles it to XLA, standard `print()` statements execute at trace time and won't work inside compiled functions.\n",
    "\n",
    "**JAX debugging tools:**\n",
    "\n",
    "- **`jax.debug.print()`**: Print intermediate values from inside JIT-compiled functions\n",
    "- **`jax.debug_nans`**: Automatically detect and raise errors when NaN values appear in computations\n",
    "\n",
    "For more debugging techniques, see the [JAX debugging documentation](https://jax.readthedocs.io/en/latest/debugging/index.html).\n",
    "\n",
    "Let's explore these debugging techniques with our miniGPT model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhgaK7w0W3yn"
   },
   "source": [
    "### Using `jax.debug.print()` inside JIT-compiled functions\n",
    "\n",
    "The `jax.debug.print()` function lets you inspect intermediate values during execution. Unlike regular `print()`, it works inside `@jax.jit` decorated functions. This is invaluable for understanding what's happening inside your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (1, 128)\n",
      "After embedding - mean: -0.0091, std: 0.2439\n",
      "After block 0 - mean: -0.0005, std: 0.7882\n",
      "After block 1 - mean: 0.0241, std: 0.7797\n",
      "After block 2 - mean: 0.0026, std: 0.7188\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate jax.debug.print inside a JIT-compiled function\n",
    "@jax.jit\n",
    "def debug_forward_pass(model, inputs):\n",
    "    \"\"\"Forward pass with debug prints to inspect intermediate values.\"\"\"\n",
    "    # Inspect input shape (static and known, so use Python print)\n",
    "    print(\"Input shape:\", inputs.shape)\n",
    "\n",
    "    # Get embeddings and inspect\n",
    "    x = model.embedding_layer(inputs)\n",
    "    jax.debug.print(\"After embedding - mean: {mean:.4f}, std: {std:.4f}\",\n",
    "                    mean=jnp.mean(x), std=jnp.std(x))\n",
    "\n",
    "    # Pass through transformer blocks\n",
    "    for i, block in enumerate(model.transformer_blocks.layers):\n",
    "        x = block(x, training=False)\n",
    "        jax.debug.print(\"After block {i} - mean: {mean:.4f}, std: {std:.4f}\",\n",
    "                        i=i, mean=jnp.mean(x), std=jnp.std(x))\n",
    "\n",
    "    logits = model.output_layer(x)\n",
    "    jax.debug.print(\"Output logits - mean: {mean:.4f}\",\n",
    "                    mean=jnp.mean(logits))\n",
    "    return logits\n",
    "\n",
    "# Run the debug forward pass with a sample input\n",
    "sample_input = jnp.ones((1, maxlen), dtype=jnp.int32)\n",
    "_ = debug_forward_pass(model, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duapvyZAW3yn"
   },
   "source": [
    "### Detecting NaN and Inf values with `jax.debug_nans` and `jax.debug_infs`\n",
    "\n",
    "Exploding gradients during training can lead to either NaN (\"Not a Number\") or Inf (infinity) values, both of which can disrupt training and indicate numerical instability.\n",
    "\n",
    "JAX provides configuration flags to help catch these issues early: `jax_debug_nans` will automatically detect and raise an error if NaN values appear in your computations, and `jax_debug_infs` does the same for Inf values. Both can be invaluable tools for debugging numerical problems in your model.\n",
    "\n",
    "For more on detecting NaN and Inf in JAX computations, see the [Debugging runtime values](https://jax.readthedocs.io/en/latest/debugging/index.html)\n",
    "and the [JAX debugging flags](https://docs.jax.dev/en/latest/debugging/flags.html) for information on useful flags like `jax_debug_nans` and `jax_debug_infs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After block 3 - mean: 0.0208, std: 1.5434\n",
      "Output logits - mean: -11.1983\n",
      "Invalid nan value encountered in the output of a jax.jit function. Calling the de-optimized version.\n",
      "Invalid nan value encountered in the output of a jax.jit function. Calling the de-optimized version.\n",
      "NaN detected during forward pass! (as expected for demonstration)\n",
      "invalid value (nan) encountered in log\n"
     ]
    }
   ],
   "source": [
    "# Enable NaN debugging globally - any NaN will raise an error with a traceback\n",
    "# This is useful during development to catch numerical issues early\n",
    "jax.config.update(\"jax_debug_nans\", True)\n",
    "\n",
    "# Example: This function would raise an error if it produced NaN\n",
    "@jax.jit\n",
    "def safe_forward_pass(model, inputs):\n",
    "    \"\"\"Forward pass that will error if NaN is produced.\"\"\"\n",
    "    logits = model(inputs)\n",
    "    # Contrived example: taking log of a negative number to induce NaN and demonstrate the NaN checker.\n",
    "    logits = jnp.log(-jnp.abs(logits))\n",
    "    return logits\n",
    "\n",
    "sample_input = jnp.ones((1, maxlen), dtype=jnp.int32)\n",
    "\n",
    "try:\n",
    "    _ = safe_forward_pass(model, sample_input)\n",
    "    print(\"Forward pass completed without NaN values\")\n",
    "except FloatingPointError as e:\n",
    "    print(\"NaN detected during forward pass! (as expected for demonstration)\")\n",
    "    print(e)\n",
    "\n",
    "# Disable NaN debugging for the rest of the notebook\n",
    "# (It adds overhead and can slow down training)\n",
    "jax.config.update(\"jax_debug_nans\", False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cuLzhlyoW3yn"
   },
   "source": [
    "**Tip:** You can also use `jax.debug.print()` inside your loss function or training step to monitor gradient health and intermediate values. This is demonstrated in the code cell above showing the forward pass with debug prints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JNxCVBVeLC5M"
   },
   "source": [
    "## Orbax: Save and restore model checkpoints\n",
    "\n",
    "[Orbax](https://orbax.readthedocs.io/) is Google's checkpointing library designed for JAX workloads. It provides efficient, scalable, and flexible checkpoint management for machine learning models.\n",
    "\n",
    "**Why Orbax?**\n",
    "\n",
    "- **JAX-native**: Designed specifically for JAX's pytree structures and distributed arrays\n",
    "- **Asynchronous saving**: Non-blocking checkpoint saves that don't interrupt training\n",
    "- **Scalable**: Handles large models efficiently with support for sharded checkpoints across devices\n",
    "- **Flexible**: Works with any pytree structure, including Flax NNX models\n",
    "- **Cloud-ready**: Integrates with cloud storage backends like Google Cloud Storage (GCS)\n",
    "\n",
    "**Key concepts:**\n",
    "\n",
    "In this tutorial, we use:\n",
    "\n",
    "1. **`nnx.state()`**: Extracts the model's state (parameters, batch stats, etc.) as a pytree\n",
    "2. **`PyTreeCheckpointer`**: The main checkpointer class for saving and loading pytrees\n",
    "3. **`PyTreeSave` / `PyTreeRestore`**: Arguments specifying how to save or restore the checkpoint\n",
    "\n",
    "Let's save our trained model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:[process=0][thread=MainThread][operation_id=1] _SignalingThread.join() waiting for signals ([]) blocking the main thread will slow down blocking save times. This is likely due to main thread calling result() on a CommitFuture.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array_metadatas       d\t\t      _METADATA        _sharding\n",
      "_CHECKPOINT_METADATA  manifest.ocdbt  ocdbt.process_0\n"
     ]
    }
   ],
   "source": [
    "import orbax.checkpoint as orbax\n",
    "\n",
    "state = nnx.state(model)\n",
    "\n",
    "checkpointer = orbax.PyTreeCheckpointer()\n",
    "checkpointer.save('/content/save', args=orbax.args.PyTreeSave(state), force=True)\n",
    "\n",
    "# Make sure the files are there\n",
    "!ls /content/save/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZC2fVGlTW3yn"
   },
   "source": [
    "### Restoring a checkpoint\n",
    "\n",
    "To resume training or use a saved model for inference, restore the checkpoint using `PyTreeRestore`. The restored state can be applied to a new model instance using `nnx.update()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text generated by restored model:\n",
      "Once upon a time there was a very persistent little girl named Lily. She loved to sing and sing all day long. One day, her mom said she couldn't sing because it got very dirty. \n",
      "Lily was sad because her mom told her it was important to go to the park. \n",
      "But then, Lily realized that she couldn't sing a beautiful song. It was her favorite color was too much better than the little girl. \n",
      "Lily realized that it was important to listen to her mom and sing songs. She sang every day and sang songs every day. \n",
      "But one day, Lily went back into!!!!"
     ]
    }
   ],
   "source": [
    "# Restore the checkpoint with proper sharding info\n",
    "from orbax.checkpoint import checkpoint_utils\n",
    "\n",
    "# Create restore args from the target state to provide sharding info\n",
    "restore_args = checkpoint_utils.construct_restore_args(state)\n",
    "restored_state = checkpointer.restore(\n",
    "    '/content/save',\n",
    "    args=orbax.args.PyTreeRestore(state, restore_args=restore_args)\n",
    ")\n",
    "\n",
    "# Create a fresh model instance and update it with restored state\n",
    "with jax.set_mesh(mesh):\n",
    "    restored_model = create_model(rngs=nnx.Rngs(42))  # Different seed to prove restoration works\n",
    "    nnx.update(restored_model, restored_state)\n",
    "\n",
    "# Verify restoration works by generating text\n",
    "print(\"Text generated by restored model:\")\n",
    "_ = restored_model.generate_text(maxlen, start_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kT28EUaDLC5M"
   },
   "source": [
    "## Tunix: Fine-tuning\n",
    "\n",
    "[Tunix](https://github.com/google/tunix) is a JAX-native LLM post-training library open sourced by Google. It supports a range of post-training techniques including supervised finetuning, preference tuning, reinforcement learning and model distillation. In this section, we are going to use Tunix to finetune the miniGPT model we just pretrained using LoRA ([Low-Rank Adaptation](https://arxiv.org/abs/2106.09685)) so that the finetuned model generates output of a different style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uxnW69mnLC5M"
   },
   "source": [
    "First we install Tunix and its dependencies, and import necessary libraries. Note that Colab will ask you to restart the runtime, but you can just ignore it.\n",
    "\n",
    "**Note:** this section assume multiple TPU cores. Free-tier Colab TPU v5e-1 cannot run here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/201.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.0/201.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2026.1.0 requires fsspec==2026.1.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -Uq google-tunix[prod]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qwix\n",
    "import numpy as np\n",
    "from tunix.sft import peft_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Od2XV7hyLC5M"
   },
   "source": [
    "We set some hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA Hyperparameters\n",
    "lora_rank = 16\n",
    "lora_alpha = 2.0\n",
    "lora_max_steps = 400\n",
    "lora_num_epochs = 20\n",
    "lora_batch_size = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOlfgGu8LC5M"
   },
   "source": [
    "For LoRA fintuning we use the [Tiny Shakespeare](https://www.tensorflow.org/datasets/catalog/tiny_shakespeare) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "nbval-ignore-output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-02-06 14:45:23--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘TinyShakespeare.txt’\n",
      "\n",
      "\r",
      "TinyShakespeare.txt   0%[                    ]       0  --.-KB/s               \r",
      "TinyShakespeare.txt 100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
      "\n",
      "2026-02-06 14:45:23 (32.0 MB/s) - ‘TinyShakespeare.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O TinyShakespeare.txt\n",
    "\n",
    "def load_shakespeare_dataset(batch_size, max_len, num_epochs):\n",
    "    input_file = \"TinyShakespeare.txt\"\n",
    "    # Read the entire text file\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Tokenize the entire text (assuming tokenizer is available in scope)\n",
    "    tokens = tokenizer.encode(text)\n",
    "\n",
    "    # Create a simple data source from the tokens\n",
    "    class TokenDataSource(pygrain.RandomAccessDataSource):\n",
    "        def __init__(self, tokens, max_len):\n",
    "            self._tokens = np.array(tokens, dtype=np.int32)\n",
    "            self._max_len = max_len\n",
    "            # Calculate how many sequences we can create\n",
    "            self._length = len(self._tokens) // max_len\n",
    "\n",
    "        def __len__(self):\n",
    "            return self._length\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            # Return a sequence of max_len tokens\n",
    "            start_idx = index * self._max_len\n",
    "            end_idx = start_idx + self._max_len\n",
    "            return self._tokens[start_idx:end_idx]\n",
    "\n",
    "    # Create the data source\n",
    "    data_source = TokenDataSource(tokens, max_len)\n",
    "\n",
    "    # Create a sampler\n",
    "    sampler = pygrain.IndexSampler(\n",
    "        num_records=len(data_source),\n",
    "        shuffle=True,\n",
    "        seed=42,\n",
    "        num_epochs=num_epochs,\n",
    "        shard_options=pygrain.NoSharding()\n",
    "    )\n",
    "\n",
    "    # Create transformations\n",
    "    class ToTrainingInputDict(pygrain.MapTransform):\n",
    "        def map(self, batch):\n",
    "            return {\n",
    "                \"input_tokens\": batch,\n",
    "                \"input_mask\": np.ones_like(batch)\n",
    "            }\n",
    "\n",
    "    # Create the data loader\n",
    "    loader = pygrain.DataLoader(\n",
    "        data_source=data_source,\n",
    "        sampler=sampler,\n",
    "        operations=[\n",
    "            pygrain.Batch(batch_size=batch_size, drop_remainder=True),\n",
    "            ToTrainingInputDict(),\n",
    "        ],\n",
    "        worker_count=0,  # Use main thread\n",
    "    )\n",
    "\n",
    "    def to_training_input(loader):\n",
    "        # The trainer expects an iterable of `peft_trainer.TrainingInput`.\n",
    "        for item in loader:\n",
    "            yield peft_trainer.TrainingInput(**item)\n",
    "\n",
    "    return to_training_input(loader)\n",
    "\n",
    "lora_train_ds = load_shakespeare_dataset(lora_batch_size, maxlen, lora_num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VWAHNKMYLC5M"
   },
   "source": [
    "We define a few helper functions to create the LoRA model, loss, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lora_model(base_model, mesh):\n",
    "  lora_provider = qwix.LoraProvider(\n",
    "      # Target only feed-forward layers (standard 2D linear layers)\n",
    "      # Note: MultiHeadAttention uses LinearGeneral with 3D weights,\n",
    "      # which qwix LoRA doesn't support\n",
    "      module_path=\".*linear1|.*linear2\",\n",
    "      rank=lora_rank,\n",
    "      alpha=lora_alpha,\n",
    "  )\n",
    "\n",
    "  model_input = base_model.get_model_input()\n",
    "  lora_model = qwix.apply_lora_to_model(\n",
    "      base_model, lora_provider, rngs=nnx.Rngs(0), **model_input\n",
    "  )\n",
    "\n",
    "  with jax.set_mesh(mesh):\n",
    "    state = nnx.state(lora_model)\n",
    "    pspecs = nnx.get_partition_spec(state)\n",
    "    sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
    "    nnx.update(lora_model, sharded_state)\n",
    "\n",
    "  return lora_model\n",
    "\n",
    "def gen_model_input_fn(x: peft_trainer.TrainingInput):\n",
    "  return {\n",
    "      'inputs': x.input_tokens,\n",
    "      'training': True\n",
    "  }\n",
    "\n",
    "def lora_loss_fn(model, inputs, training):\n",
    "    inputs = inputs\n",
    "    targets = jnp.concatenate([inputs[:, 1:], jnp.zeros((inputs.shape[0], 1), dtype=jnp.int32)], axis=1)\n",
    "    logits = model(inputs, training=training)\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=targets).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIyxXBo2LC5M"
   },
   "source": [
    "Now we can start the finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LoRA Finetuning...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35690cc9d19a44279e77dc3d76127952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/400 [00:00<?, ?step/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Starting LoRA Finetuning...\")\n",
    "with jax.set_mesh(mesh):\n",
    "    # Apply LoRA to the model\n",
    "    lora_model = get_lora_model(model, mesh)\n",
    "\n",
    "    # Setup Tunix PeftTrainer\n",
    "    training_config = peft_trainer.TrainingConfig(\n",
    "        eval_every_n_steps=None,\n",
    "        max_steps=lora_max_steps,\n",
    "        data_sharding_axis=('batch',),\n",
    "    )\n",
    "    lora_optimizer = optax.adamw(1e-2)\n",
    "    lora_trainer = peft_trainer.PeftTrainer(\n",
    "        lora_model, lora_optimizer, training_config\n",
    "    ).with_gen_model_input_fn(gen_model_input_fn).with_loss_fn(lora_loss_fn)\n",
    "\n",
    "    # Run LoRA training\n",
    "    lora_trainer.train(lora_train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text after LoRA finetuning:\n",
      "\n",
      "\n",
      "Once upon a time; but the best\n",
      "That you do, that, that I may be again.\n",
      "\n",
      "DUCHOMCOMO:\n",
      "I will not be sad to give him a soul,\n",
      "I will be sad.\n",
      "\n",
      "S?!\"\n",
      "COM:\n",
      "You have had no more!\n",
      "\n",
      "BOMY:\n",
      "I'll not tell me again, but my dear or in the sun.\n",
      "\n",
      "DUESY:\n",
      "I must not to be a woman, but your head,\n",
      "And I will find him.\n",
      "\n",
      "Jucks!.\n",
      "HINGFallBELLI Card:\n",
      "!!!!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Once upon a time; but the best\\nThat you do, that, that I may be again.\\n\\nDUCHOMCOMO:\\nI will not be sad to give him a soul,\\nI will be sad.\\n\\nS?!\"\\nCOM:\\nYou have had no more!\\n\\nBOMY:\\nI\\'ll not tell me again, but my dear or in the sun.\\n\\nDUESY:\\nI must not to be a woman, but your head,\\nAnd I will find him.\\n\\nJucks!.\\nHINGFallBELLI Card:\\n!!!!'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate text with LoRA-finetuned model\n",
    "print(\"Generating text after LoRA finetuning:\\n\\n\")\n",
    "lora_model.generate_text(maxlen, start_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcaKF4WTLC5M"
   },
   "source": [
    "## Xprof: Profiling for hyperparameter tuning\n",
    "\n",
    "[Xprof](https://openxla.org/xprof) (integrated via TensorBoard's profiler plugin) is essential for understanding and optimizing JAX/TPU performance. Profiling helps identify bottlenecks and guides hyperparameter decisions.\n",
    "\n",
    "**Why profile your training?**\n",
    "\n",
    "- **Identify bottlenecks**: See whether you're limited by compute, memory, or host-device communication\n",
    "- **Optimize utilization**: Maximize FLOPS utilization on expensive accelerators\n",
    "- **Compare configurations**: Quantitatively evaluate different batch sizes, parallelism strategies, etc.\n",
    "- **Debug performance**: Understand why training is slower than expected\n",
    "\n",
    "**Key profiling concepts:**\n",
    "\n",
    "1. **FLOPS Utilization**: Percentage of theoretical compute capacity being used (higher is better)\n",
    "2. **Step Time**: Time per training step (lower is better, but consider batch size)\n",
    "3. **Trace Viewer**: Visual timeline showing operations on each device\n",
    "4. **Memory Profile**: Track memory usage to avoid out-of-memory (OOM) errors\n",
    "\n",
    "**How to profile in JAX:**\n",
    "\n",
    "JAX provides built-in profiling through `jax.profiler`:\n",
    "- `jax.profiler.start_trace(log_dir)`: Begin recording a trace\n",
    "- `jax.profiler.StepTraceAnnotation`: Annotate training steps for easier analysis\n",
    "- `jax.profiler.stop_trace()`: Stop recording and save the trace to disk\n",
    "\n",
    "The traces can be visualized using TensorBoard with the profiler plugin.\n",
    "\n",
    "**Note:** This section assumes multiple TPU cores. Free-tier Colab TPU v5e-1 cannot run these examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 4.5.0 requires fsspec[http]<=2025.10.0,>=2023.1.0, but you have fsspec 2026.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -Uq tensorboard-plugin-profile tensorflow tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-eEyPE7iLC5M"
   },
   "source": [
    "Load the tensorboard colab extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoMR295JLC5M"
   },
   "source": [
    "As we're going to be running this model a number of times, we need some scaffolding to more easily compare our work. For a baseline, we'll need to perform some warmup to guarantee that our code is JIT'd and that our TPUs are warm. For improved comparability, we'll only start tracing after we've finished warmup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_dir = \"/tmp/jax-trace/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_step(batch, step):\n",
    "    input_batch = jnp.array(jnp.array(batch).T)\n",
    "    target_batch = prep_target_batch(input_batch)\n",
    "    with jax.set_mesh(mesh):\n",
    "        sharded_batch = jax.device_put(\n",
    "            (input_batch, target_batch),\n",
    "            NamedSharding(mesh, P(\"batch\", None))\n",
    "        )\n",
    "        train_step(model, optimizer, metrics, sharded_batch)\n",
    "\n",
    "def generate_trace():\n",
    "    tracing_steps = 30\n",
    "    warmup_steps = 5\n",
    "    for current_step in range(warmup_steps + tracing_steps):\n",
    "        if current_step == warmup_steps:\n",
    "            jax.profiler.start_trace(trace_dir)\n",
    "        with jax.profiler.StepTraceAnnotation(\"train\", step_num=current_step):\n",
    "            batch = next(text_dl)\n",
    "            loop_step(batch, current_step)\n",
    "\n",
    "    jax.profiler.stop_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRsyWzRYLC5M"
   },
   "source": [
    "Now we'll perform some traces to compare results of different batch sizes. This will take several minutes as we need to reprocess our input data to prepare new batches each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 589,692 stories\n",
      "Loaded 589,692 stories\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "text_dl = iter(load_and_preprocess_data('TinyStories-train.txt', batch_size, maxlen))\n",
    "generate_trace()\n",
    "\n",
    "batch_size = 128\n",
    "text_dl = iter(load_and_preprocess_data('TinyStories-train.txt', batch_size, maxlen))\n",
    "generate_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RS-BbP-5LC5M"
   },
   "source": [
    "Run Tensorboard with the Profiler Plugin to compare our runs. Runs are listed in order from newest to oldest, so the top run in the list will be have `batch_size = 128`.\n",
    "\n",
    "The key metrics to focus on here for this hyperparameter are FLOPS Utilization and Average Step Time.\n",
    "\n",
    "In general, we want to maximize FLOPS Utilization while minimizing the step time per training example. In this case, we can see that increasing the batch size from `32` -> `128` achieves both of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=/tmp/jax-trace/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfhZbpiZZYod"
   },
   "source": [
    "## Inference with vLLM\n",
    "\n",
    "After training a language model, you need an efficient way to serve it for inference. While our simple `generate_text()` method works for experimentation, production deployments require optimizations like batched inference, KV-cache management, and continuous batching.\n",
    "\n",
    "[vLLM](https://docs.vllm.ai/) is a high-throughput inference engine that addresses these challenges:\n",
    "\n",
    "**Why vLLM?**\n",
    "\n",
    "- **PagedAttention**: Efficient memory management for KV-cache, reducing memory waste by up to 90%\n",
    "- **Continuous batching**: Dynamically batches incoming requests for higher throughput\n",
    "- **Optimized kernels**: Hardware-specific optimizations for fast inference\n",
    "- **Simple API**: Just a few lines of code to serve any HuggingFace model\n",
    "\n",
    "**Basic vLLM usage:**\n",
    "\n",
    "```python\n",
    "# Install: pip install vllm\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "# Load a model (downloads from HuggingFace automatically)\n",
    "llm = LLM(model=\"distilgpt2\")\n",
    "\n",
    "# Configure generation parameters\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "# Generate text - vLLM handles batching automatically\n",
    "prompts = [\"Once upon a time\", \"The quick brown fox\"]\n",
    "outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "for output in outputs:\n",
    "    print(f\"Prompt: {output.prompt!r}\")\n",
    "    print(f\"Generated: {output.outputs[0].text!r}\")\n",
    "```\n",
    "\n",
    "**Serving our trained miniGPT model:**\n",
    "\n",
    "To use our JAX-trained miniGPT with vLLM, we need to convert it to a format vLLM understands. There are two approaches:\n",
    "\n",
    "**Option 1: Convert to HuggingFace format**\n",
    "\n",
    "Save the model weights and config in HuggingFace format, then load with vLLM:\n",
    "\n",
    "```python\n",
    "# Save miniGPT in HuggingFace format\n",
    "from transformers import GPT2Config, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Create HF config matching our architecture\n",
    "config = GPT2Config(\n",
    "    vocab_size=vocab_size,\n",
    "    n_positions=maxlen,\n",
    "    n_embd=embed_dim,\n",
    "    n_layer=num_transformer_blocks,\n",
    "    n_head=num_heads,\n",
    ")\n",
    "\n",
    "# Create HF model and copy weights from JAX\n",
    "hf_model = GPT2LMHeadModel(config)\n",
    "# ... (copy weights from JAX arrays to PyTorch tensors)\n",
    "\n",
    "# Save and load with vLLM\n",
    "hf_model.save_pretrained(\"./minigpt-hf\")\n",
    "llm = LLM(model=\"./minigpt-hf\")\n",
    "```\n",
    "\n",
    "**Option 2: Use vLLM's JAX/TPU support**\n",
    "\n",
    "For TPU deployment, vLLM provides a [dedicated TPU backend](https://docs.vllm.ai/projects/tpu/):\n",
    "\n",
    "```python\n",
    "# Install vLLM TPU: follow https://docs.vllm.ai/projects/tpu/\n",
    "from vllm import LLM\n",
    "\n",
    "# vLLM TPU supports JAX models directly\n",
    "llm = LLM(model=\"./minigpt-jax\", device=\"tpu\")\n",
    "```\n",
    "\n",
    "**Note:** vLLM requires a dedicated GPU or TPU runtime and cannot run in the same Python session after JAX has been initialized with a different backend. To experiment with vLLM, start a fresh runtime.\n",
    "\n",
    "For more details, see the [vLLM documentation](https://docs.vllm.ai/) and [vLLM TPU guide](https://docs.vllm.ai/projects/tpu/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kp_z3QVbLC5M",
    "tags": [
     "nbval-skip"
    ]
   },
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've successfully trained a miniGPT language model from scratch using JAX and its AI ecosystem. Let's recap what you've accomplished:\n",
    "\n",
    "**Key accomplishments:**\n",
    "\n",
    "- **Built a transformer architecture** using Flax NNX's intuitive, Pythonic API\n",
    "- **Loaded data efficiently** with Grain's high-performance data pipeline\n",
    "- **Trained with Optax** using the Adam optimizer and cross-entropy loss\n",
    "- **Leveraged parallelism** with JAX's automatic SPMD for data and tensor parallelism\n",
    "- **Debugged your model** using `jax.debug.print()` and `jax.debug_nans`\n",
    "- **Saved checkpoints** using Orbax for model persistence\n",
    "- **Fine-tuned with LoRA** using Tunix for parameter-efficient adaptation\n",
    "- **Profiled performance** with Xprof to optimize hyperparameters\n",
    "- **Explored production inference** with vLLM for high-throughput serving\n",
    "\n",
    "**JAX AI Stack libraries used:**\n",
    "\n",
    "| Library | Purpose |\n",
    "| :------ | :------ |\n",
    "| [JAX](https://jax.readthedocs.io) | High-performance array computing with automatic differentiation |\n",
    "| [Flax NNX](https://flax.readthedocs.io) | Neural network definition with intuitive API |\n",
    "| [Optax](https://optax.readthedocs.io) | Gradient processing and optimization |\n",
    "| [Grain](https://google-grain.readthedocs.io) | Efficient data loading |\n",
    "| [Orbax](https://orbax.readthedocs.io) | Checkpoint management |\n",
    "| [Tunix](https://github.com/google/tunix) | LLM fine-tuning (LoRA, RLHF, etc.) |\n",
    "\n",
    "**Next steps:**\n",
    "\n",
    "- **Scale up**: Try larger model dimensions, more transformer blocks, or longer sequences\n",
    "- **Experiment with datasets**: Train on different text corpora for varied outputs\n",
    "- **Explore advanced techniques**: Implement learning rate schedules, gradient clipping, or mixed precision training\n",
    "- **Deploy to production**: Integrate your model with vLLM for production serving\n",
    "\n",
    "**Additional resources:**\n",
    "\n",
    "- [JAX Documentation](https://jax.readthedocs.io)\n",
    "- [Flax NNX Guide](https://flax.readthedocs.io/en/latest/nnx_basics.html)\n",
    "- [JAX AI Stack Tutorials](https://docs.jaxstack.ai)\n",
    "- [Google Cloud TPU Documentation](https://cloud.google.com/tpu/docs)\n",
    "- [vLLM TPU Documentation](https://docs.vllm.ai/projects/tpu/)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
