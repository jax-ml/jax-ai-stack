{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA5x53bGMT2w"
      },
      "source": [
        "# Train a miniGPT language model with JAX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNvPJpcW7esj"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/jax-ml/jax-ai-stack/blob/main/docs/source/JAX_for_LLM_pretraining.ipynb\"><img src=\"https://www.kaggle.com/static/images/logos/kaggle-logo-transparent-300.png\" height=\"32\" width=\"70\"/>Run in Kaggle</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/jax-ml/jax-ai-stack/blob/main/docs/source/JAX_for_LLM_pretraining.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/jax-ml/jax-ai-stack/blob/main/docs/source/JAX_for_LLM_pretraining.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIOXoY1xgiww"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Welcome to this comprehensive tutorial on training a miniGPT language model using JAX and its AI ecosystem. This hands-on guide will walk you through the complete process of building, training, and optimizing a small but functional GPT-style language model.\n",
        "\n",
        "**What you'll learn:**\n",
        "\n",
        "This tutorial demonstrates how to use JAX, [Flax NNX](http://flax.readthedocs.io) and [Optax](http://optax.readthedocs.io) for language model (pre)training using data and tensor [parallelism](https://docs.jax.dev/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html). It was originally inspired by the [Keras miniGPT tutorial](https://keras.io/examples/generative/text_generation_with_miniature_gpt/).\n",
        "\n",
        "By the end of this tutorial, you will be able to:\n",
        "\n",
        "- **Understand JAX's parallelism capabilities**: Learn how to leverage data and tensor parallelism to distribute training across multiple TPU devices\n",
        "- **Build transformer models with Flax NNX**: Define a GPT-style architecture using the modern Flax NNX API\n",
        "- **Process data efficiently with Grain**: Load and preprocess training data using Google's Grain data loading library\n",
        "- **Optimize models with Optax**: Implement training loops using JAX's gradient transformation library\n",
        "- **Train on cloud TPUs**: Execute training on free TPUs available through Kaggle or Google Colab\n",
        "- **Fine-tune with LoRA**: Apply parameter-efficient fine-tuning using the Tunix library\n",
        "- **Profile and optimize**: Use JAX's profiling tools to identify bottlenecks and tune hyperparameters\n",
        "\n",
        "**What to expect:**\n",
        "\n",
        "You'll train a miniGPT model from scratch on the TinyStories dataset, watch it progress from generating random text to coherent short stories, and then fine-tune it on Shakespeare's works to change its writing style. The entire training process takes approximately 20 minutes on a free Colab TPU.\n",
        "\n",
        "**Prerequisites:**\n",
        "\n",
        "If you are new to JAX for AI, check out the [introductory tutorial](https://docs.jaxstack.ai/en/latest/neural_net_basics.html), which covers neural network building with [Flax NNX](https://flax.readthedocs.io/en/latest/nnx_basics.html). Basic familiarity with Python, neural networks, and transformers is helpful but not required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTmz5Cbco7n_"
      },
      "source": [
        "## Setup\n",
        "\n",
        "JAX installation is covered in [this guide](https://jax.readthedocs.io/en/latest/installation.html) on the JAX documentation site. We will use [Tiktoken](https://github.com/openai/tiktoken) for tokenization and [Grain](https://google-grain.readthedocs.io/en/latest/index.html) for data loading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6zMsOIc7ouCO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "734dbba3-4527-4bfc-b94e-a4289d66c76c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.0/101.0 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m466.3/466.3 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m106.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.6/585.6 kB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m180.5/180.5 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m147.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.7/79.7 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -Uq tiktoken jax-ai-stack[grain] matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rcji_799n4eA"
      },
      "source": [
        "**Note:** If you are using [Kaggle](https://www.kaggle.com/), select the free TPU v5e-8 as the hardware accelerator. If you are using [Google Colab](https://colab.research.google.com/), select the free Google Cloud TPU v5e-1 as the hardware accelerator. You may also use Google Cloud TPUs.\n",
        "\n",
        "Check the available JAX devices, or [`jax.Device`](https://jax.readthedocs.io/en/latest/_autosummary/jax.Device.html), with [`jax.devices()`](https://jax.readthedocs.io/en/latest/_autosummary/jax.devices.html). The output of the cell below will show a list of 8 (eight) devices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LS9sQEY3n0mB",
        "outputId": "29c2e1dc-e49c-484a-96db-3b6c0df901a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jax/_src/cloud_tpu_init.py:93: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import jax\n",
        "jax.devices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_CVSP5IrI80R"
      },
      "outputs": [],
      "source": [
        "# Verify that an accelerator (GPU/TPU) is available\n",
        "# This tutorial requires hardware acceleration and cannot run on CPU\n",
        "accelerator_available = any(d.platform in ('gpu', 'tpu') for d in jax.devices())\n",
        "\n",
        "if not accelerator_available:\n",
        "    raise RuntimeError(\n",
        "        \"No GPU or TPU accelerator found. This tutorial requires hardware acceleration.\\n\"\n",
        "        \"Please select an accelerator in your runtime settings:\\n\"\n",
        "        \"  - Kaggle: Settings → Accelerator → GPU or TPU\\n\"\n",
        "        \"  - Colab: Runtime → Change runtime type → GPU or TPU\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHzJ_bokoovZ"
      },
      "source": [
        "Get the [TinyStories dataset from Hugging Face](https://huggingface.co/datasets/roneneldan/TinyStories). We only use the training split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUjQsgQEmI1N"
      },
      "outputs": [],
      "source": [
        "!wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStories-train.txt?download=true -O TinyStories-train.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKE2uUafLobI"
      },
      "source": [
        "Import the necessary modules, including JAX NumPy, Flax NNX, Optax, Grain, pandas, and Tiktoken:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MKYFNOhdLq98"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax.sharding import PartitionSpec as P, NamedSharding\n",
        "\n",
        "import flax.nnx as nnx\n",
        "import optax\n",
        "\n",
        "from dataclasses import dataclass\n",
        "import grain.python as pygrain\n",
        "import pandas as pd\n",
        "import tiktoken\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XhqjGAFMT2w"
      },
      "source": [
        "## JAX: High-performance array computing\n",
        "\n",
        "[JAX](https://jax.readthedocs.io) is a Python library for high-performance numerical computing and machine learning research. It combines the familiar NumPy API with powerful program transformations to enable automatic differentiation, vectorization, and parallelization.\n",
        "\n",
        "**Key features of JAX:**\n",
        "\n",
        "- **NumPy compatibility**: JAX provides `jax.numpy`, a drop-in replacement for NumPy that runs on accelerators like GPUs and TPUs\n",
        "- **Automatic differentiation**: The `jax.grad()` function computes gradients automatically, making it easy to implement gradient-based optimization\n",
        "- **JIT compilation**: `jax.jit()` compiles Python functions to optimized machine code for faster execution\n",
        "- **Automatic parallelization**: JAX can automatically distribute computations across multiple devices using SPMD (Single Program, Multiple Data) parallelism\n",
        "- **Functional programming**: JAX encourages pure functions, which enables reliable transformations and better performance\n",
        "\n",
        "For training large language models like our miniGPT, JAX's automatic parallelization capabilities are particularly valuable. They allow us to efficiently utilize multiple TPU cores without manually managing device placement and communication."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4-gksXxMT3B"
      },
      "source": [
        "The three core JAX transforms you'll need understand are:\n",
        "- **[`jax.jit`](https://jax.readthedocs.io/en/latest/jax-101/02-jitting.html)**: Just-in-time compilation via XLA for fast execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "00mK9dhEMT3B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84300405-2ae0-4a1c-f985-577096f7a742"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "358 µs ± 30.6 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
            "250 µs ± 28 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ],
      "source": [
        "def slow_fn(x):\n",
        "    for _ in range(5):\n",
        "        x = x @ x\n",
        "    return x\n",
        "\n",
        "fast_fn = jax.jit(slow_fn)\n",
        "\n",
        "x = jnp.ones((1000, 1000))\n",
        "\n",
        "%timeit slow_fn(x).block_until_ready()\n",
        "%timeit fast_fn(x).block_until_ready()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Binla6QaMT3B"
      },
      "source": [
        "- **[`jax.grad`](https://jax.readthedocs.io/en/latest/jax-101/04-advanced-autodiff.html)**: Automatic differentiation for computing gradients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eUx06xrMT3B",
        "outputId": "3a39f0b5-814b-42a8-e31d-0f147299f905"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x = [1. 2. 3.]\n",
            "loss(x) = 14.0\n",
            "grad(loss)(x) = [2. 4. 6.]\n"
          ]
        }
      ],
      "source": [
        "def loss(x):\n",
        "    return jnp.sum(x ** 2)\n",
        "\n",
        "grad_loss = jax.grad(loss)\n",
        "\n",
        "x = jnp.array([1.0, 2.0, 3.0])\n",
        "print(f\"x = {x}\")\n",
        "print(f\"loss(x) = {loss(x)}\")\n",
        "print(f\"grad(loss)(x) = {grad_loss(x)}\")  # Derivative of x^2 is 2x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLr9GIPhMT3B"
      },
      "source": [
        "- **[`jax.vmap`](https://jax.readthedocs.io/en/latest/jax-101/03-vectorization.html)**: Automatic vectorization to batch operations efficiently"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUodslgqMT3B",
        "outputId": "4f1dd710-1a8b-4962-ed78-5f040d34f33f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.26726124 0.5345225  0.8017837 ]\n",
            " [0.45584232 0.5698029  0.6837635 ]\n",
            " [0.50257075 0.57436657 0.6461624 ]]\n"
          ]
        }
      ],
      "source": [
        "# Function that operates on a single vector\n",
        "def normalize(x):\n",
        "    return x / jnp.linalg.norm(x)\n",
        "\n",
        "# Create a batch of vectors\n",
        "batch = jnp.array([[1.0, 2.0, 3.0],\n",
        "                   [4.0, 5.0, 6.0],\n",
        "                   [7.0, 8.0, 9.0]])\n",
        "\n",
        "# vmap automatically vectorizes over the batch dimension\n",
        "batch_normalize = jax.vmap(normalize)\n",
        "print(batch_normalize(batch))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Finf-IGrMT3B"
      },
      "source": [
        "For a deeper introduction to JAX fundamentals, see the [JAX 101 tutorials](https://jax.readthedocs.io/en/latest/jax-101/index.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPyt7MV6prz1"
      },
      "source": [
        "## Define the miniGPT model with NNX and JAX automatic parallelism\n",
        "\n",
        "### NNX: A JAX-based neural network library\n",
        "\n",
        "[Flax NNX](https://flax.readthedocs.io/en/latest/nnx_basics.html) is the next-generation neural network library for JAX, designed to make building and training models more intuitive and Pythonic. NNX is part of the Flax ecosystem and represents a modernized approach to neural network development.\n",
        "\n",
        "**Why NNX?**\n",
        "\n",
        "- **Stateful and intuitive**: Unlike the original Flax (Linen), NNX uses a stateful, object-oriented API that feels more natural for Python developers\n",
        "- **Familiar syntax**: Define models using standard Python classes with `__init__` and `__call__` methods, similar to PyTorch\n",
        "- **Seamless JAX integration**: NNX works smoothly with JAX transformations like `jit`, `grad`, and `vmap`\n",
        "- **Built-in modules**: Provides common layers like `nnx.Linear`, `nnx.MultiHeadAttention`, and `nnx.LayerNorm` out of the box\n",
        "- **Flexible state management**: Easily separate and manage different types of state (parameters, batch statistics, etc.)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVon1P8RMT3B",
        "outputId": "2b8620ed-d738-482e-8a95-c54efcaa039b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: (4, 2)\n"
          ]
        }
      ],
      "source": [
        "# A simple two-layer MLP in Flax NNX\n",
        "class SimpleMLP(nnx.Module):\n",
        "    def __init__(self, in_features, hidden_size, out_features, rngs: nnx.Rngs):\n",
        "        self.linear1 = nnx.Linear(in_features, hidden_size, rngs=rngs)\n",
        "        self.linear2 = nnx.Linear(hidden_size, out_features, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = nnx.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "# Create model and run a forward pass\n",
        "simple_model = SimpleMLP(in_features=3, hidden_size=16, out_features=2, rngs=nnx.Rngs(0))\n",
        "x = jnp.ones((4, 3))  # batch of 4 samples, 3 features each\n",
        "print(f\"Output shape: {simple_model(x).shape}\")  # (4, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oB-CSHQGMT3B"
      },
      "source": [
        "In this tutorial, we'll use NNX to define our transformer architecture, leveraging its clean API to build the attention mechanisms, feed-forward networks, and embeddings that make up our miniGPT model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGBhVn4HMT3B"
      },
      "source": [
        "### Leveraging JAX's data and tensor parallelism\n",
        "\n",
        "One of the most powerful features of JAX is [device parallelism](https://docs.jax.dev/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html) for SPMD.\n",
        "\n",
        "- The data parallelism technique enables, for example, the training data to run via multiple parts (this is called sharding) - batches - in parallel and simultaneously across different devices, such as GPUs and Google TPUs. This allows to use larger batch sizes to speed up training.\n",
        "- Tensor parallelism allows us to split the model parameter tensors across several devices (sharding model tensors).\n",
        "- You can learn more about the basics of JAX parallelism in more detail in the [Introduction to parallel programming](https://jax.readthedocs.io/en/latest/sharded-computation.html) on the JAX documentation site.\n",
        "\n",
        "In this example, we'll utilize a 4-way data parallel and 2-way tensor parallel setup, which is aligned with Kaggle TPU v5e-8 or newer GCP TPUs chips.\n",
        "\n",
        "Note that as of October 2025, free-tier Colab only offers TPU v5e-1, which can no longer support SPMD.\n",
        "\n",
        "### jax.sharding.Mesh\n",
        "\n",
        "[`jax.sharding.Mesh`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.Mesh) is a multidimensional NumPy array of JAX devices, where each axis of the mesh has a name, such as `'x'` or `'y'`. This will help encapsulate the information about the TPU resource organization for distributing computations across the devices.\n",
        "\n",
        "Our `Mesh` will have two arguments:\n",
        "- `devices`: This will take the value of [`jax.make_mesh((4, 2), ('batch', 'model'))`](https://jax.readthedocs.io/en/latest/jax.experimental.mesh_utils.html), enabling us to build a device mesh. It is a NumPy ndarray with JAX devices (a list of devices from the JAX backend as obtained from [`jax.devices()`](https://jax.readthedocs.io/en/latest/_autosummary/jax.devices.html#jax.devices))..\n",
        "- `axis_names`, where:\n",
        "  - `batch`: 4 devices along the first axis - i.e. sharded into 4 - for data parallelism; and\n",
        "  - `model`: 2 devices along the second axis - i.e. sharded into 2 -  for tensor parallism\n",
        "\n",
        "This matches the structure in the Kaggle TPU v5e setup.\n",
        "\n",
        "Let's instantiate `Mesh` as `mesh` and declare the TPU configuration to define how data and model parameters are distributed across the devices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xuMlCK3Q8WJD",
        "outputId": "b42e7f88-52f3-4525-d237-8e18bf97718e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created mesh with 1 devices: Mesh('batch': 1, 'model': 1, axis_types=(Auto, Auto))\n",
            "Mesh shape: OrderedDict({'batch': 1, 'model': 1}) (batch=1, model=1)\n"
          ]
        }
      ],
      "source": [
        "# Create a `Mesh` object representing TPU device arrangement.\n",
        "# The mesh defines how we distribute computation across devices.\n",
        "\n",
        "if jax.device_count() == 8:\n",
        "    # For Kaggle TPU v5e-8 or similar 8-core TPUs:\n",
        "    # Split 8 devices into a 4x2 grid for data and model parallelism\n",
        "    mesh = jax.make_mesh((4, 2), ('batch', 'model'))\n",
        "\n",
        "    # Alternative: Use 8-way data parallelism (no model parallelism)\n",
        "    # Uncomment the line below to experiment with this configuration:\n",
        "    # mesh = jax.make_mesh((8, 1), ('batch', 'model'))\n",
        "\n",
        "elif jax.device_count() == 1:\n",
        "    # For free-tier Colab TPU v5e-1 (single core)\n",
        "    # No parallelism is possible, but we still create a mesh for consistency\n",
        "    mesh = jax.make_mesh((1, 1), ('batch', 'model'))\n",
        "\n",
        "else:\n",
        "    # For other device counts, use all devices for data parallelism\n",
        "    mesh = jax.make_mesh((jax.device_count(), 1), ('batch', 'model'))\n",
        "\n",
        "print(f\"Created mesh with {jax.device_count()} devices: {mesh}\")\n",
        "print(f\"Mesh shape: {mesh.shape} (batch={mesh.shape['batch']}, model={mesh.shape['model']})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZKdhNo98NgG"
      },
      "source": [
        "We will use the GPT-2 tokenizer from the [Tiktoken](https://github.com/openai/tiktoken) library:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iWbkk1V7-Isg"
      },
      "outputs": [],
      "source": [
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XHQ0BQ9-KIj"
      },
      "source": [
        "To leverage model parallelism, we need to instruct the JAX compiler how to shard the model tensors across the TPU devices. We'll use Flax NNX's [`flax.nnx.with_partitioning`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/spmd.html#flax.nnx.with_partitioning) to let each model layer know that the model weights or tensors need to be sharded according to our specification. We need to do this for every tensor/layer in the model.\n",
        "\n",
        "`nnx.with_partitioning` will take two arguments, such as the `initializer` (e.g. [`flax.nnx.initializers.xavier_uniform`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/initializers.html#flax.nnx.initializers.xavier_uniform) and [`flax.nnx.initializers.zeros_init`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/initializers.html#flax.nnx.initializers.zeros_init)), and a sharding tuple (e.g. `(None, 'model')` in our case) wrapped in [`jax.sharding.PartitionSpec`](https://docs.jax.dev/en/latest/jax.sharding.html#jax.sharding.PartitionSpec). The sharding tuple describe how to shard a tensor across, for example, the `model` axis or be replicated on other dimensions (which is denoted by `None`).\n",
        "\n",
        "For a more detailed discussion of Flax NNX sharding, please refer to [this SPMD guide](https://flax.readthedocs.io/en/latest/guides/flax_gspmd.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "z0p-IHurrB9i"
      },
      "outputs": [],
      "source": [
        "def causal_attention_mask(seq_len):\n",
        "    \"\"\"Create a triangular mask for causal (autoregressive) attention.\n",
        "\n",
        "    This mask ensures each token can only attend to previous tokens and itself,\n",
        "    preventing the model from \"cheating\" by looking at future tokens during training.\n",
        "    \"\"\"\n",
        "    return jnp.tril(jnp.ones((seq_len, seq_len)))\n",
        "\n",
        "\n",
        "class TransformerBlock(nnx.Module):\n",
        "    \"\"\"A single Transformer block with multi-head attention and feed-forward layers.\"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, *, rngs: nnx.Rngs, rate: float = 0.1):\n",
        "        # Sharding specs for model parallelism\n",
        "        kernel_sharding = P(None, 'model')\n",
        "        bias_sharding = P('model')\n",
        "\n",
        "        # Multi-Head Attention\n",
        "        self.mha = nnx.MultiHeadAttention(\n",
        "            num_heads=num_heads,\n",
        "            in_features=embed_dim,\n",
        "            kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), kernel_sharding),\n",
        "            bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), bias_sharding),\n",
        "            rngs=rngs\n",
        "        )\n",
        "\n",
        "        # First residual path\n",
        "        self.dropout1 = nnx.Dropout(rate=rate, rngs=rngs)\n",
        "        self.layer_norm1 = nnx.LayerNorm(\n",
        "            epsilon=1e-6, num_features=embed_dim,\n",
        "            scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), bias_sharding),\n",
        "            bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), bias_sharding),\n",
        "            rngs=rngs\n",
        "        )\n",
        "\n",
        "        # Feed-forward network\n",
        "        self.linear1 = nnx.Linear(\n",
        "            in_features=embed_dim, out_features=ff_dim,\n",
        "            kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), kernel_sharding),\n",
        "            bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), bias_sharding),\n",
        "            rngs=rngs\n",
        "        )\n",
        "        self.linear2 = nnx.Linear(\n",
        "            in_features=ff_dim, out_features=embed_dim,\n",
        "            kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), kernel_sharding),\n",
        "            bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), bias_sharding),\n",
        "            rngs=rngs\n",
        "        )\n",
        "\n",
        "        # Second residual path\n",
        "        self.dropout2 = nnx.Dropout(rate=rate, rngs=rngs)\n",
        "        self.layer_norm2 = nnx.LayerNorm(\n",
        "            epsilon=1e-6, num_features=embed_dim,\n",
        "            scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), bias_sharding),\n",
        "            bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), bias_sharding),\n",
        "            rngs=rngs\n",
        "        )\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        _, seq_len, _ = inputs.shape\n",
        "        mask = causal_attention_mask(seq_len)\n",
        "\n",
        "        # Attention block\n",
        "        attn_output = self.mha(inputs_q=inputs, mask=mask, decode=False)\n",
        "        attn_output = self.dropout1(attn_output, deterministic=not training)\n",
        "        x = self.layer_norm1(inputs + attn_output)\n",
        "\n",
        "        # Feed-forward block\n",
        "        ff_output = self.linear1(x)\n",
        "        ff_output = nnx.relu(ff_output)\n",
        "        ff_output = self.linear2(ff_output)\n",
        "        ff_output = self.dropout2(ff_output, deterministic=not training)\n",
        "        return self.layer_norm2(x + ff_output)\n",
        "\n",
        "\n",
        "class TokenAndPositionEmbedding(nnx.Module):\n",
        "    \"\"\"Combines token embeddings with positional embeddings.\"\"\"\n",
        "\n",
        "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, *, rngs: nnx.Rngs):\n",
        "        self.token_emb = nnx.Embed(num_embeddings=vocab_size, features=embed_dim, rngs=rngs)\n",
        "        self.pos_emb = nnx.Embed(num_embeddings=maxlen, features=embed_dim, rngs=rngs)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        positions = jnp.arange(0, x.shape[1])[None, :]\n",
        "        return self.token_emb(x) + self.pos_emb(positions)\n",
        "\n",
        "\n",
        "class MiniGPT(nnx.Module):\n",
        "    \"\"\"A miniature GPT-style transformer language model.\"\"\"\n",
        "\n",
        "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, num_heads: int,\n",
        "                 feed_forward_dim: int, num_transformer_blocks: int, rngs: nnx.Rngs):\n",
        "        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim, rngs=rngs)\n",
        "        self.transformer_blocks = nnx.List([\n",
        "            TransformerBlock(embed_dim, num_heads, feed_forward_dim, rngs=rngs)\n",
        "            for _ in range(num_transformer_blocks)\n",
        "        ])\n",
        "        self.output_layer = nnx.Linear(\n",
        "            in_features=embed_dim, out_features=vocab_size,\n",
        "            kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), P(None, 'model')),\n",
        "            bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), P('model')),\n",
        "            rngs=rngs\n",
        "        )\n",
        "\n",
        "    def __call__(self, inputs, training: bool = False):\n",
        "        x = self.embedding_layer(inputs)\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x, training=training)\n",
        "        return self.output_layer(x)\n",
        "\n",
        "    def get_model_input(self):\n",
        "        return dict(inputs=jnp.zeros((batch_size, maxlen), dtype=jnp.int32), training=False)\n",
        "\n",
        "    @nnx.jit\n",
        "    def sample_from(self, rng_key, logits):\n",
        "        logits, indices = jax.lax.top_k(logits, k=top_k)\n",
        "        return jax.random.choice(rng_key, indices, p=nnx.softmax(logits))\n",
        "\n",
        "    @nnx.jit\n",
        "    def generate_step(self, rng_key, padded_tokens, sample_index):\n",
        "        logits = self(padded_tokens)\n",
        "        return self.sample_from(rng_key, logits[0][sample_index])\n",
        "\n",
        "    def generate_text(self, max_tokens, start_tokens):\n",
        "        generated = []\n",
        "        rng_key = jax.random.PRNGKey(0)\n",
        "        print(tokenizer.decode(start_tokens), flush=True, end='')\n",
        "\n",
        "        for i in range(max_tokens):\n",
        "            sample_index = len(start_tokens) + len(generated) - 1\n",
        "            rng_key, step_key = jax.random.split(rng_key)\n",
        "\n",
        "            current_seq = start_tokens + generated\n",
        "            padded = jnp.array(current_seq + [0] * (maxlen - len(current_seq)))[None, :]\n",
        "            next_token = int(self.generate_step(step_key, padded, sample_index))\n",
        "\n",
        "            end_token = tokenizer.encode('<|endoftext|>', allowed_special={'<|endoftext|>'})[0]\n",
        "            if next_token == end_token:\n",
        "                break\n",
        "\n",
        "            generated.append(next_token)\n",
        "            print(tokenizer.decode([next_token]), flush=True, end='')\n",
        "\n",
        "        return tokenizer.decode(start_tokens + generated)\n",
        "\n",
        "\n",
        "def create_model(rngs):\n",
        "    \"\"\"Factory function to create a MiniGPT model with configured hyperparameters.\"\"\"\n",
        "    return MiniGPT(\n",
        "        maxlen=maxlen,\n",
        "        vocab_size=vocab_size,\n",
        "        embed_dim=embed_dim,\n",
        "        num_heads=num_heads,\n",
        "        feed_forward_dim=feed_forward_dim,\n",
        "        num_transformer_blocks=num_transformer_blocks,\n",
        "        rngs=rngs\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igX_eoGNMTGR"
      },
      "source": [
        "Set some hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GRhiDsCrMZRp"
      },
      "outputs": [],
      "source": [
        "# Model architecture hyperparameters\n",
        "vocab_size = tokenizer.n_vocab              # Size of GPT-2 vocabulary\n",
        "num_transformer_blocks = 4                  # Number of transformer layers\n",
        "maxlen = 128                                # Maximum sequence length\n",
        "embed_dim = 128                             # Embedding dimension\n",
        "num_heads = 4                               # Number of attention heads\n",
        "feed_forward_dim = 256                      # Hidden dimension in feed-forward network\n",
        "\n",
        "# Training hyperparameters\n",
        "batch_size = 32                             # Batch size\n",
        "num_epochs = 1                              # Number of passes through the dataset\n",
        "top_k = 10                                  # Top-k sampling for text generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI1ci-HyMspJ"
      },
      "source": [
        "## Grain: Load and preprocess the data\n",
        "\n",
        "[Grain](https://google-grain.readthedocs.io/en/latest/) is Google's high-performance data loading library designed specifically for machine learning workloads in JAX. It provides efficient data pipelines that can keep up with the demanding throughput requirements of modern accelerators like TPUs and GPUs.\n",
        "\n",
        "**Why Grain for data loading?**\n",
        "\n",
        "- **Performance optimized**: Grain is designed from the ground up for high-throughput data loading, minimizing bottlenecks between data processing and model training\n",
        "- **Deterministic and reproducible**: Grain ensures deterministic iteration order even with shuffling and multiple workers, which is crucial for reproducible experiments\n",
        "- **JAX integration**: Seamlessly works with JAX's device parallelism, automatically handling data sharding across multiple devices\n",
        "- **Flexible transformations**: Provides composable operations for batching, shuffling, and preprocessing data\n",
        "- **Multi-epoch support**: Built-in support for iterating over datasets multiple times without manual reinitialization\n",
        "\n",
        "**Key Grain concepts:**\n",
        "\n",
        "In this tutorial, we use three main Grain components:\n",
        "\n",
        "1. **Data source**: A custom `TextDataset` class that tokenizes and pads text sequences to a fixed length\n",
        "2. **Sampler**: An `IndexSampler` that determines the order in which examples are accessed from the data source\n",
        "3. **DataLoader**: Combines the data source, sampler, and operations (like batching) into an efficient pipeline\n",
        "\n",
        "Grain handles the complexity of data loading, allowing us to focus on model development while ensuring our TPUs stay fully utilized during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rGUFsn1GMuzh"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class TextDataset:\n",
        "    \"\"\"A simple dataset for tokenized text sequences.\"\"\"\n",
        "    data: list\n",
        "    maxlen: int\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        # Tokenize and truncate\n",
        "        encoding = tokenizer.encode(self.data[idx], allowed_special={'<|endoftext|>'})[:self.maxlen]\n",
        "        # Pad to maxlen\n",
        "        return encoding + [0] * (self.maxlen - len(encoding))\n",
        "\n",
        "\n",
        "def load_and_preprocess_data(file_path, batch_size, maxlen):\n",
        "    \"\"\"Load the TinyStories dataset and create a Grain DataLoader.\"\"\"\n",
        "\n",
        "    # Read the file\n",
        "    with open(file_path, 'r') as f:\n",
        "        text = f.read(1 << 29)\n",
        "\n",
        "    # Split into stories\n",
        "    stories = text.split('<|endoftext|>')\n",
        "    stories = [story + '<|endoftext|>' for story in stories if story.strip()]\n",
        "    print(f\"Loaded {len(stories):,} stories\")\n",
        "\n",
        "    dataset = TextDataset(stories, maxlen)\n",
        "\n",
        "    sampler = pygrain.IndexSampler(\n",
        "        len(dataset),\n",
        "        shuffle=False,\n",
        "        seed=42,\n",
        "        shard_options=pygrain.NoSharding(),\n",
        "        num_epochs=num_epochs,\n",
        "    )\n",
        "\n",
        "    dl = pygrain.DataLoader(\n",
        "        data_source=dataset,\n",
        "        sampler=sampler,\n",
        "        operations=[pygrain.Batch(batch_size=batch_size, drop_remainder=True)],\n",
        "    )\n",
        "\n",
        "    return dl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "cellView": "form",
        "id": "b9tmfMzj7eso"
      },
      "outputs": [],
      "source": [
        "# @title [hidden cell; used for testing]\n",
        "# This cell is run only in the JAX AI Stack's CI testing and should otherwise be ignored.\n",
        "import os\n",
        "AI_STACK_TEST_MODE = os.getenv('AI_STACK_TEST_MODE') == 'true'\n",
        "\n",
        "if AI_STACK_TEST_MODE:\n",
        "    num_transformer_blocks = 2\n",
        "    maxlen = 16\n",
        "    embed_dim = 16\n",
        "    num_heads = 2\n",
        "    feed_forward_dim = 8\n",
        "\n",
        "    def load_and_preprocess_data(file_path, batch_size, maxlen):\n",
        "        del file_path\n",
        "\n",
        "        @dataclass\n",
        "        class TestTextDataset:\n",
        "            maxlen: int\n",
        "\n",
        "            def __len__(self):\n",
        "                return 64\n",
        "\n",
        "            def __getitem__(self, idx: int):\n",
        "                encoding = jax.random.randint(jax.random.key(idx), [self.maxlen], minval=0, maxval=1e6)\n",
        "                return jnp.unstack(encoding)\n",
        "\n",
        "        dataset = TestTextDataset(maxlen)\n",
        "\n",
        "        sampler = pygrain.IndexSampler(\n",
        "            len(dataset),\n",
        "            shuffle=False,\n",
        "            seed=42,\n",
        "            shard_options=pygrain.NoSharding(),\n",
        "            num_epochs=num_epochs,\n",
        "        )\n",
        "\n",
        "        dl = pygrain.DataLoader(\n",
        "            data_source=dataset,\n",
        "            sampler=sampler,\n",
        "            operations=[pygrain.Batch(batch_size=batch_size, drop_remainder=True)],\n",
        "        )\n",
        "\n",
        "        return dl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnfW4Z7l7eso",
        "outputId": "1ce0a9e8-31a6-4ba6-f255-a2e24ec0538f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading TinyStories dataset with batch_size=32, maxlen=128...\n",
            "Loaded 589,692 stories\n",
            "Dataset loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "# Create the data loader\n",
        "# This will load, tokenize, and batch the TinyStories dataset\n",
        "print(f\"Loading TinyStories dataset with batch_size={int(batch_size)}, maxlen={maxlen}...\")\n",
        "text_dl = load_and_preprocess_data('TinyStories-train.txt', int(batch_size), maxlen)\n",
        "print(\"Dataset loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKVSD8KSM1um"
      },
      "source": [
        "## Defining the loss function and training step function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8rRuTmABNV4b"
      },
      "outputs": [],
      "source": [
        "def loss_fn(model, batch):\n",
        "    \"\"\"Compute the cross-entropy loss for language modeling.\n",
        "\n",
        "    Args:\n",
        "        model: The MiniGPT model\n",
        "        batch: A tuple of (input_tokens, target_tokens)\n",
        "\n",
        "    Returns:\n",
        "        A tuple of (loss, logits) where:\n",
        "        - loss: Scalar cross-entropy loss\n",
        "        - logits: Model predictions (returned as auxiliary output for metrics)\n",
        "    \"\"\"\n",
        "    input_tokens, target_tokens = batch\n",
        "\n",
        "    # Forward pass: get logits for each position\n",
        "    logits = model(input_tokens)\n",
        "\n",
        "    # Compute cross-entropy loss between predictions and targets\n",
        "    # This measures how well the model predicts the next token\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
        "        logits=logits,\n",
        "        labels=target_tokens\n",
        "    ).mean()\n",
        "\n",
        "    return loss, logits\n",
        "\n",
        "\n",
        "@nnx.jit  # JIT-compile for performance (runs once, then cached)\n",
        "def train_step(model: MiniGPT, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
        "    \"\"\"Perform a single training step.\n",
        "\n",
        "    This function:\n",
        "    1. Computes the loss and gradients\n",
        "    2. Updates model parameters using the optimizer\n",
        "    3. Updates metrics for tracking\n",
        "\n",
        "    Args:\n",
        "        model: The MiniGPT model to train\n",
        "        optimizer: The Optax optimizer (wrapped in nnx.Optimizer)\n",
        "        metrics: Metrics tracker for monitoring training\n",
        "        batch: A tuple of (input_tokens, target_tokens)\n",
        "    \"\"\"\n",
        "    # Create a function that computes both loss value and gradients\n",
        "    # has_aux=True means loss_fn returns (loss, auxiliary_data)\n",
        "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
        "\n",
        "    # Compute loss, logits, and gradients in one go\n",
        "    (loss, logits), grads = grad_fn(model, batch)\n",
        "\n",
        "    # Update metrics for tracking\n",
        "    metrics.update(loss=loss, logits=logits, labels=batch[1])\n",
        "\n",
        "    # Update model parameters using the computed gradients\n",
        "    # The optimizer applies the Adam update rule\n",
        "    optimizer.update(model, grads)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5um2vkeUNckm"
      },
      "source": [
        "## Optax: Train the model\n",
        "\n",
        "[Optax](https://optax.readthedocs.io) is a gradient processing and optimization library for JAX. It provides composable components for building custom optimizers and a collection of popular optimization algorithms used in deep learning.\n",
        "\n",
        "**Why Optax?**\n",
        "\n",
        "- **Modular design**: Optax uses a functional, composable approach where optimizers are built by chaining together simple transformations\n",
        "- **Rich optimizer collection**: Includes popular optimizers like Adam, SGD, AdamW, and many more advanced variants\n",
        "- **Gradient transformations**: Provides utilities for gradient clipping, normalization, and other preprocessing steps\n",
        "- **JAX compatibility**: Designed specifically for JAX, working seamlessly with `jax.grad()` and other transformations\n",
        "- **Stateless and functional**: Optimizer state is explicitly managed, making it easy to understand and debug\n",
        "\n",
        "**Key Optax concepts:**\n",
        "\n",
        "In this tutorial, we use:\n",
        "\n",
        "1. **Loss function**: We define a `loss_fn` that computes cross-entropy loss using `optax.softmax_cross_entropy_with_integer_labels`\n",
        "2. **Optimizer**: We use `optax.adam(1e-3)`, the Adam optimizer with a learning rate of 0.001\n",
        "3. **NNX Optimizer wrapper**: `nnx.Optimizer` integrates Optax optimizers with Flax NNX models, handling parameter updates automatically\n",
        "\n",
        "The training loop combines these components: we compute gradients with `nnx.value_and_grad`, then use the optimizer to update model parameters based on those gradients.\n",
        "\n",
        "**Training details:**\n",
        "\n",
        "Start training. It takes approximately 20 minutes on Colab TPU v5e-1.\n",
        "\n",
        "Note that for data parallel, we are sharding the training data along the `batch` axis using `jax.device_put`.\n",
        "\n",
        "We are also using the `jax.vmap` transformation to produce the target sequences faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ysl6CsfENeJN",
        "outputId": "442ae785-be4d-4fe6-99a2-05615c976d80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial generated text (before training):\n",
            "Once upon a time Wired Donkey genometionsRing Wired surprises45beyaf partner Mazda inquired resonate oversized killsbeybeybey Process Kimmelbey Lance urgently choosesardkbRing circaincluding Kimmel waved45odondiff156 litres encrypt arrests inquired Detect preschooloi Detect316 Silenceallas Pyrrha Wiredallah McCitemsallah wh ExperbeyChristmas urgentlyallah floatstions ridiculouslyvag litres tradingincludingbey45 nontoted lovedbeyitems lied Silence bald nause Topic postp acknowledgment atrocensor brut dog strictbey Ingramallahentimes Process leaps arbitrationinen loved arrestsantage decisivekb floatsparticularlybey nont strict atroc atroc lied Silence Toadvag Kimmeltions arrests gh sparks Converted156 wors Albion spying complaints SwordsDoctor Millennials encrypt Shar!!!\n",
            "\n",
            "\n",
            "Step 200, Loss: 6.0531, Elapsed Time: 22.34s\n",
            "Generated text:\n",
            "Once upon a time!. a named she the a boy named Tim her big big little a girl.\n",
            "The little boy. The friends to said the a big a little girl his big big friends to the little little girl.\n",
            "Suddenly to her and her the little a a big friends and the a big the mom and her play to play.\n",
            "One day to it.!\n",
            "The a very big, \"But the mom the park. They the big friends. They said to the park with the mom was a little big mom the park on a big. He had was very very play on and's friends. He and his!!!!\n",
            "\n",
            "\n",
            "Step 400, Loss: 4.0967, Elapsed Time: 6.25s\n",
            "Generated text:\n",
            "Once upon a time was a there who wanted to play, and his friend. She would play outside to the big. He saw a special and ran back for a big, he got very special and a big.\n",
            "The little girl went to the park and said, \"Let's help?\"\n",
            "Mummy is so excited and saw a big. They saw a new at the boy and the park and Ben was very scared. It is a big and said, I have to the ball. He looked up to go to the park.\n",
            "\"That, you to play,\" Lily's go up on the house and said, please you!!!!\n",
            "\n",
            "\n",
            "Step 600, Loss: 3.6144, Elapsed Time: 5.10s\n",
            "Generated text:\n",
            "Once upon a time the sun upon was a little girl called Tim was three years old bear. Lily loved to play with her friends. He loved her friend Timmy, she had the world around.\n",
            "One day, Lily's mom told her. It was so happy and Lily. The girl was playing with her friends. Lily.\n",
            "As they were playing, Tim and she would make a little girl who had a walk in her. She was sad. She had a while she could play with the little girl and her mom.\n",
            "After a walk. She was happy because her mommy went over her to play with her dad felt!!!!\n",
            "\n",
            "\n",
            "Step 800, Loss: 3.3899, Elapsed Time: 5.20s\n",
            "Generated text:\n",
            "Once upon a time in a brave time. It was so brave. They had a house and he wanted to play with a big tree.\n",
            "So one of the bird was so high. He said he wanted to go to the park with him. He asked his mom.\n",
            "The dog said, ��Let me do it's go!\" The bird said, it and the day, they did not go home. But the sun. They were happy and they were so proud of their friends. The bird and it was very sad.\n",
            "The other friends and they saw some fun. They ran on the ball. They wanted to look!!!!\n",
            "\n",
            "\n",
            "Step 1000, Loss: 3.1378, Elapsed Time: 5.29s\n",
            "Generated text:\n",
            "Once upon a time.\n",
            " he decided, so he wanted to explore a special place that he wanted to go. He would play all day. But, so he could his friends to play in the woods.\n",
            "The other animals were excited to explore and his friends. But he was a little girl came to the forest. It was so he wanted to play.  He said yes and he started to eat him. Then he had never seen a special place. \n",
            "So the end of the birds and his mom said, \"You can't help me!\"\n",
            "So that they were so proud. The animals had a new idea to!!!!\n",
            "\n",
            "\n",
            "Step 1200, Loss: 3.0387, Elapsed Time: 5.23s\n",
            "Generated text:\n",
            "Once upon a time and Jack liked the store to go down and explore his toy car and play. He had a big toy car and big smile that could not find them to make noises and share them to do. He liked to make it feel very happy to be fun with the ball, but he always have fun to make the way to the car.\n",
            "But when a big birdie came and said hello. He looked at Tim and saw a man who had a big dog and Tom. He asked his friend if he could be careful and not to share his friends. Sam smiled and said no more more, \"Thank you and,!!!!\n",
            "\n",
            "\n",
            "Step 1400, Loss: 2.9564, Elapsed Time: 5.42s\n",
            "Generated text:\n",
            "Once upon a time one day there had been playing with the ball in the garden.\n",
            "The family were very excited to find a new home and when the sun was a little boy and his family was a very excited. He was so sad and asked if he could help his friends.\n",
            "The boy went to the boy and the boy went to the park. He had found a special idea. He looked all his friends all the things and they were like their friends.\n",
            "When they got to the race, the little boy had to go on the ball. He was very impressed in the park with lots of toys and they played with him!!!!\n",
            "\n",
            "\n",
            "Step 1600, Loss: 2.8896, Elapsed Time: 5.17s\n",
            "Generated text:\n",
            "Once upon a time and his dad would make the race with the game in the garden and the family would go to the town. They saw many new kids to the animals and the beach and had fun.\n",
            "One day, they decided they decided to take their home and go for a walk in the animals. They saw a beautiful pond and wanted to go. They asked they if it was time to go inside the park and play a game.\n",
            "They said, \"Let's go down the hill.\"\n",
            "They looked around and saw a little girl named Sarah. She asked if she could go inside it, so they decided to get back!!!!\n",
            "\n",
            "\n",
            "Step 1800, Loss: 2.7661, Elapsed Time: 5.19s\n",
            "Generated text:\n",
            "Once upon a time the girl who looked up in her backyard. She found a big pile of bright green grass. She thought it was cool to take out her house and make it safe. She ran inside and looked around the room. The sun shone and looked around, and found a beautiful sun. \n",
            "The little girl felt very happy and she hugged her eyes. Suddenly, something amazing happened. She took out the ground and ran back to the sky. She saw a bright yellow and had never seen before.\n",
            "The next day, she found an old man who had come out of the street! The girl smiled and smiled back until!!!!\n",
            "\n",
            "\n",
            "Step 2000, Loss: 2.7402, Elapsed Time: 5.03s\n",
            "Generated text:\n",
            "Once upon a time and his mom wanted to play a game. So the boy went on a sunny day and he went to the park. Suddenly he spotted an old toy. He had an idea and decided to go to the store. The little boy asked the boy, \"Why don't you help? I'm looking for help.\"\n",
            "The boy said, \"It will be careful and wait for me and have a picnic.\"\n",
            "The boy was very excited. He said, \"Thank you, let's take your toy!\"\n",
            "The boy said his mom, \"Okay you like it.\" He ran and took a big bite the sandwich!!!!\n",
            "\n",
            "\n",
            "Step 2200, Loss: 2.6682, Elapsed Time: 5.26s\n",
            "Generated text:\n",
            "Once upon a time and his dad drove the town in the ocean. Dad had a race in a boat. Dad was so excited and started to walk through the ocean.\n",
            "The family saw lots of things, but the sea was very hot. Bob and Dad were very scared and wanted to go on one. They ran to the beach and ran to the boat. They found a little fish swimming and some sticks, and there was lots of fish. They were so happy and the boat sailed around. They were all so happy and enjoyed the sea. The water was so excited for a while, he couldn't go home. \n",
            "At!!!!\n",
            "\n",
            "\n",
            "Step 2400, Loss: 2.6272, Elapsed Time: 5.10s\n",
            "Generated text:\n",
            "Once upon a time but he were brave boy. One day, the sun was shining through the forest. He was very scared and he had never hurt him! He didn't know, but he wanted to be scared. He tried to be able to escape.\n",
            "Suddenly, a little boy came to visit. He saw the boy's little boy and said, \"Tim,\" Tim and said, \"What��s the little boy?\"\n",
            "Tim and his mom smiled and said, \"Don't worry, Tim. I'll give you the little dog. I'm going first now.\"\n",
            "Tim thought about his way home and they asked!!!!\n",
            "\n",
            "\n",
            "Step 2600, Loss: 2.6160, Elapsed Time: 5.17s\n",
            "Generated text:\n",
            "Once upon a time a boy called Joe liked to play outside. He was a very curious little 3-year-old, but he liked to help her family. One day, Toby's mom asked him to go on a big adventure to the lake. The water was too big and warm.\n",
            "When they got there, Joe and the lake was very big and bright to see a big hill. It said to Joe to the water and they both put on the water. Joe and Peter put the rope on a rope and the raft. They were too happy and they had no idea.\n",
            "He went back to the lake and ran inside!!!!\n",
            "\n",
            "\n",
            "Step 2800, Loss: 2.5682, Elapsed Time: 5.20s\n",
            "Generated text:\n",
            "Once upon a time in there were many animals in the woods. They lived in a cozy cave with a big, dark cave. They saw a huge hole a big, scary bear's foot.\n",
            "One day, a little girl came in the woods. She saw the bear and said, \"Hello little bird, bird. Do you want to help me?\" The bear laughed and smiled.\n",
            "The little bunny felt sad because she didn't know what to do.\n",
            "The next day, the bird came to find out what it was inside. The wise owl said, \"If you want to share, you help me find my nest together!!!!\n",
            "\n",
            "\n",
            "Step 3000, Loss: 2.3932, Elapsed Time: 5.26s\n",
            "Generated text:\n",
            "Once upon a time. One day the store to play in the park, the park with a lot. The park was so big and could jump up to catch. \n",
            "The big and the wind started to move. The sun was warm and the birds were happy. \n",
            "The little girl and Lily were happy to see the sun. They were so happy. They hugged each other and said, \"Mom, I can't play with you.\"\n",
            "Lily and her friends played in the park, but then the rain started to rain. They felt much better and happy and the rain. Lily saw that her friend was happy that their!!!!\n",
            "\n",
            "\n",
            "Step 3200, Loss: 2.5339, Elapsed Time: 5.45s\n",
            "Generated text:\n",
            "Once upon a time a bee and he lived in a cozy forest. The hive was flying around, and he saw a beautiful bird. He loved looking all by the world and he would sing it around him. He would sing and sing his song, twinkling in the wind. \n",
            "One night, the bee saw something in the sky. He was so happy with his own feathers and the bird sang on the trees. The bird flew around and around the tree. Then, the bird flew over to the nest and landed in the air. Suddenly, the bird felt dizzy and the sun was warm in the sky. It saw!!!!\n",
            "\n",
            "\n",
            "Step 3400, Loss: 2.3519, Elapsed Time: 5.21s\n",
            "Generated text:\n",
            "Once upon a time. In there, a little girl named Lily. She loved to dance and her favorite song. One day, she found a new bed and wanted to draw on something. It was very pretty.\n",
            "Lily wanted to show her friends her to her. She asked her friends to have a party. They played on the rug every day. They were excited to be happy and had to learn the best song.\n",
            "When they got to the park, Lily's mom gave her a kiss. It was a big bag of ice cream from her. Lily loved his favorite ice cream cone too. Her mom said it had some!!!!\n",
            "\n",
            "\n",
            "Step 3600, Loss: 2.4056, Elapsed Time: 5.60s\n",
            "Generated text:\n",
            "Once upon a time. They both both wanted to have fun, but each other. Mom said that they had to go for a walk in the woods. She said, \"Let me see if you go on a big rock. I am here to stay inside.\"\n",
            "\"OK, I want to try,\" said the little boy. \"But I want to stay there! I am the same place. Maybe the little boy will find the way to stay.\"\n",
            "\"No, no! I will stay here and find the little boy,\" said Mom for the little boy. His mom smiled and gave the little boy some food and gave it!!!!\n",
            "\n",
            "\n",
            "Step 3800, Loss: 2.3333, Elapsed Time: 5.27s\n",
            "Generated text:\n",
            "Once upon a time on a tall river, but there was a big log. The lion was so happy that he wanted to find out all day to go up. One day, something amazing happened. He saw a big, shiny rock on the ground and started to move it around. He wanted to make it up close, but it was too high up.\n",
            "Just then, he heard a voice. ��Don��t worry, I'll help you. I'll help you.��\n",
            "The hunter said. He was very happy with his new home. It made a big voice up.\n",
            "The lion thanked him,!!!!\n",
            "\n",
            "\n",
            "Step 4000, Loss: 2.4324, Elapsed Time: 5.33s\n",
            "Generated text:\n",
            "Once upon a time on a small dog. The dog loved to run. The dog always liked to play with the dog, the dog and the other. \n",
            "One day, they decided to play with the dog. The dog was scared of the dog, but the dog did not know how to chase the dog. The dog and the dog became friends. \n",
            "A little girl came to the dog, said, \"Hi! Do you want to play with me?\" \n",
            "The dog barked and the dog barked happily. The dog jumped out of the house with a smile. They went to the park and found the boy!!!!\n",
            "\n",
            "\n",
            "Step 4200, Loss: 2.2829, Elapsed Time: 5.01s\n",
            "Generated text:\n",
            "Once upon a time. A girl and she loved to do new things. One day her mom made her a new promise to her mom, so the girl went to the store. As she walked, the girl saw a man sitting on the ground.\n",
            "The man asked the man, \"Why do I have a sign?\" The man replied, \"I want to be rich.\n",
            "The man said, \"Yes, but be careful, I have a sign that said you should not give me something?\"\n",
            "The girl was so excited. She said to the man, \"Let's open it. Do you know how to make you promise!!!!\n",
            "\n",
            "\n",
            "Step 4400, Loss: 2.3588, Elapsed Time: 5.31s\n",
            "Generated text:\n",
            "Once upon a time and, there were two friends. One was very important. They liked to share their toys and make new friends. The sun had bright blue and it was so happy. They wanted to play with the sun.\n",
            "They decided to play in the park and the park. They had to go to the park and have fun. They had so much fun to play the park. But they were not sure it was important.\n",
            "They were very sad. They did not like the other other kids anymore. They had a great time. So they decided to have their best friends. The kids said goodbye to the kids, who!!!!\n",
            "\n",
            "\n",
            "Step 4600, Loss: 2.2506, Elapsed Time: 5.21s\n",
            "Generated text:\n",
            "Once upon a time on the day there lived a little boy named Tom. Tom loved to play in the garden with his favorite toy.\n",
            "One day, a little girl came to help Tom. She put the toy in the garden and found a little bird with a big red apple. She picked it up and put it in the dirt. Tom was very happy and said that he wanted to play with it for a little fun day.\n",
            "Tom was so happy to see the big tree that he could get it up to the tree. He went to get it and saw the branch of the tree and found a tree. He thought the view!!!!\n",
            "\n",
            "\n",
            "Step 4800, Loss: 2.2329, Elapsed Time: 5.31s\n",
            "Generated text:\n",
            "Once upon a time a tall bear went out for a long walk in the woods. It was so tall that it was almost up in a tall, strong field. It was so bright and sparkly and sparkly. \n",
            "One day, a brave rabbit came and said \"Hello!\" The rabbit didn't know how to go, so it hopped away. \n",
            "Finally, it hopped up to the edge of the hill and the rabbit hopped away. The rabbit was so relieved to find its way around the hill and the rabbit couldn't help but come out of the forest for miles. \n",
            "After the little bit, the bear thanked!!!!\n",
            "\n",
            "\n",
            "Step 5000, Loss: 2.2691, Elapsed Time: 5.16s\n",
            "Generated text:\n",
            "Once upon a time. The people liked to play together, and other things was always. One day, the other day was playing in the forest near her garden. Suddenly, the stars flew and over to the trees. The sun was bright and warm and beautiful.\n",
            "The stars started to grow. All of the birds sang, and the stars sang. The sun clap and watched as the stars began the clouds began. The sky was so happy and the stars started to sing. The sun shone so brightly. The stars danced and the sun sang as happy.\n",
            "The people clapped and enjoyed having fun! The sun had made from!!!!\n",
            "\n",
            "\n",
            "Step 5200, Loss: 2.2129, Elapsed Time: 5.21s\n",
            "Generated text:\n",
            "Once upon a time a small rabbit hopped around in a cozy nest. He lived in a big nest in a tree, so he flew out to see his friend, a bird named Kitty.\n",
            "\"Hello, Kitty! Can I help me find some yummy food?\" Lily asked.\n",
            "\"Sure!\" Kitty said, holding her hand.\n",
            "They walked to the nest of a bird and sat down to watch the nest. The bird said, \"Hello, Kitty. Do you want to help me, please?\"\n",
            "Kitty nodded and flew to Sue's nest.\n",
            "Sue was very happy, and helped them fly away. But!!!!\n",
            "\n",
            "\n",
            "Step 5400, Loss: 2.1684, Elapsed Time: 5.14s\n",
            "Generated text:\n",
            "Once upon a time one sunny time he went to the store. He loved going to a restaurant every day. He was so excited to help and so his little boy, he saw his little brother. \n",
            "The little boy was so surprised! He had a big smile on his face and he asked why he was too scared. \n",
            "The little boy said he didn't know where he was. His parents were so worried. His parents told him it was ok to be more careful and not to go. \n",
            "The little boy was very happy too. He said goodbye with the little boy, happy to help. He went on some!!!!\n",
            "\n",
            "\n",
            "Step 5600, Loss: 2.2058, Elapsed Time: 5.21s\n",
            "Generated text:\n",
            "Once upon a time on there were many people who liked to play together. One day the family wanted to go to the store to buy something new and she couldn't wait to buy them. The mom asked the storekeeper to buy some snacks, but the shopkeeper said it would be nice to the people. The shopkeeper and the shopkeeper bought the money and he knew it had to buy the grocery, so he took the grocery. The store keeper was very happy and went home and bought the toy.\n",
            "When the storekeeper was all the little children were very nice looking. The little one thing he wanted to buy it all of!!!!\n",
            "\n",
            "\n",
            "Step 5800, Loss: 2.2442, Elapsed Time: 5.26s\n",
            "Generated text:\n",
            "Once upon a time.\n",
            "Once were a little boy and his family had a big smile. Every day, the family would go to the store in their car. One day, Mom asked the little boy, \"What do you want?\". The little boy was so excited! He wanted to buy a tasty treat. \n",
            "His mom said, \"Let's go outside together!\" So they went to the kind boy and he went on a picnic. \n",
            "When the boy saw the boy's mom, the boy, the boy and the boy went down the street. He had lots of fun exploring!\n",
            "After they were there, Tim!!!!\n",
            "\n",
            "\n",
            "Step 6000, Loss: 2.1729, Elapsed Time: 5.20s\n",
            "Generated text:\n",
            "Once upon a time. Mom had made the car so big and was ready for the fun. When it was time to go to the airport. He got a big umbrella. He got on the plane. He was so excited! He knew it would be a good place.\n",
            "The car drove away! He drove and drove in the car. He drove around the airport to the park. He was so surprised, he had never been before.\n",
            "The airport was so excited to go outside. He had been looking around and he couldn't see anything to be able to play and have fun. \n",
            "But he didn't notice. This!!!!\n",
            "\n",
            "\n",
            "Step 6200, Loss: 2.1210, Elapsed Time: 5.37s\n",
            "Generated text:\n",
            "Once upon a time on the sun shining a big tree in the sun. They were always so pretty. They had a picnic in the shade and then the birds were in the sky when it came down!\n",
            "The birds flew down and found a small tree. They were so excited! They all wanted a picnic. The tree looked at the tree and said, \"I love to the tree.\"\n",
            "They took the tree with the tree. They had fun. They were having a picnic and had a lot of picnic. They were so happy.\n",
            "As they were playing in the sun, they laughed, happy, and they were playing and!!!!\n",
            "\n",
            "\n",
            "Step 6400, Loss: 2.1174, Elapsed Time: 5.13s\n",
            "Generated text:\n",
            "Once upon a time and his mother lived there was a big nation. One day, when they were playing in the rain. The nation got stuck into an ugly shelter. The family started to get frustrated because they were all alone.\n",
            "But then something was happening. The little 3 year old was very angry. The child said to the little child, ��It��s so hard that it can move the way,��\n",
            "The little boy was so sad. He had no idea. He said, ��Let��s have more fuel.�� He just smiled and went out. The family was very angry because!!!!\n",
            "\n",
            "\n",
            "Step 6600, Loss: 2.1141, Elapsed Time: 5.25s\n",
            "Generated text:\n",
            "Once upon a time a boy and Jack wanted to play. He put his favorite toy cars on and went to the park. He asked, \"Are you sleepy, Jack?\"\n",
            "A big dog, a little boy named Max, said, \"No, Max, we are too small and strong.\" Tim said, \"But you want to play with me.\"\n",
            "Tom took the new toy and went to see the big, old man. He saw that the old man was a man selling apples. \"Max, this is a man who are rich for a long time!\" the man said, holding up the blue ones. \"Wow,!!!!\n",
            "\n",
            "\n",
            "Step 6800, Loss: 2.1658, Elapsed Time: 5.32s\n",
            "Generated text:\n",
            "Once upon a time there were best friend who liked to share their friends. They always loved to share and have fun. One day, the friend found an empty box of candy. His friend was sad and didn't know what to do.\n",
            "So, the friend and his friends went to the store together. They brought the candy to the store. The store was so nice and made the store happy. When it was gone, they saw a big ball. The ball was happy to see the ball again. The ball was happy to share with the toys.\n",
            "\n",
            "\n",
            "\n",
            "Step 7000, Loss: 2.1754, Elapsed Time: 5.03s\n",
            "Generated text:\n",
            "Once upon a time a boy and Jack are very excited for a picnic to celebrate. But Jack always have a picnic and Jack was feeling hungry and went out into the cooler.\n",
            "On their picnic, Jack saw a big tree with lots of yummy fruits. He asked if he could go, but his mom said yes.\n",
            "Jack and Jack went to the tree to the tree with a big smile on.\n",
            "When Jack got to the tree, he saw a big tree and started to climb up it. The tree was so big and tall and had lots of leaves! But the tree was not feeling bad because it was just too late!!!!\n",
            "\n",
            "\n",
            "Step 7200, Loss: 2.1066, Elapsed Time: 5.15s\n",
            "Generated text:\n",
            "Once upon a time there was an adorable boy. He wanted to do lots of work all day, so he decided to organize his work.\n",
            "First he bought a new box. There was a big box full of toys and books. Then he decided to go to the park with his new toy. As he walked through the park, he saw a big slide. It looked like he was having fun! The boy wanted to go up to it. He asked his mom if he could take the top and she could play with it.\n",
            "So he sat down on the slide. He looked around in wonder. But when he reached it he!!!!\n",
            "\n",
            "\n",
            "Step 7400, Loss: 2.1370, Elapsed Time: 5.25s\n",
            "Generated text:\n",
            "Once upon a time there lived two two best friends, Max and Jack. They were all playing in the park and having fun. \n",
            "One sunny day, Max and Jack decided, decided to play hide and seek. They both had lots of fun together and won the way to hide behind them. The two friends laughed and ran back home. \n",
            "When they woke up, the two friends noticed a shiny box. It was the perfect gift from their mom and told them about a puzzle. They had a very special puzzle. \n",
            "After they got the puzzle, they took it to the box, but when they were all sad to!!!!\n",
            "\n",
            "\n",
            "Step 7600, Loss: 2.1750, Elapsed Time: 5.36s\n",
            "Generated text:\n",
            "Once upon a time in his morning to a special morning morning. Mom was preparing a nice party to celebrate. The family were so excited that they were excited!\n",
            "The family had come and their birthday! They packed their presents and went outside. Mom said, \"We are going to make you a very special party!\"\n",
            "When they arrived at the party, Mom said yes.\n",
            "The family packed some bags with snacks. Mom said, \"We have to celebrate!\" Mom said, \"We will serve you some of our picnic!\"\n",
            "When they made dinner, Mom said that was the best friend ever. Dad and Dad were in their!!!!\n",
            "\n",
            "\n",
            "Step 7800, Loss: 2.0963, Elapsed Time: 5.31s\n",
            "Generated text:\n",
            "Once upon a time a big fat monster who wanted to play. He wanted to make something special for himself. He went to the park with his family to get a toy car. As he drove around the park, he saw a big hill. He thought he would be a great idea and would try. He ran around the hill and got his car. He put his little car out of the top and got stuck in it. \n",
            "The monster said, \"Oh no! That's not a toy! It's not a good friend. I need you to get your toys, but it looks nice. Can I have the toys and!!!!\n",
            "\n",
            "\n",
            "Step 8000, Loss: 2.1016, Elapsed Time: 5.41s\n",
            "Generated text:\n",
            "Once upon a time there were two rabbits and birds. The birds had big feathers and wings were buzzing around the trees.\n",
            "One day a group of squirrel flew by the trees and landed in a nearby field. The squirrel was so excited.\n",
            "The two friends decided to climb the tree. They climbed on a branch, and flew high in the sky. They had a very long long, fluffy cloud and green trees. \n",
            "The rabbits were so excited! They jumped up and laughed. They laughed, clapping their wings and flew around the tree as the birds flew away with their feathers.\n",
            "Finally, they found a beautiful lake in!!!!\n",
            "\n",
            "\n",
            "Step 8200, Loss: 2.0866, Elapsed Time: 5.22s\n",
            "Generated text:\n",
            "Once upon a time there lived a rich little boy named Timmy. Timmy loved playing outside outside and he had a great time playing. He liked to dig and find new friends around in the forest.\n",
            "One day, Timmy was playing in the woods when he heard a loud noise. He saw a big bear and a rabbit! Timmy was so happy that the rabbit started barking and the bear had lots of fun.\n",
            "After playing, Timmy's mom came in and said, \"Don't worry Timmy, we'll play hide!\" Timmy was very sad. They started playing on the path, but then the next!!!!\n",
            "\n",
            "\n",
            "Step 8400, Loss: 2.2073, Elapsed Time: 5.33s\n",
            "Generated text:\n",
            "Once upon a time a small bunny called Max. He lived in a small meadow that was soft and cozy. One day, Max decided to have an important day. He hopped off the ground and found a big, shiny diamond. The diamond was very special and Max wanted to show him.\n",
            "The diamond was very happy. It took Max to the park and he hopped on it. Max saw the diamond, but also a small boy. Max had a big, mean dog. Max was scared of the dog and didn't know what to do. He wanted to be able to get it, so Max said he could be kind of!!!!\n",
            "\n",
            "\n",
            "Step 8600, Loss: 2.1149, Elapsed Time: 5.11s\n",
            "Generated text:\n",
            "Once upon a time there lived two rabbits, a squirrel. One lived near a big jungle.\n",
            "One day, the squirrel was looking for food in a place. He saw a tall tree with a tall tree. He wanted to climb on the tree. So he climbed down the tree and climbed up and down the branches. He looked around and saw a big, green leaves. \n",
            "The squirrel smiled too and said to the tree, \"I will help you.\" The rabbits agreed to help him, and they took the leaves. They were happy that the tree was the whole, but they would take a look at the leaves together,!!!!\n",
            "\n",
            "\n",
            "Step 8800, Loss: 2.1774, Elapsed Time: 5.18s\n",
            "Generated text:\n",
            "Once upon a time there lived a small rabbit. The rabbit was always hungry. One day, a little rabbit came to the rabbit and wanted to take the bunny. The rabbit hopped over and saw a bird. The rabbit was so excited, she ran away, but the rabbit hopped away.\n",
            "The bird was so tired that she wanted to take a nap. The rabbit looked around to find food. Suddenly he found a new spot and he saw a little bird. The bird had an idea. He took a long walk and brought the bunny to the tree. The bird flew to the tree and looked all around, but he could feel happy!!!!\n",
            "\n",
            "\n",
            "Step 9000, Loss: 2.1707, Elapsed Time: 5.12s\n",
            "Generated text:\n",
            "Once upon a time a big lion wanted to help people in a very intelligent, but his friends knew that he had to help him. Every day the day the lion and the other started to build a fort. \n",
            "One day, they found a small, shiny brick in the garden. The brick wanted to build it, but it was too small to build a house and it would make the house. Then the lion heard a loud noise and he got to work. \n",
            "He was so excited he could hardly believe his eyes. He quickly grabbed out the brick and put down on his new hammer! The little bird was so pleased he!!!!\n",
            "\n",
            "\n",
            "Step 9200, Loss: 2.0203, Elapsed Time: 5.12s\n",
            "Generated text:\n",
            "Once upon a time there lived two two little boy. Timmy loved to explore and learn new things. One day, he decided to have an adventurous adventure! He wanted to explore it and see what he could find. He walked and he saw a little mouse in the forest. The mouse was scared at first, but Timmy didn't know what to do. \n",
            "Suddenly, a big bird appeared running towards him! Timmy was scared and asked, \"What's wrong?\" The bird said, \"My bird lost its home and it's gone! I lost and couldn't find it to play with you. I'm sorry.!!!!\n",
            "\n",
            "\n",
            "Step 9400, Loss: 2.0419, Elapsed Time: 5.05s\n",
            "Generated text:\n",
            "Once upon a time there lived in the east. Every morning the west went for a drive. On the way, the sun was shining. All around in the forest were a beautiful bird who was very friendly. The bird was so happy. It wanted to have a friend. The bird and the bird wanted to be friends. The bird and the bird played together.\n",
            "Then, they met a friendly squirrel named Bob. Bob and Bob had a friend, a big, friendly rabbit. They were very grateful for Bob. Bob and Bob and Bob had shared a friend.\n",
            "At the end of the trip was part of the journey to give!!!!\n",
            "\n",
            "\n",
            "Step 9600, Loss: 2.0792, Elapsed Time: 5.22s\n",
            "Generated text:\n",
            "Once upon a time there lived an ordinary dog. The dog liked to go and play outside. The dog was very happy. He liked to run, run all day.\n",
            "One sunny day, the sun went down and the sun came up. The sun came out and the sky was so bright. Suddenly, a little boy named Tim saw the sun and he wanted to drink it. He asked his mom what he wanted and he told him to go to the park.\n",
            "Tim and his mom went home and found some yummy treats. Tim was excited and he ran to their home. He saw lots of candy and he ate the delicious!!!!\n",
            "\n",
            "\n",
            "Step 9800, Loss: 2.0424, Elapsed Time: 5.13s\n",
            "Generated text:\n",
            "Once upon a time there were a smart bird named Jack. Jack had a red bird and a pretty bird named Blue. They loved to fly together. Jack's bird would sing and be the bird with his bird.\n",
            "One day, the bird wanted to make a big knot for himself. Jack tried, but he was too scared to fly away. He tried to flap their wings too hard and the bird did not move. Jack tried to fly away from Blue, but he couldn't get up. Jack felt sad that he couldn't fly anymore. Jack felt sad and realized it had a plan to take the bird.\n",
            "He tried his!!!!\n",
            "\n",
            "\n",
            "Step 10000, Loss: 2.1913, Elapsed Time: 5.19s\n",
            "Generated text:\n",
            "Once upon a time a tall bald prince named Tim was a little bird who lived in his nest with his family. One day, Timmy went outside and noticed that his nest was a beautiful birdcage. He asked his mom if she could go outside and play.\n",
            "Timmy said, \"No, I can't get it. I don't want to leave because it won't be alone.\" So, he got to the birdcage and started to sing. But it didn't work for Timmy.\n",
            "Suddenly, Timmy saw an old bird flying away with its nest. Timmy said, \"I'm scared but!!!!\n",
            "\n",
            "\n",
            "Step 10200, Loss: 2.1352, Elapsed Time: 5.30s\n",
            "Generated text:\n",
            "Once upon a time there lived an independent rabbit. Every day, the rabbit was always exploring the woods and looking for something new. One day, as a small rabbit hopped around the bush, the rabbit heard a noise. The rabbit hopped away and saw a beautiful, fluffy bunny. The rabbit hopped up closer and closer until it hopped away.\n",
            "The rabbit hopped and hopped in the spring, but the bunny followed was a small, strong tree. The bunny hopped and hopped until it came to a big field. It hopped around the valley, looking for its way back to the ground.\n",
            "The bunny noticed a small bird and was a tiny!!!!\n",
            "\n",
            "\n",
            "Step 10400, Loss: 2.1165, Elapsed Time: 5.25s\n",
            "Generated text:\n",
            "Once upon a time in the sky! In the sun, there was a bright blue cloud. All the birds were so excited! They couldn't see what was happening. The cloud stopped singing. It looked so beautiful! It was so big that it couldn't believe it. It was so happy! From that day on, the cloud was a little girl. She loved being so enthusiastic about it. The sky felt so cozy and the wind would smile on her face. The cloud kept singing and dancing until it was full. The girl was so happy with the sun.\n",
            "But then, the sky had changed and a beautiful blue cloud that!!!!\n",
            "\n",
            "\n",
            "Step 10600, Loss: 2.1307, Elapsed Time: 5.08s\n",
            "Generated text:\n",
            "Once upon a time in the sun decided to go for a ride in the sun. They were going on a picnic in the park. As they sat on a bench, they saw another little girl who had already had a special treat.\n",
            "The girl's mum said, \"I want to eat you to eat the yummy fruit. But it will be delicious! We must be able to eat it later! The girl had so much fun that she started to feel so happy with her special treat. \n",
            "When it was time to go home, Mom said, \"It is a good day. Are you sure it will eat so many!!!!\n",
            "\n",
            "\n",
            "Step 10800, Loss: 2.1001, Elapsed Time: 5.14s\n",
            "Generated text:\n",
            "Once upon a time a man lived outside the town. He loved to cook yummy cakes.\n",
            "One day, the man went to his neighbor's house and he was very hungry, so he asked him to help him. The neighbor didn't want to give him a piece of paper and the man got a big bag. The man was so happy that he decided to clean the basket and put the paper to make it happy.\n",
            "The man was so proud of the work. He kept scrubbing, and the neighbor's house, and even the dog's house. He had worked hard to be organized. \n",
            "The man's family!!!!\n",
            "\n",
            "\n",
            "Step 11000, Loss: 2.0595, Elapsed Time: 5.29s\n",
            "Generated text:\n",
            "Once upon a time there were two bears and Jack. They were walking around the forest looking for food. Suddenly, Jack saw a big oak tree! It was so tall and tall inside. Jack and his friends were so excited and they quickly ran inside. They saw a squirrel and he was very hungry. Jack said to the squirrel, \"Let's go and see it together!\" He looked up and said yes, but Jack was very careful.\n",
            "Suddenly, Jack's friend, Sam, said, \"Let's play a game. We should play together.\" Jack said, \"No, Sam! They are so stupid!\" They started playing!!!!\n",
            "\n",
            "\n",
            "Step 11200, Loss: 2.0623, Elapsed Time: 5.26s\n",
            "Generated text:\n",
            "Once upon a time in the garden surrounded at the flowers in the garden. One day he decided he wanted to go to the garden and find out that something delicious.\n",
            "He went home, feeling excited as he could find out what he wanted to eat, so he started looking for something yummy. But he couldn't find a tasty apple!\n",
            "He looked around for his mom in the garden, but no matter how he could not find it. Then he heard a voice from inside.\n",
            "\"Come here,\" it said. \"I can help my mom.\"\n",
            "The mother looked around and found an apple, then he carefully cut his!!!!\n",
            "\n",
            "\n",
            "Step 11400, Loss: 1.9875, Elapsed Time: 5.33s\n",
            "Generated text:\n",
            "Once upon a time there lived two good friends. They loved to play with other friends, and all the other friends. One day, a little squirrel wanted to play a game. His friend was a big, red ball. He said that it was his best friend. They all wanted to play with other friends. They played together every day and had a great time. Everyone was a great idea.\n",
            "After playing, all the other friends played with their friends. They had so much fun playing games with the ball, but it was time to go away. They were sad but decided to share with him. The end.\n",
            "\n",
            "\n",
            "\n",
            "Step 11600, Loss: 2.1601, Elapsed Time: 5.35s\n",
            "Generated text:\n",
            "Once upon a time there lived two boys called Jack and Tim. One day, Jack said to Tim, \"Let's go to the beach together to see it!\" Tim was excited when he saw the sandcastle.\n",
            "He ran to get a big towel to put on the towel. Tim grabbed the towel and started to float down on the beach. He was so proud, he started to laugh.\n",
            "\"Mom, do you want to go to the beach?\" Jack asked.\n",
            "His mom smiled. \"Yes, Jack,\" she said. \"Can you help me get to the beach?\"\n",
            "When they arrived, Jack saw his daughter!!!!\n",
            "\n",
            "\n",
            "Step 11800, Loss: 2.0504, Elapsed Time: 5.21s\n",
            "Generated text:\n",
            "Once upon a time in the garden to cook dinner. He had a pot and a toy car were filled with water. The pot was a great little. It was very hot and had to be warm.\n",
            "The little girl's mom asked her to clean up the pot and help her clean up. Lily was excited and started to fill the pot with water. She washed all the soup and scrubbed. After the pot had finished clean.\n",
            "When she was done, she hugged her mom and said she had done it. Her mom was so happy she couldn't wait to be back.\n",
            "The mom hugged her and said goodbye to the!!!!\n",
            "\n",
            "\n",
            "Step 12000, Loss: 2.0466, Elapsed Time: 5.15s\n",
            "Generated text:\n",
            "Once upon a time in the land to three year old. It was full of energy! The 3 year old was very special because it could be. Every morning, the little girl was happy and she went to the park. She saw many other kids playing together. They played tag and have fun. After a while, the little girl's mommy said, \"We made a beautiful new home. You will always be happy.\"\n",
            "The little girl said, \"Thank you for helping me, Mommy. You can play with me.\" But then one day, the little girl asked her mommy said, \"Mommy, what was!!!!\n",
            "\n",
            "\n",
            "Step 12200, Loss: 2.0492, Elapsed Time: 5.22s\n",
            "Generated text:\n",
            "Once upon a time there were two cats named Sam and a mouse. Sam loved to help others with the other animals. They would often help others who helped help them to help. He would often get the cat and the mouse to help his friends.\n",
            "One day, Sam saw a new friend, new friend, the mouse, and asked the mouse, \"Can you carry me away my friend?\" The baby bear said, \"Yes, please!\"\n",
            "Sam and the mouse ran as fast as they could. They could carry the cat and they were the mouse. They all helped him carry the toy in a new way. They worked at!!!!\n",
            "\n",
            "\n",
            "Step 12400, Loss: 2.0782, Elapsed Time: 5.13s\n",
            "Generated text:\n",
            "Once upon a time in his family in the park. It was so happy that it didn't look like it. The sun was shining, so everyone had fun playing in the park together. \n",
            "When the sun was shining, the air was bright and it was so warm. The sky was so clear that it had to be even more happy. \n",
            "The next night was closed the rain and it began to rain. The rain stopped and the sun was getting colder. \n",
            "Finally, the sun was over again and the rain stopped. The mom was so happy and gave her a warm towel with her friends. They thanked each another!!!!\n",
            "\n",
            "\n",
            "Step 12600, Loss: 2.0751, Elapsed Time: 5.20s\n",
            "Generated text:\n",
            "Once upon a time in the sun always looked out for breakfast for breakfast for breakfast. But one day, a big storm blew and the sun came up into the sky. The wind began to blow, and the wind blew the raindrops to the sky. \n",
            "The wind was so loud and the sun was getting bigger and bigger. Suddenly, the wind blew and the ground started to fall. The wind hit a tree with a sharp pain and the rain fell down. The rain got hard and the wind was wet and the wind made the rain fall! \n",
            "The wind stopped blowing and the wind got very hard and the wind came outside!!!!\n",
            "\n",
            "\n",
            "Step 12800, Loss: 2.0992, Elapsed Time: 5.11s\n",
            "Generated text:\n",
            "Once upon a time in a far side, there was a little girl named Lucy. Lucy loved to play with her toys, especially her toys. Every morning she would wake up and say, \"One morning, mommy and daddy, daddy?\" They went outside to play.\n",
            "When they got to their room, they saw a big box with a big box inside. When they opened the box, they all found a lot of things inside. They found a big box with lots of toys and toys, and some cars. They all wanted to play a game with the toys to make a game with it. They all thought of something funny!!!!\n",
            "\n",
            "\n",
            "Step 13000, Loss: 2.0690, Elapsed Time: 5.40s\n",
            "Generated text:\n",
            "Once upon a time in, a family wanted to play with the kids. The kids saw the children and wanted to play too. They played all day long, but the kids were having a great time.\n",
            "The family went to the beach and the family went to the beach. They all played in the sand and the kids loved to play. They had so much fun on the shore and had so much fun.\n",
            "But then, the kids saw something disgusting. They all started to quarrel. The kids were so scared that they had to go away, so they decided to run away. They both wanted to go back to the sea but!!!!\n",
            "\n",
            "\n",
            "Step 13200, Loss: 2.0420, Elapsed Time: 5.00s\n",
            "Generated text:\n",
            "Once upon a time in the morning little boy named Tim saw a beautiful sunset. He felt so proud of his friend, Timmy, came over and played in the snow. Timmy wanted to show off his friend, Timmy, who was so jealous of Timmy because he didn't want to give up.\n",
            "Timmy thought about it and decided to play with a friend Billy. He took a leaf from Timmy and threw it back to Timmy's friend Billy, said, \"I have a lot of fun!\" Timmy was sad he didn't want to listen to Timmy, so he started to cry and didn't!!!!\n",
            "\n",
            "\n",
            "Step 13400, Loss: 2.0283, Elapsed Time: 5.41s\n",
            "Generated text:\n",
            "Once upon a time there was a clever bunny who loved to eat. Every day, it would eat and eat yummy food.\n",
            "One day, it had a very special idea because that something was different. It was a big, brown bunny named Benny's home. Benny loved to eat carrots and run around and make sure they were too big and strong.\n",
            "Benny was happy to have found out to share his lunch. He ate all the carrots and the carrots and they became the best of the best carrots.\n",
            "After they finished eating, it made Benny felt even bigger than his friend, his carrot that was very tired and not!!!!\n",
            "\n",
            "\n",
            "Step 13600, Loss: 2.0726, Elapsed Time: 5.22s\n",
            "Generated text:\n",
            "Once upon a time there lived two brave knights. He loved to go for walks in a big lake. One day, he decided to travel to the river, and find something fun on the river. \n",
            "As he walked, he stumbled upon a small hole in the water. He looked around and couldn't see the deep deep deep deep deep and dark. Suddenly, there was no help. \n",
            "When he got in, he saw a little boy. The boy was so sad, he started to cry.\n",
            "The brave knight asked his friends. He said, \"It's okay, that people can make me happy again. Just!!!!\n",
            "\n",
            "\n",
            "Step 13800, Loss: 2.0745, Elapsed Time: 5.35s\n",
            "Generated text:\n",
            "Once upon a time in an open, a small boy named Tim. Tim loved to eat yummy food. He would eat all day, but every time the sun was shining. But one day, the sky became dark and Tim's friend, the sky. Tim was scared, but he was still alone.\n",
            "Tim's friend, Billy, came over to play. They played on the ground together. But Tim was nowhere to be found. Billy saw Tim and wanted to share his lunch. He gave him some water and they were both very happy and ate it.\n",
            "\n",
            "\n",
            "\n",
            "Step 14000, Loss: 2.0485, Elapsed Time: 5.10s\n",
            "Generated text:\n",
            "Once upon a time in his backyard in a small village. In this middle of the world, he was a very happy place to explore and discover what wonderful he could be.\n",
            "But then something unexpected happened. A big, scary monster started to get very scared. He looked up and saw that it wasn't scary, but he couldn't find it.\n",
            "He quickly decided to find a way to get down the village to find his way home. As he was walking, he noticed something strange. The monster stopped and looked closer to see what it looked like.\n",
            "He stopped to look closer. A big man had a friendly look on!!!!\n",
            "\n",
            "\n",
            "Step 14200, Loss: 1.9969, Elapsed Time: 5.23s\n",
            "Generated text:\n",
            "Once upon a time there lived a rich girl named Jane. She lived in a big, beautiful castle. She loved to play with her dolls, so every day she would go to visit her friend's kingdom.\n",
            "One day, Jane asked her mom to go to the post to the post office. Mom said yes and they took a walk to the post office.\n",
            "First, Jane saw a beautiful sign about the post on the wall. She asked her mom, \"What are you doing?\" Her mom said, \"I want to mail a letter too. I want to write the letter to someone's help?\" Jane smiled and said,!!!!\n",
            "\n",
            "\n",
            "Step 14400, Loss: 1.9903, Elapsed Time: 5.26s\n",
            "Generated text:\n",
            "Once upon a time in his mom said she had a great idea. She asked her what she saw. Her mom said she was a big surprise and was ready. She said she should get some of her friends.\n",
            "The little girl had to unpack all the things she had. Her mom said no, but she was too scared to unpack the bags. The two friends was sad.\n",
            "Then a big, shiny coin appeared. It was the most beautiful thing she had ever seen. The coins were all shiny and shiny. The little girl asked her mom if she would have one of her money. She said yes, but now!!!!\n",
            "\n",
            "\n",
            "Step 14600, Loss: 2.0687, Elapsed Time: 5.08s\n",
            "Generated text:\n",
            "Once upon a time in the woods there lived a small bear named Ben. Ben loved to play outside and look for bugs. He had a big box in the forest and he wanted to see what was in the forest.\n",
            "One day, he saw a big tree. The tree was a little bird and wanted to touch it. Ben wanted to touch the tree, but there was only a big tree blocking a tree. The tree was not a good branch.\n",
            "Ben tried to reach the tree, but it was too high for him. He fell into the tree and got very hurt. He had eaten some medicine. He wanted the needles!!!!\n",
            "\n",
            "\n",
            "Step 14800, Loss: 1.9957, Elapsed Time: 5.94s\n",
            "Generated text:\n",
            "Once upon a time there were friends all friends who loved to play with her toys. The sun would always always make everyone happy and they played together and everyone thought it was the best when the same. \n",
            "One day, the friends saw a little boy with a big ball and wanted to play with them. But he didn't have any friends to play with. They all said \"I don't like it, but we should share it.\" \n",
            "The boy's friends tried to find the ball back, but the ball was too high. They said \"No, we will play with the ball instead!\" The boys was sad because it!!!!\n",
            "\n",
            "\n",
            "Step 15000, Loss: 1.9643, Elapsed Time: 5.19s\n",
            "Generated text:\n",
            "Once upon a time there lived a kind old man. He lived in a small house on a tall tree. Every day he would go on an ordinary holiday in the woods.\n",
            "At the park, the old man saw a little girl in the park. The girl was so excited to have the perfect time to go home. The old man was very happy to be back by the city. He was very glad he had the little girl's home.\n",
            "The old man was happy to see the little girl and he had a wonderful day. He thanked Anna and said he would love to come back. Together, he and the girl went into!!!!\n",
            "\n",
            "\n",
            "Step 15200, Loss: 2.0306, Elapsed Time: 5.48s\n",
            "Generated text:\n",
            "Once upon a time there lived an ordinary bear. He had a very important job to be a bad guy. He always wanted to be brave. \n",
            "One day, the bear met a wise old bear. The bear was very wise. He thought that it would be a very important operation to make things better.\n",
            "The wise bear wanted to do something new and fun in the woods. He decided to help out for an adventure. He set off and set off to the big adventure. \n",
            "He set out and soon he found a new place he could find. He thought for a moment, but he knew it was too difficult for!!!!\n",
            "\n",
            "\n",
            "Step 15400, Loss: 2.0063, Elapsed Time: 5.14s\n",
            "Generated text:\n",
            "Once upon a time there were two animals, a bunny and a mouse. They were both very friendly and loved to play together. One day, a bird hopped over to the rabbit to visit the bunny. The bunny said, \"Let's play hide and seek!\"\n",
            "The bunny was very excited and quickly ran to the rabbit. The rabbit said, \"I want to jump in your burrow and hop. Let's do it!\"\n",
            "The bunny was so excited and hopped on the other animals. The rabbit said, \"I want to hop faster.\" The rabbit said, \"Let's hop over our paws.\" The rabbit agreed, hopping!!!!\n",
            "\n",
            "\n",
            "Step 15600, Loss: 2.0088, Elapsed Time: 5.28s\n",
            "Generated text:\n",
            "Once upon a time in an oak land was so happy that everyone had seen a beautiful world. \n",
            "One day, a little girl wanted to take her big sister. She asked everyone: \"How can I go to the forest?\" Everyone was surprised, but they said no. \n",
            "The little girl was sad and said, \"Why don't you go to the nearby forest so we can find the secret!\" The oak tree smiled and said, \"I can't find the secret, but I want to be your friend.\"\n",
            "The little girl nodded and said, \"No! I don't need it.\" She said, \"But!!!!\n",
            "\n",
            "\n",
            "Step 15800, Loss: 2.0433, Elapsed Time: 5.20s\n",
            "Generated text:\n",
            "Once upon a time there were two animals that were in a jungle. The birds were happy and liked to play together.\n",
            "One day the rabbit said, ��Let��s play hide and seek!��\n",
            "The rabbit ran up to the trees and hid behind a tree. But the leaves were too high and too high. The rabbit was very sad and did not like the other animals. So, the squirrel decided to go and find the other animals.\n",
            "They decided to hide in a tree. But the rabbit was too strong and strong. The rabbit kept playing, and eventually found an old branch.\n",
            "Finally, after!!!!\n",
            "\n",
            "\n",
            "Step 16000, Loss: 2.0014, Elapsed Time: 5.13s\n",
            "Generated text:\n",
            "Once upon a time a girl called Amy wanted to bake for her first time. She put on the biggest cake in the store. She loved it! When it was time to bake all day, Amy put on her favorite cake. She went to the bakery and bought lots of delicious cake. She was very happy and started to mix the cake.\n",
            "When Amy got to baking, the cake was so big to everyone. It was very impressive! She loved cake! She was so excited that she couldn't wait to get to bake cake. When it looked like cake, she wanted it. She saw how big the cake was in the garden!!!!\n",
            "\n",
            "\n",
            "Step 16200, Loss: 2.0052, Elapsed Time: 5.24s\n",
            "Generated text:\n",
            "Once upon a time in an enormous, tall mountain was always so happy! Everyone was having so many fun!\n",
            "One day, a brave boy called John found the volcano. He quickly grabbed the volcano and ran outside to show his friends!\n",
            "The volcano was so big and the boy couldn't believe that he had been there! John was so excited! He quickly grabbed one of his friends and ran off, laughing and showing off to everyone.\n",
            "When the volcano arrived, the family were so surprised to see him and they all laughed. John smiled when he saw the volcano in his way. They ran home with their friends, who loved!!!!\n",
            "\n",
            "\n",
            "Step 16400, Loss: 1.9982, Elapsed Time: 5.09s\n",
            "Generated text:\n",
            "Once upon a time there lived an intelligent mouse. He loved to eat food and eat all day. One day, he saw a big piece of honeycomb! He wanted to eat everything but he couldn't find it. So he decided to try to eat the honey. He was so hungry and he wanted to eat it. \n",
            "So he decided to try to get something else for the honey. He climbed and climbed on the ground. He was so happy! He ate it all up and he couldn't wait to eat it. \n",
            "He even found the perfect honey, so he ate it right away! It was a beautiful feeling!!!!\n",
            "\n",
            "\n",
            "Step 16600, Loss: 1.9576, Elapsed Time: 5.18s\n",
            "Generated text:\n",
            "Once upon a time a big man called Joe was walking on a path. He saw his friend Sarah. She wanted to play with it so she could be very happy. \n",
            "As Joe was walking around, he saw a little girl crying. She went up to say hello to the girl. She had a big smile on her face and she was very happy. She walked up and said \"Hi!\" The little girl stopped and listened to the man. \n",
            "Joe was very excited. He asked Sarah if he could play too. Sarah said yes! She gave him a stick and showed him that it was too difficult to catch with both!!!!\n",
            "\n",
            "\n",
            "Step 16800, Loss: 2.0174, Elapsed Time: 4.96s\n",
            "Generated text:\n",
            "Once upon a time in his kitchen for a special day. A little boy wanted to eat his cake. He went to his room and asked his dad, ��Can I have an exam?�� His dad said, ��Not yet, Tim.��\n",
            "The little boy was very upset. He tried to think about what to do. But, his Dad had no idea. He tried very hard to climb the wall. But the boy was too small.\n",
            "The boy was very upset. He didn't know what to do. Suddenly, he had an idea. He decided to ask his parents if he could use some!!!!\n",
            "\n",
            "\n",
            "Step 17000, Loss: 1.8310, Elapsed Time: 5.47s\n",
            "Generated text:\n",
            "Once upon a time there lived two cats who liked to run. One of the cats was a fun and they were running to the other cars. Suddenly they noticed a big red car coming up ahead. They were stuck in the car and got stuck. They started to pull the car down and the car fell down. They were very scared. The cats were stuck in the gas car. The Cat and the dog helped the two cars back. They worked together to get the car. They fixed the car, and the owner was very happy. They were free. The end.Once\n",
            "\n",
            "\n",
            "Step 17200, Loss: 2.0736, Elapsed Time: 4.92s\n",
            "Generated text:\n",
            "Once upon a time there lived an honest dog named Spot. Spot loved to play with his friends, but he was always grumpy. He didn't mean to anyone, but he kept running and jumping around.\n",
            "One day, Spot got very sick and hurt. He needed to go to the doctor for a doctor to help him heal. The doctor said, \"The vet will not cut your medicine and you can heal you.\" Spot was very sad, but he still felt better.\n",
            "The doctor was very happy. He learned to be kind and happy. He learned that taking better things that are bad to be happy. And Spot could!!!!\n",
            "\n",
            "\n",
            "Step 17400, Loss: 1.9063, Elapsed Time: 5.35s\n",
            "Generated text:\n",
            "Once upon a time a boy called Tommy and his name Joe. He loved to play soccer. \n",
            "One day, Jack was playing with his favorite football and he was so excited as he was jumping on the ball. He couldn't resist it as a ball! \n",
            "The ball started to bounce. It was a bit too loud, but it was too late for being able to bounce the ball again in his mouth! \n",
            "Tommy was sad and he didn't want to get his ball so he could never get it right. \n",
            "Finally, he had a time for the ball, not sure how he was, but one!!!!\n",
            "\n",
            "\n",
            "Step 17600, Loss: 2.0356, Elapsed Time: 5.25s\n",
            "Generated text:\n",
            "Once upon a time in his kitchen that lived all the other children were getting up. It was an ordinary day, but then something bad happened. Everyone around to see the poor children had lost. \n",
            "The children were so sad that their mom had an idea. She asked if she wanted to help people. The kids agreed and the kids went to the store to get something to happen. \n",
            "At home, the mom showed the importance of being so hard. The kids said it was important to always be safe to help each other. They worked to get the food for all. \n",
            "At first, they had to work with it!!!!\n",
            "\n",
            "\n",
            "Step 17800, Loss: 1.8474, Elapsed Time: 5.29s\n",
            "Generated text:\n",
            "Once upon a time there lived a kind and compassionate little dog. The cat liked to walk around and play in the grass. One day, the owner said \"Let's go to get the owner.\"\n",
            "The owner went to the owner. The owner looked in the house, and saw a big, heavy tree. The owner was so happy to see the owner and pet it. She was so happy that he could hardly move.\n",
            "The owner went back to the owner and told the owner to go. The owner followed the owner to the owner and went on a big road and the owner was not friendly. They had to wait together forever!!!!\n",
            "\n",
            "\n",
            "Step 18000, Loss: 1.9653, Elapsed Time: 5.15s\n",
            "Generated text:\n",
            "Once upon a time there lived an ugly boy named Jack. He had no friends to do.\n",
            "One day, Jack saw a big bird flying on a birdcage. He tried to fly around and see how high he could fly and fly high in the sky. Jack tried to fly up, but he couldn't reach it.\n",
            "Suddenly, Jack saw a bird swooped down from the birdcage. The bird flew away and Jack was very sad. Jack wanted to help the bird, but he didn't know how. He had an idea. He flew over his house and found the birdcage.\n",
            "The bird felt!!!!\n",
            "\n",
            "\n",
            "Step 18200, Loss: 2.0237, Elapsed Time: 5.07s\n",
            "Generated text:\n",
            "Once upon a time in the kitchen in the kitchen, a small girl named Sally. She was playing in the kitchen when she saw apron hanging in a vase. She wanted apron too, but she was too busy.\n",
            "She asked her mom for help. Her mom said, \"Be polite, Sally. It will be apron.\" Sally was sad and felt sorry for her. She asked the next door and the door opened.\n",
            "Her mom said, \"Let's play a game with the toys, but it is too dangerous. Don't worry, we should wait and see where alligator has been.\"\n",
            "Sally!!!!\n",
            "\n",
            "\n",
            "Step 18400, Loss: 1.9865, Elapsed Time: 5.14s\n",
            "Generated text:\n",
            "Once upon a time there were two two brothers who were walking through a tall building. One of their brothers were looking for help. The brothers was excited! But then they saw a pile of blocks and he was very excited. He wanted to help the other workers.\n",
            "So the brother grabbed the blocks of blocks and they started to stack the blocks. The stack of blocks apart and soon the blocks were very full of blocks! The brothers were so happy to have found the blocks to complete their project. They were very proud of their tower and thanked the children for being so generous and they all had so creative.\n",
            "\n",
            "\n",
            "\n",
            "Final generated text (after training):\n",
            "Once upon a time there was a very persistent little girl named Lily. She loved to sing and sing all day long. One day, her mom said she couldn't sing because it got very dirty. \n",
            "Lily was sad because her mom told her it was important to go to the park. \n",
            "But then, Lily realized that she couldn't sing a beautiful song. It was her favorite color was too much better than the little girl. \n",
            "Lily realized that it was important to listen to her mom and sing songs. She sang every day and sang songs every day. \n",
            "But one day, Lily went back into!!!!"
          ]
        }
      ],
      "source": [
        "# Initialize the model and optimizer within the mesh context\n",
        "# This ensures model parameters are properly sharded according to our partition specs\n",
        "with jax.set_mesh(mesh):\n",
        "    model = create_model(rngs=nnx.Rngs(0))\n",
        "    # Create Adam optimizer with learning rate 1e-3\n",
        "    # wrt=nnx.Param means we only optimize parameters (not other state like batch stats)\n",
        "    optimizer = nnx.Optimizer(model, optax.adam(1e-3), wrt=nnx.Param)\n",
        "\n",
        "# Initialize metrics tracking\n",
        "metrics = nnx.MultiMetric(\n",
        "    loss=nnx.metrics.Average(\"loss\"),\n",
        ")\n",
        "\n",
        "# Initialize random number generator (not used in current training but good practice)\n",
        "rng = jax.random.PRNGKey(0)\n",
        "\n",
        "# Test the model before training by generating some text\n",
        "start_prompt = \"Once upon a time\"\n",
        "start_tokens = tokenizer.encode(start_prompt)[:maxlen]\n",
        "print(\"Initial generated text (before training):\")\n",
        "generated_text = model.generate_text(maxlen, start_tokens)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Dictionary to store training metrics over time\n",
        "metrics_history = {\n",
        "    \"train_loss\": [],\n",
        "}\n",
        "\n",
        "# Helper function to create target sequences from input sequences\n",
        "# In language modeling, the target is the input shifted by one position\n",
        "# Uses jax.vmap to vectorize the operation across the batch dimension efficiently\n",
        "prep_target_batch = jax.vmap(\n",
        "    lambda tokens: jnp.concatenate((tokens[1:], jnp.array([0])))\n",
        ")\n",
        "\n",
        "# Training loop\n",
        "step = 0\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()\n",
        "\n",
        "    for batch in text_dl:\n",
        "        # Skip batches that don't divide evenly across devices\n",
        "        # This ensures each device gets the same amount of data\n",
        "        if len(batch) % len(jax.devices()) != 0:\n",
        "            continue\n",
        "\n",
        "        # Prepare input and target batches\n",
        "        # Transpose to get shape (batch_size, seq_len)\n",
        "        input_batch = jnp.array(jnp.array(batch).T)\n",
        "\n",
        "        # Create targets by shifting inputs left by one position\n",
        "        # E.g., if input is [1, 2, 3, 4], target is [2, 3, 4, 0]\n",
        "        target_batch = prep_target_batch(input_batch)\n",
        "\n",
        "        # Shard the data across devices for data parallelism\n",
        "        # NamedSharding with P(\"batch\", None) means:\n",
        "        # - Shard along the batch dimension (first axis)\n",
        "        # - Replicate along the sequence dimension (second axis)\n",
        "        # jax.device_put moves the data to the devices with the specified sharding\n",
        "        sharded_batch = jax.device_put(\n",
        "            (input_batch, target_batch),\n",
        "            NamedSharding(mesh, P(\"batch\", None))\n",
        "        )\n",
        "\n",
        "        # Perform one training step\n",
        "        # JAX automatically handles the distributed computation!\n",
        "        train_step(model, optimizer, metrics, sharded_batch)\n",
        "\n",
        "        # Log metrics every 200 steps\n",
        "        if (step + 1) % 200 == 0:\n",
        "            # Compute and store current metrics\n",
        "            for metric, value in metrics.compute().items():\n",
        "                metrics_history[f\"train_{metric}\"].append(value)\n",
        "            metrics.reset()\n",
        "\n",
        "            # Print progress\n",
        "            elapsed_time = time.time() - start_time\n",
        "            print(f\"\\nStep {step + 1}, Loss: {metrics_history['train_loss'][-1]:.4f}, \"\n",
        "                  f\"Elapsed Time: {elapsed_time:.2f}s\")\n",
        "            start_time = time.time()\n",
        "\n",
        "            # Generate sample text to see progress\n",
        "            print(\"Generated text:\")\n",
        "            generated_text = model.generate_text(maxlen, start_tokens)\n",
        "            print(\"\\n\")\n",
        "\n",
        "        step += 1\n",
        "\n",
        "# Final text generation to see the fully trained model\n",
        "print(\"\\nFinal generated text (after training):\")\n",
        "generated_text = model.generate_text(maxlen, start_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thaLs6TD0lt5"
      },
      "source": [
        "Visualize the training loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "B6Eg1Cz2y_iP",
        "outputId": "3169ac2b-5ef8-405a-d7b3-14413a3634dc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(metrics_history['train_loss'])\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Step')\n",
        "plt.ylabel('Loss')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB-ExEt1Zl1C"
      },
      "source": [
        "As you can see, the model goes from generating completely random words at the beginning to generating sensible tiny stories at the end of the training. So essentially we have pretrained a small LLM to write tiny stories for us."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDCVkgMTLC5M"
      },
      "source": [
        "## Debugging the model\n",
        "\n",
        "When working with JAX and JIT-compiled functions, debugging requires different techniques than traditional Python debugging. Since JIT compilation traces your function and compiles it to XLA, standard `print()` statements won't work inside compiled functions.\n",
        "\n",
        "**JAX debugging tools:**\n",
        "\n",
        "- **`jax.debug.print()`**: Print intermediate values from inside JIT-compiled functions\n",
        "- **`jax.debug_nans`**: Automatically detect and raise errors when NaN values appear in computations\n",
        "\n",
        "For more debugging techniques, see the [JAX debugging documentation](https://jax.readthedocs.io/en/latest/debugging/index.html).\n",
        "\n",
        "Let's explore these debugging techniques with our miniGPT model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhgaK7w0W3yn"
      },
      "source": [
        "### Using `jax.debug.print()` inside JIT-compiled functions\n",
        "\n",
        "The `jax.debug.print()` function lets you inspect intermediate values during execution. Unlike regular `print()`, it works inside `@jax.jit` decorated functions. This is invaluable for understanding what's happening inside your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcALManWW3yn",
        "outputId": "b249a88a-2656-4806-cbb2-8aef3bda5416"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: (Array(1, dtype=int32), Array(128, dtype=int32))\n",
            "After embedding - shape: (Array(1, dtype=int32), Array(128, dtype=int32), Array(128, dtype=int32)), mean: -0.0091, std: 0.2439\n"
          ]
        }
      ],
      "source": [
        "# Demonstrate jax.debug.print inside a JIT-compiled function\n",
        "@jax.jit\n",
        "def debug_forward_pass(model, inputs):\n",
        "    \"\"\"Forward pass with debug prints to inspect intermediate values.\"\"\"\n",
        "    # Inspect input\n",
        "    jax.debug.print(\"Input shape: {shape}\", shape=inputs.shape)\n",
        "\n",
        "    # Get embeddings and inspect\n",
        "    x = model.embedding_layer(inputs)\n",
        "    jax.debug.print(\"After embedding - shape: {shape}, mean: {mean:.4f}, std: {std:.4f}\",\n",
        "                    shape=x.shape, mean=jnp.mean(x), std=jnp.std(x))\n",
        "\n",
        "    # Pass through transformer blocks\n",
        "    for i, block in enumerate(model.transformer_blocks):\n",
        "        x = block(x, training=False)\n",
        "        jax.debug.print(\"After block {i} - mean: {mean:.4f}, std: {std:.4f}\",\n",
        "                        i=i, mean=jnp.mean(x), std=jnp.std(x))\n",
        "\n",
        "    logits = model.output_layer(x)\n",
        "    jax.debug.print(\"Output logits - shape: {shape}, mean: {mean:.4f}\",\n",
        "                    shape=logits.shape, mean=jnp.mean(logits))\n",
        "    return logits\n",
        "\n",
        "# Run the debug forward pass with a sample input\n",
        "sample_input = jnp.ones((1, maxlen), dtype=jnp.int32)\n",
        "_ = debug_forward_pass(model, sample_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duapvyZAW3yn"
      },
      "source": [
        "### Detecting NaN values with `jax.debug_nans`\n",
        "\n",
        "A common issue during training is exploding gradients that result in NaN (Not a Number) values. JAX provides a convenient configuration flag to automatically detect NaNs and raise an error immediately when they occur."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ra93eHdQW3yn",
        "outputId": "f217951d-e98a-41e1-9176-bdb5573ddde4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After block 0 - mean: -0.0005, std: 0.7882\n",
            "After block 1 - mean: 0.0241, std: 0.7797\n",
            "After block 2 - mean: 0.0026, std: 0.7188\n",
            "After block 3 - mean: 0.0208, std: 1.5434\n",
            "Output logits - shape: (Array(1, dtype=int32), Array(128, dtype=int32), Array(50257, dtype=int32)), mean: -11.1983\n",
            "Forward pass completed without NaN values\n"
          ]
        }
      ],
      "source": [
        "# Enable NaN debugging globally - any NaN will raise an error with a traceback\n",
        "# This is useful during development to catch numerical issues early\n",
        "jax.config.update(\"jax_debug_nans\", True)\n",
        "\n",
        "# Example: This function would raise an error if it produced NaN\n",
        "@jax.jit\n",
        "def safe_forward_pass(model, inputs):\n",
        "    \"\"\"Forward pass that will error if NaN is produced.\"\"\"\n",
        "    logits = model(inputs)\n",
        "    return logits\n",
        "\n",
        "# Test with our model (should work fine with a trained model)\n",
        "sample_input = jnp.ones((1, maxlen), dtype=jnp.int32)\n",
        "_ = safe_forward_pass(model, sample_input)\n",
        "print(\"Forward pass completed without NaN values\")\n",
        "\n",
        "# Disable NaN debugging for the rest of the notebook\n",
        "# (It adds overhead and can slow down training)\n",
        "jax.config.update(\"jax_debug_nans\", False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuLzhlyoW3yn"
      },
      "source": [
        "**Tip:** You can also use `jax.debug.print()` inside your loss function or training step to monitor gradient health and intermediate values. This is demonstrated in the code cell above showing the forward pass with debug prints."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNxCVBVeLC5M"
      },
      "source": [
        "## Orbax: Save and restore model checkpoints\n",
        "\n",
        "[Orbax](https://orbax.readthedocs.io/) is Google's checkpointing library designed for JAX workloads. It provides efficient, scalable, and flexible checkpoint management for machine learning models.\n",
        "\n",
        "**Why Orbax?**\n",
        "\n",
        "- **JAX-native**: Designed specifically for JAX's pytree structures and distributed arrays\n",
        "- **Asynchronous saving**: Non-blocking checkpoint saves that don't interrupt training\n",
        "- **Scalable**: Handles large models efficiently with support for sharded checkpoints across devices\n",
        "- **Flexible**: Works with any pytree structure, including Flax NNX models\n",
        "- **Cloud-ready**: Integrates with cloud storage backends like Google Cloud Storage (GCS)\n",
        "\n",
        "**Key concepts:**\n",
        "\n",
        "In this tutorial, we use:\n",
        "\n",
        "1. **`nnx.state()`**: Extracts the model's state (parameters, batch stats, etc.) as a pytree\n",
        "2. **`PyTreeCheckpointer`**: The main checkpointer class for saving and loading pytrees\n",
        "3. **`PyTreeSave` / `PyTreeRestore`**: Arguments specifying how to save or restore the checkpoint\n",
        "\n",
        "Let's save our trained model checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ic06KGLiLC5M",
        "outputId": "875a2213-4b24-4d52-c245-be143a15f1ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:[process=0][thread=MainThread][operation_id=1] _SignalingThread.join() waiting for signals ([]) blocking the main thread will slow down blocking save times. This is likely due to main thread calling result() on a CommitFuture.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "array_metadatas       d\t\t      _METADATA        _sharding\n",
            "_CHECKPOINT_METADATA  manifest.ocdbt  ocdbt.process_0\n"
          ]
        }
      ],
      "source": [
        "import orbax.checkpoint as orbax\n",
        "\n",
        "state = nnx.state(model)\n",
        "\n",
        "checkpointer = orbax.PyTreeCheckpointer()\n",
        "checkpointer.save('/content/save', args=orbax.args.PyTreeSave(state), force=True)\n",
        "\n",
        "# Make sure the files are there\n",
        "!ls /content/save/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZC2fVGlTW3yn"
      },
      "source": [
        "### Restoring a checkpoint\n",
        "\n",
        "To resume training or use a saved model for inference, restore the checkpoint using `PyTreeRestore`. The restored state can be applied to a new model instance using `nnx.update()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mA6VC4jOW3yn",
        "outputId": "457867ff-56a1-469e-f596-76ca9721dadc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text generated by restored model:\n",
            "Once upon a time there was a very persistent little girl named Lily. She loved to sing and sing all day long. One day, her mom said she couldn't sing because it got very dirty. \n",
            "Lily was sad because her mom told her it was important to go to the park. \n",
            "But then, Lily realized that she couldn't sing a beautiful song. It was her favorite color was too much better than the little girl. \n",
            "Lily realized that it was important to listen to her mom and sing songs. She sang every day and sang songs every day. \n",
            "But one day, Lily went back into!!!!"
          ]
        }
      ],
      "source": [
        "# Restore the checkpoint with proper sharding info\n",
        "from orbax.checkpoint import checkpoint_utils\n",
        "\n",
        "# Create restore args from the target state to provide sharding info\n",
        "restore_args = checkpoint_utils.construct_restore_args(state)\n",
        "restored_state = checkpointer.restore(\n",
        "    '/content/save',\n",
        "    args=orbax.args.PyTreeRestore(state, restore_args=restore_args)\n",
        ")\n",
        "\n",
        "# Create a fresh model instance and update it with restored state\n",
        "with jax.set_mesh(mesh):\n",
        "    restored_model = create_model(rngs=nnx.Rngs(42))  # Different seed to prove restoration works\n",
        "    nnx.update(restored_model, restored_state)\n",
        "\n",
        "# Verify restoration works by generating text\n",
        "print(\"Text generated by restored model:\")\n",
        "_ = restored_model.generate_text(maxlen, start_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kT28EUaDLC5M"
      },
      "source": [
        "## Tunix: Fine-tuning\n",
        "\n",
        "[Tunix](https://github.com/google/tunix) is a JAX-native LLM post-training library open sourced by Google. It supports a range of post-training techniques including supervised finetuning, preference tuning, reinforcement learning and model distillation. In this section, we are going to use Tunix to finetune the miniGPT model we just pretrained using LoRA ([Low-Rank Adaptation](https://arxiv.org/abs/2106.09685)) so that the finetuned model generates output of a different style."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxnW69mnLC5M"
      },
      "source": [
        "First we install Tunix and its dependencies, and import necessary libraries. Note that Colab will ask you to restart the runtime, but you can just ignore it.\n",
        "\n",
        "**Note:** this section assume multiple TPU cores. Free-tier Colab TPU v5e-1 cannot run here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDf7R6SVLC5M",
        "outputId": "675a9255-1e33-4970-a4dd-57f397f4eff3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/162.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m153.6/162.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.4/156.4 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.9/310.9 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m112.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.1/96.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m131.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.7/76.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for antlr4-python3-runtime (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pylatexenc (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -Uq google-tunix[prod]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "TM4P37kaLC5M"
      },
      "outputs": [],
      "source": [
        "import qwix\n",
        "import numpy as np\n",
        "from tunix.sft import peft_trainer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Od2XV7hyLC5M"
      },
      "source": [
        "We set some hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "NVFpndnjLC5M"
      },
      "outputs": [],
      "source": [
        "# LoRA Hyperparameters\n",
        "lora_rank = 16\n",
        "lora_alpha = 2.0\n",
        "lora_max_steps = 400\n",
        "lora_num_epochs = 20\n",
        "lora_batch_size = 80"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOlfgGu8LC5M"
      },
      "source": [
        "For LoRA fintuning we use the [Tiny Shakespeare](https://www.tensorflow.org/datasets/catalog/tiny_shakespeare) dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ouP0hlHLC5M",
        "outputId": "c4a145a9-422c-4b76-a512-0aed8732e5a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-22 22:35:25--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘TinyShakespeare.txt’\n",
            "\n",
            "\rTinyShakespeare.txt   0%[                    ]       0  --.-KB/s               \rTinyShakespeare.txt 100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2026-01-22 22:35:26 (20.1 MB/s) - ‘TinyShakespeare.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O TinyShakespeare.txt\n",
        "\n",
        "def load_shakespeare_dataset(batch_size, max_len, num_epochs):\n",
        "    input_file = \"TinyShakespeare.txt\"\n",
        "    # Read the entire text file\n",
        "    with open(input_file, 'r', encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "\n",
        "    # Tokenize the entire text (assuming tokenizer is available in scope)\n",
        "    tokens = tokenizer.encode(text)\n",
        "\n",
        "    # Create a simple data source from the tokens\n",
        "    class TokenDataSource(pygrain.RandomAccessDataSource):\n",
        "        def __init__(self, tokens, max_len):\n",
        "            self._tokens = np.array(tokens, dtype=np.int32)\n",
        "            self._max_len = max_len\n",
        "            # Calculate how many sequences we can create\n",
        "            self._length = len(self._tokens) // max_len\n",
        "\n",
        "        def __len__(self):\n",
        "            return self._length\n",
        "\n",
        "        def __getitem__(self, index):\n",
        "            # Return a sequence of max_len tokens\n",
        "            start_idx = index * self._max_len\n",
        "            end_idx = start_idx + self._max_len\n",
        "            return self._tokens[start_idx:end_idx]\n",
        "\n",
        "    # Create the data source\n",
        "    data_source = TokenDataSource(tokens, max_len)\n",
        "\n",
        "    # Create a sampler\n",
        "    sampler = pygrain.IndexSampler(\n",
        "        num_records=len(data_source),\n",
        "        shuffle=True,\n",
        "        seed=42,\n",
        "        num_epochs=num_epochs,\n",
        "        shard_options=pygrain.NoSharding()\n",
        "    )\n",
        "\n",
        "    # Create transformations\n",
        "    class ToTrainingInputDict(pygrain.MapTransform):\n",
        "        def map(self, batch):\n",
        "            return {\n",
        "                \"input_tokens\": batch,\n",
        "                \"input_mask\": np.ones_like(batch)\n",
        "            }\n",
        "\n",
        "    # Create the data loader\n",
        "    loader = pygrain.DataLoader(\n",
        "        data_source=data_source,\n",
        "        sampler=sampler,\n",
        "        operations=[\n",
        "            pygrain.Batch(batch_size=batch_size, drop_remainder=True),\n",
        "            ToTrainingInputDict(),\n",
        "        ],\n",
        "        worker_count=0,  # Use main thread\n",
        "    )\n",
        "\n",
        "    def to_training_input(loader):\n",
        "        # The trainer expects an iterable of `peft_trainer.TrainingInput`.\n",
        "        for item in loader:\n",
        "            yield peft_trainer.TrainingInput(**item)\n",
        "\n",
        "    return to_training_input(loader)\n",
        "\n",
        "lora_train_ds = load_shakespeare_dataset(lora_batch_size, maxlen, lora_num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWAHNKMYLC5M"
      },
      "source": [
        "We define a few helper functions to create the LoRA model, loss, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "6Ekbr3YlLC5M"
      },
      "outputs": [],
      "source": [
        "def get_lora_model(base_model, mesh):\n",
        "  lora_provider = qwix.LoraProvider(\n",
        "      # Target only feed-forward layers (standard 2D linear layers)\n",
        "      # Note: MultiHeadAttention uses LinearGeneral with 3D weights,\n",
        "      # which qwix LoRA doesn't support\n",
        "      module_path=\".*linear1|.*linear2\",\n",
        "      rank=lora_rank,\n",
        "      alpha=lora_alpha,\n",
        "  )\n",
        "\n",
        "  model_input = base_model.get_model_input()\n",
        "  lora_model = qwix.apply_lora_to_model(\n",
        "      base_model, lora_provider, rngs=nnx.Rngs(0), **model_input\n",
        "  )\n",
        "\n",
        "  with jax.set_mesh(mesh):\n",
        "    state = nnx.state(lora_model)\n",
        "    pspecs = nnx.get_partition_spec(state)\n",
        "    sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n",
        "    nnx.update(lora_model, sharded_state)\n",
        "\n",
        "  return lora_model\n",
        "\n",
        "def gen_model_input_fn(x: peft_trainer.TrainingInput):\n",
        "  return {\n",
        "      'inputs': x.input_tokens,\n",
        "      'training': True\n",
        "  }\n",
        "\n",
        "def lora_loss_fn(model, inputs, training):\n",
        "    inputs = inputs\n",
        "    targets = jnp.concatenate([inputs[:, 1:], jnp.zeros((inputs.shape[0], 1), dtype=jnp.int32)], axis=1)\n",
        "    logits = model(inputs, training=training)\n",
        "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=targets).mean()\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIyxXBo2LC5M"
      },
      "source": [
        "Now we can start the finetuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "b3296ffa7d484ddb86b8045973ecea34",
            "28cc1cf178f747d0b1a308318ebd8318",
            "dab699d4a6e747bf8541b0d8f91ca805",
            "46487f2bdbce4bcd877884d8865ac2f7",
            "e45aec5b8dd34226979dff369820e48a",
            "f4b272b6430f40f18c7488322dc66c0c",
            "a3605fe4ab5e4f4aa74f169d28af7a19",
            "e03eda6a65814d5287569f3eabe7873e",
            "456d4230468149bfa31bc3787ec91877",
            "012f4a89b5ab40dbaf31efe4e06fe88f",
            "5fb7bafefea141e5853e0e2adf06af61"
          ]
        },
        "id": "M5xo6Z_VLC5M",
        "outputId": "c3c14f84-034d-4a2d-bea1-2ce85316552b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting LoRA Finetuning...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training:   0%|          | 0/400 [00:00<?, ?step/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b3296ffa7d484ddb86b8045973ecea34"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "print(\"Starting LoRA Finetuning...\")\n",
        "with jax.set_mesh(mesh):\n",
        "    # Apply LoRA to the model\n",
        "    lora_model = get_lora_model(model, mesh)\n",
        "\n",
        "    # Setup Tunix PeftTrainer\n",
        "    training_config = peft_trainer.TrainingConfig(\n",
        "        eval_every_n_steps=None,\n",
        "        max_steps=lora_max_steps,\n",
        "        data_sharding_axis=('batch',),\n",
        "    )\n",
        "    lora_optimizer = optax.adamw(1e-2)\n",
        "    lora_trainer = peft_trainer.PeftTrainer(\n",
        "        lora_model, lora_optimizer, training_config\n",
        "    ).with_gen_model_input_fn(gen_model_input_fn).with_loss_fn(lora_loss_fn)\n",
        "\n",
        "    # Run LoRA training\n",
        "    lora_trainer.train(lora_train_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "PvFPnu5VLC5M",
        "outputId": "dc1dd87b-c91d-4014-f3a4-4dc1bb41b6a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating text after LoRA finetuning:\n",
            "\n",
            "\n",
            "Once upon a time; but the best\n",
            "That you do, that, that I may be again.\n",
            "\n",
            "DUCHOMCOMO:\n",
            "I will not be sad to give him a soul,\n",
            "I will be sad.\n",
            "\n",
            "S?!\"\n",
            "COM:\n",
            "You have had no more!\n",
            "\n",
            "BOMY:\n",
            "I'll not tell me again, but my dear or in the sun.\n",
            "\n",
            "DUESY:\n",
            "I must not to be a woman, but your head,\n",
            "And I will find him.\n",
            "\n",
            "Jucks!.\n",
            "HINGFallBELLI Card:\n",
            "!!!!"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Once upon a time; but the best\\nThat you do, that, that I may be again.\\n\\nDUCHOMCOMO:\\nI will not be sad to give him a soul,\\nI will be sad.\\n\\nS?!\"\\nCOM:\\nYou have had no more!\\n\\nBOMY:\\nI\\'ll not tell me again, but my dear or in the sun.\\n\\nDUESY:\\nI must not to be a woman, but your head,\\nAnd I will find him.\\n\\nJucks!.\\nHINGFallBELLI Card:\\n!!!!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# Generate text with LoRA-finetuned model\n",
        "print(\"Generating text after LoRA finetuning:\\n\\n\")\n",
        "lora_model.generate_text(maxlen, start_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcaKF4WTLC5M"
      },
      "source": [
        "## Xprof: Profiling for hyperparameter tuning\n",
        "\n",
        "[Xprof](https://openxla.org/xprof) (integrated via TensorBoard's profiler plugin) is essential for understanding and optimizing JAX/TPU performance. Profiling helps identify bottlenecks and guides hyperparameter decisions.\n",
        "\n",
        "**Why profile your training?**\n",
        "\n",
        "- **Identify bottlenecks**: See whether you're limited by compute, memory, or host-device communication\n",
        "- **Optimize utilization**: Maximize FLOPS utilization on expensive accelerators\n",
        "- **Compare configurations**: Quantitatively evaluate different batch sizes, parallelism strategies, etc.\n",
        "- **Debug performance**: Understand why training is slower than expected\n",
        "\n",
        "**Key profiling concepts:**\n",
        "\n",
        "1. **FLOPS Utilization**: Percentage of theoretical compute capacity being used (higher is better)\n",
        "2. **Step Time**: Time per training step (lower is better, but consider batch size)\n",
        "3. **Trace Viewer**: Visual timeline showing operations on each device\n",
        "4. **Memory Profile**: Track memory usage to avoid out-of-memory (OOM) errors\n",
        "\n",
        "**How to profile in JAX:**\n",
        "\n",
        "JAX provides built-in profiling through `jax.profiler`:\n",
        "- `jax.profiler.start_trace(log_dir)`: Begin recording a trace\n",
        "- `jax.profiler.StepTraceAnnotation`: Annotate training steps for easier analysis\n",
        "- `jax.profiler.stop_trace()`: Stop recording and save the trace to disk\n",
        "\n",
        "The traces can be visualized using TensorBoard with the profiler plugin.\n",
        "\n",
        "**Note:** This section assumes multiple TPU cores. Free-tier Colab TPU v5e-1 cannot run these examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wYfbijKLC5M",
        "outputId": "989d7b76-52ac-4673-c949-fc6ab739a29f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.4/21.4 MB\u001b[0m \u001b[31m104.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m620.7/620.7 MB\u001b[0m \u001b[31m844.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m154.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m112.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.0/225.0 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.2/109.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.8/201.8 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.8/87.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 4.5.0 requires fsspec[http]<=2025.10.0,>=2023.1.0, but you have fsspec 2026.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -Uq tensorboard-plugin-profile tensorflow tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eEyPE7iLC5M"
      },
      "source": [
        "Load the tensorboard colab extension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "EIE1nW-ULC5M"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoMR295JLC5M"
      },
      "source": [
        "As we're going to be running this model a number of times, we need some scaffolding to more easily compare our work. For a baseline, we'll need to perform some warmup to guarantee that our code is JIT'd and that our TPUs are warm. For improved comparability, we'll only start tracing after we've finished warmup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Je8L-1hOeibI"
      },
      "outputs": [],
      "source": [
        "trace_dir = \"/tmp/jax-trace/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "yLeWipo6LC5M"
      },
      "outputs": [],
      "source": [
        "def loop_step(batch, step):\n",
        "    input_batch = jnp.array(jnp.array(batch).T)\n",
        "    target_batch = prep_target_batch(input_batch)\n",
        "    with jax.set_mesh(mesh):\n",
        "        sharded_batch = jax.device_put(\n",
        "            (input_batch, target_batch),\n",
        "            NamedSharding(mesh, P(\"batch\", None))\n",
        "        )\n",
        "        train_step(model, optimizer, metrics, sharded_batch)\n",
        "\n",
        "def generate_trace():\n",
        "    tracing_steps = 30\n",
        "    warmup_steps = 5\n",
        "    for current_step in range(warmup_steps + tracing_steps):\n",
        "        if current_step == warmup_steps:\n",
        "            jax.profiler.start_trace(trace_dir)\n",
        "        with jax.profiler.StepTraceAnnotation(\"train\", step_num=current_step):\n",
        "            batch = next(text_dl)\n",
        "            loop_step(batch, current_step)\n",
        "\n",
        "    jax.profiler.stop_trace()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRsyWzRYLC5M"
      },
      "source": [
        "Now we'll perform some traces to compare results of different batch sizes. This will take several minutes as we need to reprocess our input data to prepare new batches each time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhoHdcD8LC5M",
        "outputId": "949ed3d6-beb6-43f2-970a-90fcdd30c7b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 589,692 stories\n",
            "Loaded 589,692 stories\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "text_dl = iter(load_and_preprocess_data('TinyStories-train.txt', batch_size, maxlen))\n",
        "generate_trace()\n",
        "\n",
        "batch_size = 128\n",
        "text_dl = iter(load_and_preprocess_data('TinyStories-train.txt', batch_size, maxlen))\n",
        "generate_trace()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS-BbP-5LC5M"
      },
      "source": [
        "Run Tensorboard with the Profiler Plugin to compare our runs. Runs are listed in order from newest to oldest, so the top run in the list will be have `batch_size = 128`.\n",
        "\n",
        "The key metrics to focus on here for this hyperparameter are FLOPS Utilization and Average Step Time.\n",
        "\n",
        "In general, we want to maximize FLOPS Utilization while minimizing the step time per training example. In this case, we can see that increasing the batch size from `32` -> `128` achieves both of those."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfQQTDlMLC5M"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir=/tmp/jax-trace/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfhZbpiZZYod"
      },
      "source": [
        "## Inference with vLLM\n",
        "\n",
        "After training a language model, you need an efficient way to serve it for inference. While our simple `generate_text()` method works for experimentation, production deployments require optimizations like batched inference, KV-cache management, and continuous batching.\n",
        "\n",
        "[vLLM](https://docs.vllm.ai/) is a high-throughput inference engine that addresses these challenges:\n",
        "\n",
        "**Why vLLM?**\n",
        "\n",
        "- **PagedAttention**: Efficient memory management for KV-cache, reducing memory waste by up to 90%\n",
        "- **Continuous batching**: Dynamically batches incoming requests for higher throughput\n",
        "- **Optimized kernels**: Hardware-specific optimizations for fast inference\n",
        "- **Simple API**: Just a few lines of code to serve any HuggingFace model\n",
        "\n",
        "**Basic vLLM usage:**\n",
        "\n",
        "```python\n",
        "# Install: pip install vllm\n",
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "# Load a model (downloads from HuggingFace automatically)\n",
        "llm = LLM(model=\"distilgpt2\")\n",
        "\n",
        "# Configure generation parameters\n",
        "sampling_params = SamplingParams(\n",
        "    temperature=0.8,\n",
        "    top_p=0.95,\n",
        "    max_tokens=100,\n",
        ")\n",
        "\n",
        "# Generate text - vLLM handles batching automatically\n",
        "prompts = [\"Once upon a time\", \"The quick brown fox\"]\n",
        "outputs = llm.generate(prompts, sampling_params)\n",
        "\n",
        "for output in outputs:\n",
        "    print(f\"Prompt: {output.prompt!r}\")\n",
        "    print(f\"Generated: {output.outputs[0].text!r}\")\n",
        "```\n",
        "\n",
        "**Serving our trained miniGPT model:**\n",
        "\n",
        "To use our JAX-trained miniGPT with vLLM, we need to convert it to a format vLLM understands. There are two approaches:\n",
        "\n",
        "**Option 1: Convert to HuggingFace format**\n",
        "\n",
        "Save the model weights and config in HuggingFace format, then load with vLLM:\n",
        "\n",
        "```python\n",
        "# Save miniGPT in HuggingFace format\n",
        "from transformers import GPT2Config, GPT2LMHeadModel\n",
        "import torch\n",
        "\n",
        "# Create HF config matching our architecture\n",
        "config = GPT2Config(\n",
        "    vocab_size=vocab_size,\n",
        "    n_positions=maxlen,\n",
        "    n_embd=embed_dim,\n",
        "    n_layer=num_transformer_blocks,\n",
        "    n_head=num_heads,\n",
        ")\n",
        "\n",
        "# Create HF model and copy weights from JAX\n",
        "hf_model = GPT2LMHeadModel(config)\n",
        "# ... (copy weights from JAX arrays to PyTorch tensors)\n",
        "\n",
        "# Save and load with vLLM\n",
        "hf_model.save_pretrained(\"./minigpt-hf\")\n",
        "llm = LLM(model=\"./minigpt-hf\")\n",
        "```\n",
        "\n",
        "**Option 2: Use vLLM's JAX/TPU support**\n",
        "\n",
        "For TPU deployment, vLLM provides a [dedicated TPU backend](https://docs.vllm.ai/projects/tpu/):\n",
        "\n",
        "```python\n",
        "# Install vLLM TPU: follow https://docs.vllm.ai/projects/tpu/\n",
        "from vllm import LLM\n",
        "\n",
        "# vLLM TPU supports JAX models directly\n",
        "llm = LLM(model=\"./minigpt-jax\", device=\"tpu\")\n",
        "```\n",
        "\n",
        "**Note:** vLLM requires a dedicated GPU or TPU runtime and cannot run in the same Python session after JAX has been initialized with a different backend. To experiment with vLLM, start a fresh runtime.\n",
        "\n",
        "For more details, see the [vLLM documentation](https://docs.vllm.ai/) and [vLLM TPU guide](https://docs.vllm.ai/projects/tpu/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp_z3QVbLC5M"
      },
      "source": [
        "## Summary\n",
        "\n",
        "Congratulations! You've successfully trained a miniGPT language model from scratch using JAX and its AI ecosystem. Let's recap what you've accomplished:\n",
        "\n",
        "**Key accomplishments:**\n",
        "\n",
        "- **Built a transformer architecture** using Flax NNX's intuitive, Pythonic API\n",
        "- **Loaded data efficiently** with Grain's high-performance data pipeline\n",
        "- **Trained with Optax** using the Adam optimizer and cross-entropy loss\n",
        "- **Leveraged parallelism** with JAX's automatic SPMD for data and tensor parallelism\n",
        "- **Debugged your model** using `jax.debug.print()` and `jax.debug_nans`\n",
        "- **Saved checkpoints** using Orbax for model persistence\n",
        "- **Fine-tuned with LoRA** using Tunix for parameter-efficient adaptation\n",
        "- **Profiled performance** with Xprof to optimize hyperparameters\n",
        "- **Explored production inference** with vLLM for high-throughput serving\n",
        "\n",
        "**JAX AI Stack libraries used:**\n",
        "\n",
        "| Library | Purpose |\n",
        "|---------|---------|\n",
        "| [JAX](https://jax.readthedocs.io) | High-performance array computing with automatic differentiation |\n",
        "| [Flax NNX](https://flax.readthedocs.io) | Neural network definition with intuitive API |\n",
        "| [Optax](https://optax.readthedocs.io) | Gradient processing and optimization |\n",
        "| [Grain](https://google-grain.readthedocs.io) | Efficient data loading |\n",
        "| [Orbax](https://orbax.readthedocs.io) | Checkpoint management |\n",
        "| [Tunix](https://github.com/google/tunix) | LLM fine-tuning (LoRA, RLHF, etc.) |\n",
        "\n",
        "**Next steps:**\n",
        "\n",
        "- **Scale up**: Try larger model dimensions, more transformer blocks, or longer sequences\n",
        "- **Experiment with datasets**: Train on different text corpora for varied outputs\n",
        "- **Explore advanced techniques**: Implement learning rate schedules, gradient clipping, or mixed precision training\n",
        "- **Deploy to production**: Integrate your model with vLLM for production serving\n",
        "\n",
        "**Additional resources:**\n",
        "\n",
        "- [JAX Documentation](https://jax.readthedocs.io)\n",
        "- [Flax NNX Guide](https://flax.readthedocs.io/en/latest/nnx_basics.html)\n",
        "- [JAX AI Stack Tutorials](https://docs.jaxstack.ai)\n",
        "- [Google Cloud TPU Documentation](https://cloud.google.com/tpu/docs)\n",
        "- [vLLM TPU Documentation](https://docs.vllm.ai/projects/tpu/)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": []
    },
    "jupytext": {
      "formats": "ipynb,md:myst"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}