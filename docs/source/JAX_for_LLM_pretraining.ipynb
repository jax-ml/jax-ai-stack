{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIOXoY1xgiww"
   },
   "source": [
    "# Train a miniGPT language model with JAX\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jax-ml/jax-ai-stack/blob/main/docs/source/JAX_for_LLM_pretraining.ipynb)\n",
    "\n",
    "This tutorial demonstrates how to use JAX, [Flax NNX](http://flax.readthedocs.io) and [Optax](http://optax.readthedocs.io) for language model (pre)training using data and tensor [parallelism](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization) for [Single-Program Multi-Data](https://en.wikipedia.org/wiki/Single_program,_multiple_data)). It was originally inspired by the [Keras miniGPT tutorial](https://keras.io/examples/generative/text_generation_with_miniature_gpt/).\n",
    "\n",
    "Here, you will learn how to:\n",
    "\n",
    "- Define the miniGPT model with Flax and JAX automatic parallelism\n",
    "- Load and preprocess the dataset\n",
    "- Create the loss and training step functions\n",
    "- Train the model on Google Colab’s Cloud TPU v2\n",
    "- Profile for hyperparameter tuning\n",
    "\n",
    "If you are new to JAX for AI, check out the [introductory tutorial](https://jax-ai-stack.readthedocs.io/en/latest/neural_net_basics.html), which covers neural network building with [Flax NNX](https://flax.readthedocs.io/en/latest/nnx_basics.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTmz5Cbco7n_"
   },
   "source": [
    "## Setup\n",
    "\n",
    "JAX installation is covered in [this guide](https://jax.readthedocs.io/en/latest/installation.html) on the JAX documentation site. We will use [Tiktoken](https://github.com/openai/tiktoken) for tokenization and [Grain](https://google-grain.readthedocs.io/en/latest/index.html) for data loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6zMsOIc7ouCO",
    "outputId": "ad486e3b-dd63-405f-d786-79b0b6d60cbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m0.9/1.2 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m478.8/478.8 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m110.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -Uq tiktoken grain matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rcji_799n4eA"
   },
   "source": [
    "**Note:** If you are using [Google Colab](https://colab.research.google.com/), select the free Google Cloud TPU v2 as the hardware accelerator.\n",
    "\n",
    "Check the available JAX devices, or [`jax.Device`](https://jax.readthedocs.io/en/latest/_autosummary/jax.Device.html), with [`jax.devices()`](https://jax.readthedocs.io/en/latest/_autosummary/jax.devices.html). The output of the cell below will show a list of 8 (eight) devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LS9sQEY3n0mB",
    "outputId": "c18a63d0-696e-4c93-f8d5-8c7942045005"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
       " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
       " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
       " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
       " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
       " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
       " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
       " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jax\n",
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHzJ_bokoovZ"
   },
   "source": [
    "Get the [TinyStories dataset from Hugging Face](https://huggingface.co/datasets/roneneldan/TinyStories). We only use the training split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wUjQsgQEmI1N",
    "outputId": "8a75571a-8339-4f1a-958b-5805aa285bb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-04-25 01:37:13--  https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStories-train.txt?download=true\n",
      "Resolving huggingface.co (huggingface.co)... 18.172.134.124, 18.172.134.4, 18.172.134.24, ...\n",
      "Connecting to huggingface.co (huggingface.co)|18.172.134.124|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs.hf.co/repos/42/7f/427f7497b6c6596c18b46d5a72e61364fcad12aa433c60a0dbd4d344477b9d81/c5cf5e22ff13614e830afbe61a99fbcbe8bcb7dd72252b989fa1117a368d401f?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27TinyStories-train.txt%3B+filename%3D%22TinyStories-train.txt%22%3B&response-content-type=text%2Fplain&Expires=1745548633&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NTU0ODYzM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy80Mi83Zi80MjdmNzQ5N2I2YzY1OTZjMThiNDZkNWE3MmU2MTM2NGZjYWQxMmFhNDMzYzYwYTBkYmQ0ZDM0NDQ3N2I5ZDgxL2M1Y2Y1ZTIyZmYxMzYxNGU4MzBhZmJlNjFhOTlmYmNiZThiY2I3ZGQ3MjI1MmI5ODlmYTExMTdhMzY4ZDQwMWY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=o3wAd1AJqh8HJqOoCSsN2UhG0CD18l4DvfzfbXQYuGu1ULd41OH67qYah6Gqa8UoiOR-vY2mL68PRKmR5xzN86-u0A-ONmHjeXfKuQj3JtAD2jJQ9Y1IvItldi8bW6yfpKMqtz8VKQ5iU%7EWsjkopTP%7EuLX%7EqbJleJ%7E2QoIeMfHQGSA-5ijnrTlJdjcsrlP-owPmmZ0xS8cWPFYLIFrL4Wi3JuddcN1AZDY9XraKobFVUrzzCgJBF5xgmRfGBejZbWtmm6VBhViB1m1CSoPSji5mrRlr1LclBFBmAcrkQ2QuPWjgYGBT7ONwmVSI0kjTO09z0%7E8mneFC3vQYJIaLItw__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
      "--2025-04-25 01:37:13--  https://cdn-lfs.hf.co/repos/42/7f/427f7497b6c6596c18b46d5a72e61364fcad12aa433c60a0dbd4d344477b9d81/c5cf5e22ff13614e830afbe61a99fbcbe8bcb7dd72252b989fa1117a368d401f?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27TinyStories-train.txt%3B+filename%3D%22TinyStories-train.txt%22%3B&response-content-type=text%2Fplain&Expires=1745548633&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NTU0ODYzM319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy80Mi83Zi80MjdmNzQ5N2I2YzY1OTZjMThiNDZkNWE3MmU2MTM2NGZjYWQxMmFhNDMzYzYwYTBkYmQ0ZDM0NDQ3N2I5ZDgxL2M1Y2Y1ZTIyZmYxMzYxNGU4MzBhZmJlNjFhOTlmYmNiZThiY2I3ZGQ3MjI1MmI5ODlmYTExMTdhMzY4ZDQwMWY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=o3wAd1AJqh8HJqOoCSsN2UhG0CD18l4DvfzfbXQYuGu1ULd41OH67qYah6Gqa8UoiOR-vY2mL68PRKmR5xzN86-u0A-ONmHjeXfKuQj3JtAD2jJQ9Y1IvItldi8bW6yfpKMqtz8VKQ5iU%7EWsjkopTP%7EuLX%7EqbJleJ%7E2QoIeMfHQGSA-5ijnrTlJdjcsrlP-owPmmZ0xS8cWPFYLIFrL4Wi3JuddcN1AZDY9XraKobFVUrzzCgJBF5xgmRfGBejZbWtmm6VBhViB1m1CSoPSji5mrRlr1LclBFBmAcrkQ2QuPWjgYGBT7ONwmVSI0kjTO09z0%7E8mneFC3vQYJIaLItw__&Key-Pair-Id=K3RPWS32NSSJCE\n",
      "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 3.167.152.106, 3.167.152.37, 3.167.152.12, ...\n",
      "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|3.167.152.106|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1924281556 (1.8G) [text/plain]\n",
      "Saving to: ‘TinyStories-train.txt’\n",
      "\n",
      "TinyStories-train.t 100%[===================>]   1.79G   259MB/s    in 7.3s    \n",
      "\n",
      "2025-04-25 01:37:20 (251 MB/s) - ‘TinyStories-train.txt’ saved [1924281556/1924281556]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/datasets/roneneldan/TinyStories/resolve/main/TinyStories-train.txt?download=true -O TinyStories-train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKE2uUafLobI"
   },
   "source": [
    "Import the necessary modules, including JAX NumPy, Flax NNX, Optax, Grain, pandas, and Tiktoken:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "MKYFNOhdLq98"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from jax.sharding import Mesh, PartitionSpec as P, NamedSharding # For data and model parallelism (explained in more detail later)\n",
    "from jax.experimental import mesh_utils\n",
    "\n",
    "import flax.nnx as nnx\n",
    "import optax\n",
    "\n",
    "from dataclasses import dataclass\n",
    "import grain.python as pygrain\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPyt7MV6prz1"
   },
   "source": [
    "## Define the miniGPT model with Flax and JAX automatic parallelism\n",
    "\n",
    "### Leveraging JAX's data and tensor parallelism\n",
    "\n",
    "One of the most powerful features of JAX is [device parallelism](https://jax.readthedocs.io/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization) for SPMD.\n",
    "\n",
    "- The data parallelism technique enables, for example, the training data to run via multiple parts (this is called sharding) - batches - in parallel and simultaneously across different devices, such as GPUs and Google TPUs. This allows to use larger batch sizes to speed up training.\n",
    "- Tensor parallelism allows us to split the model parameter tensors across several devices (sharding model tensors).\n",
    "- You can learn more about the basics of JAX parallelism in more detail in the [Introduction to parallel programming](https://jax.readthedocs.io/en/latest/sharded-computation.html) on the JAX documentation site.\n",
    "\n",
    "In this example, we'll utilize a 4-way data parallel and 2-way tensor parallel setup. The free Google Cloud TPU v2 on Google Colab offers 4 chips, each with 2 TPU cores. The TPU v2 architeture aligns with the proposed setup.\n",
    "\n",
    "### jax.sharding.Mesh\n",
    "\n",
    "Earlier, we imported [`jax.sharding.Mesh`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.Mesh) - is a multidimensional NumPy array of JAX devices, where each axis of the mesh has a name, such as `'x'` or `'y'`. This will help encapsulate the information about the TPU resource organization for distributing computations across the devices.\n",
    "\n",
    "Our `Mesh` will have two arguments:\n",
    "- `devices`: This will take the value of [`jax.experimental.mesh_utils((4, 2))`](https://jax.readthedocs.io/en/latest/jax.experimental.mesh_utils.html), enabling us to build a device mesh. It is a NumPy ndarray with JAX devices (a list of devices from the JAX backend as obtained from [`jax.devices()`](https://jax.readthedocs.io/en/latest/_autosummary/jax.devices.html#jax.devices))..\n",
    "- `axis_names`, where:\n",
    "  - `batch`: 4 devices along the first axis - i.e. sharded into 4 - for data parallelism; and\n",
    "  - `model`: 2 devices along the second axis - i.e. sharded into 2 -  for tensor paralleism, mapping to the TPU v2 cores.\n",
    "\n",
    "This matches the `(4, 2)` structure in the Colab's TPU v2 setup.\n",
    "\n",
    "Let's instantiate `Mesh` as `mesh` and declare the TPU configuration to define how data and model parameters are distributed across the devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "xuMlCK3Q8WJD"
   },
   "outputs": [],
   "source": [
    "# Create a `Mesh` object representing TPU device arrangement.\n",
    "mesh = Mesh(mesh_utils.create_device_mesh((4, 2)), ('batch', 'model'))\n",
    "\n",
    "### Alternatively, we could use the 8-way data parallelism with only one line of code change.\n",
    "### JAX enables quick experimentation with different partitioning strategies\n",
    "### like this. We will come back to this point at the end of this tutorial.\n",
    "# mesh = Mesh(mesh_utils.create_device_mesh((8, 1)), ('batch', 'model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZKdhNo98NgG"
   },
   "source": [
    "We will use the GPT-2 tokenizer from the [Tiktoken](https://github.com/openai/tiktoken) library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "iWbkk1V7-Isg"
   },
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XHQ0BQ9-KIj"
   },
   "source": [
    "To leverage model parallelism, we need to instruct the JAX compiler how to shard the model tensors across the TPU devices. Earlier, we also imported [`jax.sharding.PartitionSpec`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.PartitionSpec) and [`jax.sharding.NamedSharding`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.NamedSharding):\n",
    "- [`PartitionSpec`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.PartitionSpec) (using alias `P`) defines how tensors are sharded across the devices in our `Mesh`. Its elements describe how an input dimension is partitioned across mesh dimensions. For example, in `PartitionSpec('x', 'y')` the first dimension of data is sharded across `x` axis of the mesh, and the second one - across the `y` axis.\n",
    "  - We'll use `PartitionSpec` to describe how to shard a tensor across, for example, the `model` axis or be replicated on other dimensions (which is denoted by `None`).\n",
    "- [`NamedSharding`](https://jax.readthedocs.io/en/latest/jax.sharding.html#jax.sharding.NamedSharding) is a (`Mesh`, `PartitionSpec`) pair that describes how to shard a model tensor across our `mesh`.\n",
    "- We combine `Mesh` (the TPU resources) with `PartitionSpec` and create a `NamedSharding`, which instructs how to shard each model tensor across the TPU devices.\n",
    "\n",
    "Additionally, we'll use Flax NNX's [`flax.nnx.with_partitioning`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/spmd.html#flax.nnx.with_partitioning) to let each model layer know that the model weights or tensors need to be sharded according to our specification. We need to do this for every tensor/layer in the model.\n",
    "- `nnx.with_partitioning` will take two arguments, such as the `initializer` (such as [`flax.nnx.initializers.xavier_uniform`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/initializers.html#flax.nnx.initializers.xavier_uniform) and [`flax.nnx.initializers.zeros_init`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/initializers.html#flax.nnx.initializers.zeros_init)) and `sharding` (e.g. `NamedSharding(Mesh, PartitionSpec)` or `NamedSharding(mesh, P('model')` in our case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "z0p-IHurrB9i"
   },
   "outputs": [],
   "source": [
    "# Define a triangular mask for causal attention with `jax.numpy.tril` and `jax.numpy.ones`.\n",
    "def causal_attention_mask(seq_len):\n",
    "    return jnp.tril(jnp.ones((seq_len, seq_len)))\n",
    "\n",
    "class TransformerBlock(nnx.Module):\n",
    "    \"\"\" A single Transformer block.\n",
    "\n",
    "    Each Transformer block processes input sequences via self-attention and feed-forward networks.\n",
    "\n",
    "    Args:\n",
    "        embed_dim (int): Embedding dimensionality.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        ff_dim (int): Dimensionality of the feed-forward network.\n",
    "        rngs (flax.nnx.Rngs): A Flax NNX stream of JAX PRNG keys.\n",
    "        rate (float): Dropout rate. Defaults to 0.1.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, *, rngs: nnx.Rngs, rate: float = 0.1):\n",
    "        # Multi-Head Attention (MHA) with `flax.nnx.MultiHeadAttention`.\n",
    "        # Specifies tensor sharding (depending on the mesh configuration)\n",
    "        # where we shard the weights across devices for parallel computation.\n",
    "        self.mha = nnx.MultiHeadAttention(num_heads=num_heads,\n",
    "                                          in_features=embed_dim,\n",
    "                                          kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
    "                                          bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
    "                                          rngs=rngs)\n",
    "        # The first dropout with `flax.nnx.Dropout`.\n",
    "        self.dropout1 = nnx.Dropout(rate=rate)\n",
    "        # First layer normalization with `flax.nnx.LayerNorm`.\n",
    "        self.layer_norm1 = nnx.LayerNorm(epsilon=1e-6,\n",
    "                                         num_features=embed_dim,\n",
    "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), NamedSharding(mesh, P('model'))),\n",
    "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
    "                                         rngs=rngs)\n",
    "        # The first linear transformation for the feed-forward network with `flax.nnx.Linear`.\n",
    "        self.linear1 = nnx.Linear(in_features=embed_dim,\n",
    "                                  out_features=ff_dim,\n",
    "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
    "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
    "                                  rngs=rngs)\n",
    "        # The second linear transformation for the feed-forward network with `flax.nnx.Linear`.\n",
    "        self.linear2 = nnx.Linear(in_features=ff_dim,\n",
    "                                  out_features=embed_dim,\n",
    "                                  kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
    "                                  bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),\n",
    "                                  rngs=rngs)\n",
    "        # The second dropout with `flax.nnx.Dropout`.\n",
    "        self.dropout2 = nnx.Dropout(rate=rate)\n",
    "        # Second layer normalization with `flax.nnx.LayerNorm`.\n",
    "        self.layer_norm2 = nnx.LayerNorm(epsilon=1e-6,\n",
    "                                         num_features=embed_dim,\n",
    "                                         scale_init=nnx.with_partitioning(nnx.initializers.ones_init(), NamedSharding(mesh, P(None, 'model'))),\n",
    "                                         bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P(None, 'model'))),\n",
    "                                         rngs=rngs)\n",
    "\n",
    "\n",
    "    # Apply the Transformer block to the input sequence.\n",
    "    def __call__(self, inputs, training: bool = False):\n",
    "        input_shape = inputs.shape\n",
    "        _, seq_len, _ = input_shape\n",
    "\n",
    "        # Instantiate the causal attention mask.\n",
    "        mask = causal_attention_mask(seq_len)\n",
    "\n",
    "        # Apply Multi-Head Attention with the causal attention mask.\n",
    "        attention_output = self.mha(\n",
    "            inputs_q=inputs,\n",
    "            mask=mask,\n",
    "            decode=False\n",
    "        )\n",
    "        # Apply the first dropout.\n",
    "        attention_output = self.dropout1(attention_output, deterministic=not training)\n",
    "        # Apply the first layer normalization.\n",
    "        out1 = self.layer_norm1(inputs + attention_output)\n",
    "\n",
    "        # The feed-forward network.\n",
    "        # Apply the first linear transformation.\n",
    "        ffn_output = self.linear1(out1)\n",
    "        # Apply the ReLU activation with `flax.nnx.relu`.\n",
    "        ffn_output = nnx.relu(ffn_output)\n",
    "        # Apply the second linear transformation.\n",
    "        ffn_output = self.linear2(ffn_output)\n",
    "        # Apply the second dropout.\n",
    "        ffn_output = self.dropout2(ffn_output, deterministic=not training)\n",
    "        # Apply the second layer normalization and return the output of the Transformer block.\n",
    "        return self.layer_norm2(out1 + ffn_output)\n",
    "\n",
    "class TokenAndPositionEmbedding(nnx.Module):\n",
    "    \"\"\" Combines token embeddings (words in an input sentence) with\n",
    "    positional embeddings (the position of each word in a sentence).\n",
    "\n",
    "    Args:\n",
    "        maxlen (int): Matimum sequence length.\n",
    "        vocal_size (int): Vocabulary size.\n",
    "        embed_dim (int): Embedding dimensionality.\n",
    "        rngs (flax.nnx.Rngs): A Flax NNX stream of JAX PRNG keys.\n",
    "    \"\"\"\n",
    "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, *, rngs: nnx.Rngs):\n",
    "        # Initialize token embeddings (using `flax.nnx.Embed`).\n",
    "        # Each unique word has an embedding vector.\n",
    "        self.token_emb = nnx.Embed(num_embeddings=vocab_size, features=embed_dim, rngs=rngs)\n",
    "        # Initialize positional embeddings (using `flax.nnx.Embed`).\n",
    "        self.pos_emb = nnx.Embed(num_embeddings=maxlen, features=embed_dim, rngs=rngs)\n",
    "\n",
    "    # Takes a token sequence (integers) and returns the combined token and positional embeddings.\n",
    "    def __call__(self, x):\n",
    "        # Generate a sequence of positions for the input tokens.\n",
    "        positions = jnp.arange(0, x.shape[1])[None, :]\n",
    "        # Look up the positional embeddings for each position in the input sequence.\n",
    "        position_embedding = self.pos_emb(positions)\n",
    "        # Look up the token embeddings for each token in the input sequence.\n",
    "        token_embedding = self.token_emb(x)\n",
    "        # Combine token and positional embeddings.\n",
    "        return token_embedding + position_embedding\n",
    "\n",
    "class MiniGPT(nnx.Module):\n",
    "    \"\"\" A miniGPT transformer model, inherits from `flax.nnx.Module`.\n",
    "\n",
    "    Args:\n",
    "        maxlen (int): Maximum sequence length.\n",
    "        vocab_size (int): Vocabulary size.\n",
    "        embed_dim (int): Embedding dimensionality.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        feed_forward_dim (int): Dimensionality of the feed-forward network.\n",
    "        num_transformer_blocks (int): Number of transformer blocks. Each block contains attention and feed-forward networks.\n",
    "        rngs (nnx.Rngs): A Flax NNX stream of JAX PRNG keys.\n",
    "    \"\"\"\n",
    "    # Initialize miniGPT model components.\n",
    "    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, num_heads: int, feed_forward_dim: int, num_transformer_blocks: int, rngs: nnx.Rngs):\n",
    "        # Initiliaze the `TokenAndPositionEmbedding` that combines token and positional embeddings.\n",
    "        self.embedding_layer = TokenAndPositionEmbedding(\n",
    "                    maxlen, vocab_size, embed_dim, rngs=rngs\n",
    "                )\n",
    "        # Create a list of `TransformerBlock` instances.\n",
    "        # Each block processes input sequences using attention and feed-forward networks.\n",
    "        self.transformer_blocks = [TransformerBlock(\n",
    "            embed_dim, num_heads, feed_forward_dim, rngs=rngs\n",
    "        ) for _ in range(num_transformer_blocks)]\n",
    "        # Initialize the output `flax.nnx.Linear` layer producing logits over the vocabulary for next-token prediction.\n",
    "        self.output_layer = nnx.Linear(in_features=embed_dim,\n",
    "                                       out_features=vocab_size,\n",
    "                                       kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),\n",
    "                                       bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P(None, 'model'))),\n",
    "                                       rngs=rngs)\n",
    "\n",
    "    def __call__(self, inputs, training: bool = False):\n",
    "        # Pass the input tokens through the `embedding_layer` to get token embeddings.\n",
    "        # Apply each transformer block sequentially to the embedded input, use the `training` flag for the behavior of `flax.nnx.Dropout`.\n",
    "        x = self.embedding_layer(inputs)\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x, training=training)\n",
    "        # Pass the output of the transformer blocks through the output layer,\n",
    "        # and obtain logits for each token in the vocabulary (for next token prediction).\n",
    "        outputs = self.output_layer(x)\n",
    "        return outputs\n",
    "\n",
    "    @nnx.jit\n",
    "    def sample_from(self, logits):\n",
    "        logits, indices = jax.lax.top_k(logits, k=top_k)\n",
    "        logits = nnx.softmax(logits)\n",
    "        return jax.random.choice(jax.random.PRNGKey(0), indices, p=logits)\n",
    "\n",
    "    @nnx.jit\n",
    "    def generate_step(self, padded_tokens, sample_index):\n",
    "        logits = self(padded_tokens)\n",
    "        next_token = self.sample_from(logits[0][sample_index])\n",
    "        return next_token\n",
    "\n",
    "    def generate_text(self, max_tokens, start_tokens):\n",
    "        generated = []\n",
    "        print(tokenizer.decode(start_tokens), flush=True, end='')\n",
    "        for i in range(max_tokens):\n",
    "            sample_index = len(start_tokens) + len(generated) - 1\n",
    "\n",
    "            padded_tokens = jnp.array((start_tokens + generated + [0] * (maxlen - len(start_tokens) - len(generated))))[None, :]\n",
    "            next_token = int(self.generate_step(padded_tokens, sample_index))\n",
    "            if next_token == tokenizer.encode('<|endoftext|>', allowed_special={'<|endoftext|>'})[0]:\n",
    "              break\n",
    "            generated.append(next_token)\n",
    "            # decode and print next_token\n",
    "            print(tokenizer.decode([next_token]), flush=True, end='')\n",
    "        return tokenizer.decode(start_tokens + generated)\n",
    "\n",
    "# Creates the miniGPT model with 4 transformer blocks.\n",
    "def create_model(rngs):\n",
    "    return MiniGPT(maxlen, vocab_size, embed_dim, num_heads, feed_forward_dim, num_transformer_blocks=4, rngs=rngs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igX_eoGNMTGR"
   },
   "source": [
    "Set some hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GRhiDsCrMZRp"
   },
   "outputs": [],
   "source": [
    "vocab_size = tokenizer.n_vocab\n",
    "num_transformer_blocks = 8\n",
    "maxlen = 256\n",
    "embed_dim = 256\n",
    "num_heads = 8\n",
    "feed_forward_dim = 256\n",
    "batch_size = 256 # You can set a bigger batch size if you use Kaggle's Cloud TPU.\n",
    "num_epochs = 1\n",
    "top_k = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mI1ci-HyMspJ"
   },
   "source": [
    "## Loading and preprocessing the data\n",
    "\n",
    "Data loading and preprocessing with [Grain](https://github.com/google/grain)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "rGUFsn1GMuzh"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TextDataset:\n",
    "    data: list\n",
    "    maxlen: int\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        # Use Tiktoken for tokenization\n",
    "        encoding = tokenizer.encode(self.data[idx], allowed_special={'<|endoftext|>'})[:self.maxlen]  # Tokenize and truncate\n",
    "        return encoding + [0] * (self.maxlen - len(encoding))  # Pad to maxlen\n",
    "\n",
    "def load_and_preprocess_data(file_path, batch_size, maxlen):\n",
    "\n",
    "    with open(file_path, 'r') as f:\n",
    "      text = f.read()\n",
    "\n",
    "    stories = text.split('<|endoftext|>')\n",
    "    stories = [story+'<|endoftext|>' for story in stories if story.strip()]\n",
    "    df = pd.DataFrame({'text': stories})\n",
    "    data = df['text'].dropna().tolist()\n",
    "    dataset = TextDataset(data, maxlen)\n",
    "\n",
    "    sampler = pygrain.IndexSampler(\n",
    "        len(dataset),\n",
    "        shuffle=False,\n",
    "        seed=42,\n",
    "        shard_options=pygrain.NoSharding(),\n",
    "        num_epochs=num_epochs,\n",
    "    )\n",
    "\n",
    "    dl = pygrain.DataLoader(\n",
    "        data_source=dataset,\n",
    "        sampler=sampler,\n",
    "        operations=[pygrain.Batch(batch_size=batch_size, drop_remainder=True)],\n",
    "    )\n",
    "\n",
    "    return dl\n",
    "\n",
    "text_dl = load_and_preprocess_data('TinyStories-train.txt', batch_size, maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKVSD8KSM1um"
   },
   "source": [
    "## Defining the loss function and training step function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "8rRuTmABNV4b"
   },
   "outputs": [],
   "source": [
    "# Defines the loss function using `optax.softmax_cross_entropy_with_integer_labels`.\n",
    "def loss_fn(model, batch):\n",
    "    logits = model(batch[0])\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(logits=logits, labels=batch[1]).mean()\n",
    "    return loss, logits\n",
    "\n",
    "# Define the training step with the `flax.nnx.jit` transformation decorator.\n",
    "@nnx.jit\n",
    "def train_step(model: MiniGPT, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch):\n",
    "    grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, logits), grads = grad_fn(model, batch)\n",
    "    metrics.update(loss=loss, logits=logits, lables=batch[1])\n",
    "    optimizer.update(grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5um2vkeUNckm"
   },
   "source": [
    "## Training the model\n",
    "\n",
    "Start training. It takes ~50 minutes on Colab.\n",
    "\n",
    "Note that for data parallel, we are sharding the training data along the `batch` axis using `jax.device_put` with `NamedSharding`.\n",
    "\n",
    "We are also using the `jax.vmap` transformation to produce the target sequences faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ysl6CsfENeJN",
    "outputId": "b713cc63-a4ff-4fea-96ea-c234348e0ea4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial generated text:\n",
      "Once upon a timeaciaGender gearuser Analysisval {} Bruce Lauren helic Lauren Bruce againstliterally SQU retire Path {}valascript northwest {} Bruceuit Pathascript northwestdrops freelyvic996 curated hysteria survivor {}sclaxteradvert Sitting qualifiers snack {} scenariovalameron {} Path {}Nick VeganExcept peasantascript Whites retire {} retire {} Analysisrest {} Mine psychedelic flankForgeModLoader Path Bravo {} inflic {} strutConnector psychedelic beyond Beforeocker interesting Dani {}sclaxter retire {}Nick sorrow Typesrest interestingUV FSyrus resorts {} Dani {} perished {} retire interesting sorrow reversibleurned {} Womanlast 118 reass gentlestudyManager {} retire {} verb Captain forbid Bruce {} Analysis ox {} inexplicable tumor psychedelic {} serverpelrest Sky {} cropDisclaimeruti Nortonocated twins Path {} psychedeliccre motionsundrum {} northwestroid variable {} Whites {} dancers iPod {} {} verb retire {} Fred Noble {}ampionscre lineman servesShould decision1024� serveraez {} retire interesting Tangrest Carly juice,. allowsmodulerest Antarsumameroncre Flesh --> northwestroidENN {} Gustav rolledMuch challengundrum {}val retire {}scl less {} perished Brigham Analysis developersSomething hiding {}scl Houthval {} northwest appease miles {} escalationManager {} northwest {} {} Cube psychedelic {} inflic {} retire {} Whites dancers {}scl FS lore appease Din {} Whites abnorm[] {} {}scl FS appease dangling Bruce abnormcre97 psychedeliccre!!!\n",
      "\n",
      "Step 200, Loss: 4.653054714202881, Elapsed Time: 100.71 seconds\n",
      "Generated text:\n",
      "Once upon a time, there a little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little little!!!!\n",
      "\n",
      "Step 400, Loss: 3.0780816078186035, Elapsed Time: 59.71 seconds\n",
      "Generated text:\n",
      "Once upon a time there was a little girl named Lily. She loved to play outside and play with her mommy was very much fun. She loved to play with her mommy and her mommy's mommy's mommy said, but she went to her mommy's mommy's mommy's mommy said, \"I'm going to play with her mommy.\n",
      "Lily said, \"I'm going to the park. She said, \"I'm going to play with her mommy said, \"I'm going to play with you want to play with you can't want to play with her mommy and said, \"I'm so happy to play with her mommy and said, \"I'm sorry, \"I'm sorry, \"I'm sorry, I can't worry, \"I'm sorry, I can't want to be a good.\"\n",
      "\n",
      "\n",
      "Step 600, Loss: 2.4993953704833984, Elapsed Time: 31.43 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lily. She loved to play with her toys and play with her toys. One day, she went to the park with her mommy and daddy.\n",
      "\"Let's go to the park!\" Lily asked.\n",
      "\"Let's go to the park,\" her mommy said.\n",
      "\"No, we can't have to go to the park,\" said.\n",
      "\"Sure, Lily.\n",
      "\"Sure, I want to play with you!\" Lily said.\n",
      "\"Yes, I'm going to play with you!\" Lily said.\n",
      "\"Yes, I want to play with you!\" Lily said.\n",
      "Lily and Lily went to the park. She saw a big dog. She was so happy and had a lot of fun.\n",
      "\"Wow, Lily, Lily, Lily, Lily, Lily!\" she said.\n",
      "\"I'm sorry, Lily. \"I'm sorry, Lily. I'm sorry, Lily. I can't have a good friend.\"\n",
      "Lily and Lily said, \"Yes, Lily. I can't like to play with you.\"\n",
      "Lily and Lily and Lily and Lily went to play with her mommy. They played together and Lily played together. They played together and played!!!!\n",
      "\n",
      "Step 800, Loss: 2.1457183361053467, Elapsed Time: 32.42 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lily. She loved to play with her toys and her toys. One day, she went to the park with her mommy and dad. She saw a big dog named Max. Lily was scared and ran to Max.\n",
      "\"Hi, Lily! What's wrong?\" asked Lily.\n",
      "\"I'm sorry, but I saw a big dog. I found a big dog. I found a big dog. I found a big dog and Lily said, \"Don't worry, I will help you.\"\n",
      "Lily was happy and said, \"Thank you, Max. You are a good friend.\"\n",
      "Lily felt happy and said, \"Thank you, Lily. You are a good friend.\"\n",
      "\n",
      "\n",
      "Step 1000, Loss: 1.9495660066604614, Elapsed Time: 31.16 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lily. She loved to play with her toys and her toys. One day, she found a big box in the box. She was very happy and wanted to show her mom her mom.\n",
      "Lily was very happy. She said, \"Mom, can I have a letter?\"\n",
      "Her mom said, \"Sure, but you can't find a letter. It's a letter. It's a letter.\" Lily was so happy and thanked her mom.\n",
      "After they finished, Lily's mom said, \"Thank you, Lily. It's a letter.\" Lily was so happy to see her mom. She said, \"Thank you, Lily. I love you, my letter.\"\n",
      "Lily was happy to have a new letter. She said, \"Thank you, Lily. I love you, my letter!\"\n",
      "\n",
      "\n",
      "Step 1200, Loss: 1.8355252742767334, Elapsed Time: 31.50 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lily. She loved to play with her toys and play with her toys. One day, she found a big box in her room. She was so happy and wanted to play with it.\n",
      "Lily's mom said, \"Lily, you can play with your toys. It's a toy car.\" Lily was so happy and said, \"Thank you, Lily. I am so happy to have her toy car.\"\n",
      "Lily was happy and said, \"Thank you, Lily. I love you, my car is my car.\"\n",
      "Lily was happy and said, \"Thank you, Lily. I love you, Lily. I love you too!\"\n",
      "\n",
      "\n",
      "Step 1400, Loss: 1.7667107582092285, Elapsed Time: 31.08 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lily. She loved to play outside in the sunshine. One day, she saw a big red ball on the ground. She wanted to play with it, but her mom said no.\n",
      "Lily went to the park and saw a big tree. She wanted to climb it. She climbed up and down the tree. She climbed up and down. She climbed up the tree and climbed up. She felt the wind on her face. She felt so happy and free.\n",
      "But then, she heard a loud noise. It was coming from the tree. She was scared and ran away. She ran back to her mom and told her to go back to the tree. Her mom said she had to go to the tree and play on the swings. She was so happy that she had found a new friend. She was so happy to have found her new friend.\n",
      "\n",
      "\n",
      "Step 1600, Loss: 1.7037931680679321, Elapsed Time: 31.64 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lily. She loved to play with her toys and play with her toys. One day, she went to the park with her mommy and daddy. She saw a big dog and wanted to pet it. She asked her mommy, \"Can I pet the dog, please?\" Her mommy said, \"No, you can't. I want to pet the dog.\"\n",
      "Lily was sad and didn't want to be sad. She wanted to pet the dog, but she was not happy. She said, \"No, you have to be naughty and not to play with the dog. You have to be kind and gentle. You have to be kind and gentle and gentle.\"\n",
      "The dog was happy and said, \"Thank you, Lily. You are a good dog. I'm sorry for you. I'm sorry for the dog.\"\n",
      "Lily felt bad and said, \"I'm sorry, but I didn't mean to hurt you. I'm sorry for the dog. I didn't mean to hurt you. I'm sorry for the dog. I was mean to be rude and not to hurt you. I'm sorry for you. I'm sorry for you. I'm sorry to forgive you!!!!\n",
      "\n",
      "Step 1800, Loss: 1.661496639251709, Elapsed Time: 32.36 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lucy. She was very excited to go on a tour with her mommy and daddy.\n",
      "When they arrived at the park, Lucy saw a big tree with lots of leaves. She wanted to climb it and see what was on the tree.\n",
      "Lucy climbed the tree and climbed the tree. She climbed up the tree and climbed the tree. She climbed higher and higher until she reached the top.\n",
      "When Lucy reached the top, she saw a big, scary dog. The dog was barking loudly and Lucy was scared. She ran away, but the dog was too fast.\n",
      "Lucy was scared and ran away. She tried to get back, but she couldn't. She was stuck in the tree, but she was too scared. She tried to get back, but the dog was too fast.\n",
      "The dog was too fast and Lucy was scared. She tried to run away, but the dog was too fast. The dog was too fast and Lucy was too scared to get back. She was safe and happy.\n",
      "\n",
      "\n",
      "Step 2000, Loss: 1.635001301765442, Elapsed Time: 32.21 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lucy. She was three years old and loved to play with her toys. One day, Lucy went to the park with her mom. She saw a big tree with lots of leaves. Lucy wanted to climb it.\n",
      "Lucy asked her mom, \"Mommy, can I climb the tree?\" Her mom said, \"Yes, but you can climb the tree. It's too high up.\" Lucy was so excited. She climbed up the tree and climbed up. She climbed higher and higher until she reached the top.\n",
      "When she got to the top, she saw a big tree with a big tree. She climbed up the tree and climbed up. She climbed up the tree and climbed up. She climbed up the tree and climbed up. She climbed higher and higher, higher and higher until she was almost up.\n",
      "When she was done, Lucy was so happy. She had a great adventure and was happy to have helped her mom.\n",
      "\n",
      "\n",
      "Step 2200, Loss: 1.5896366834640503, Elapsed Time: 32.09 seconds\n",
      "Generated text:\n",
      "Once upon a time there was a little girl named Lucy. She was very happy and loved to play with her toys. One day, she found a big box in the box. She was so excited to open it.\n",
      "Lucy opened the box and found a big box. Inside the box was a toy car. She was so happy and played with it all day long.\n",
      "But then, Lucy accidentally knocked over the box. She was very sad. She wanted to see what was inside the box.\n",
      "Lucy's mom saw her and said, \"Lucy, you can't open the box. It's a toy car.\" Lucy was so happy and said, \"Thank you, Mom. I'm so glad I could help.\"\n",
      "\n",
      "\n",
      "Step 2400, Loss: 1.5428193807601929, Elapsed Time: 31.43 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Sarah. She was three years old and loved to play outside. One day, Sarah went to the park to play. She saw a big slide and wanted to go on it. She climbed up and slid down the slide. She was so happy to see the slide.\n",
      "But then, Sarah saw a big dog. She was scared and ran away. She ran to her mom and said, \"Mommy, look! It's so big!\" Her mom smiled and said, \"Yes, it's okay. We can go on the slide again.\"\n",
      "Sarah was so happy and said, \"Thank you, Mommy! I'm so glad I could go to the slide again!\" Her mom smiled and said, \"Me too! Let's go!\"\n",
      "\n",
      "\n",
      "Step 2600, Loss: 1.5633305311203003, Elapsed Time: 31.28 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lucy. She was very curious and loved to explore. One day, she found a big box in the attic. She was curious and wanted to see what was inside.\n",
      "Lucy opened the box and saw many things inside. She was curious and wanted to see what was inside. She opened the box and saw many things inside. She saw a big, scary monster.\n",
      "Lucy was scared and ran away. She hid behind the box and waited for the monster to come back. The monster was hiding behind the box and the box was gone. Lucy was sad and scared. She wanted to go back to the box and play with the monster.\n",
      "The monster was very angry and scared. It ran away from the box and Lucy never came back. The end.\n",
      "\n",
      "\n",
      "Step 2800, Loss: 1.541506052017212, Elapsed Time: 31.95 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lucy. She was very happy and loved to play with her friends. One day, Lucy's mom said, \"Lucy, you have to go to the park today. It's time for lunch.\" Lucy was sad and didn't want to go to the park.\n",
      "But then, a big dog came and started to play. Lucy's mom saw the dog and said, \"Don't worry, Lucy. We can go to the park and play.\" Lucy was so happy and ran to the dog. She ran and ran, but the dog was too fast. Lucy was scared and ran away.\n",
      "The next day, Lucy and her mom went back to the park. They saw a big slide and ran to it. Lucy was so happy and said, \"Thank you, mom! You're the best!\" The dog smiled and said, \"You're welcome, Lucy. I'm glad you're safe.\" Lucy smiled and said, \"I'm glad you're safe.\"\n",
      "\n",
      "\n",
      "Step 3000, Loss: 1.538221001625061, Elapsed Time: 31.71 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lucy. She was three years old and she loved to play with her toys. One day, Lucy's mommy said, \"Lucy, I want to play with your toys.\" Lucy was very excited and said, \"Yes, please!\"\n",
      "So, Lucy and her mommy went to the store to buy some candy. Lucy was so happy and said, \"Thank you, Mommy!\" Her mommy said, \"You're welcome, Lucy. I'm glad you like candy.\" Lucy smiled and said, \"I'm glad you like it.\"\n",
      "Lucy's mommy said, \"That's a great idea, Lucy. Let's go home and have some fun together.\" Lucy was so happy and said, \"Yay! I love playing with my toys!\"\n",
      "\n",
      "\n",
      "Step 3200, Loss: 1.4795030355453491, Elapsed Time: 31.69 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lucy. She was three years old and loved to play with her toys. One day, Lucy's mom said, \"Lucy, let's go for a walk!\" Lucy was so excited. She ran to the park and saw a big, shiny rock. She picked it up and showed it to her mom.\n",
      "\"Look, Mommy! I found a rock!\" said Lucy. \"It's so pretty!\"\n",
      "Her mom smiled and said, \"That's a great rock, Lucy. It's a pretty rock. It's a pretty rock.\"\n",
      "Lucy put the rock on the rock and rocked back and forth. She felt happy and proud. She had found a new rock and showed it to her mom.\n",
      "\n",
      "\n",
      "Step 3400, Loss: 1.510359764099121, Elapsed Time: 31.32 seconds\n",
      "Generated text:\n",
      "Once upon a time there was a little girl named Lucy. She was very excited because she was going to the park. She was going to the park and she saw a big slide. She wanted to go on the slide, but she was too scared to go down.\n",
      "Suddenly, Lucy saw a big, scary monster. It was scary and Lucy was scared. She ran to her mom and said, \"Mom, what is that scary monster in the park?\"\n",
      "Her mom smiled and said, \"That's a monster, Lucy. It's just a big, scary monster.\"\n",
      "The monster said, \"I'm scared of the monster. It's just a monster.\"\n",
      "Lucy was scared, but she was brave. She said, \"Don't worry, I'll help you.\"\n",
      "The monster was friendly and Lucy was happy to hear the monster. She said, \"Thank you, monster!\"\n",
      "The monster smiled and said, \"You're welcome, little one. I'm glad you're safe.\"\n",
      "The monster smiled and said, \"You're welcome, Lucy. I'm glad you're safe.\"\n",
      "\n",
      "\n",
      "Step 3600, Loss: 1.4839898347854614, Elapsed Time: 32.21 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lucy. She was three years old and loved to play outside. One day, Lucy's mommy said she had to go to the store to buy some groceries. Lucy was so excited! She ran to the store and bought some groceries.\n",
      "When Lucy got there, she saw a big, red apple. She was so happy! She picked it up and started to eat it. It was so yummy! Lucy was so happy! She ate the apple and felt so good.\n",
      "After she finished eating, Lucy's mommy said she had to go home. Lucy was sad because she couldn't play with her toys anymore. She was sad because she couldn't play with her toys anymore.\n",
      "\n",
      "\n",
      "Step 3800, Loss: 1.4641729593276978, Elapsed Time: 31.44 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl called Lucy. She was three years old and loved to play outside. One day, she was playing in the garden when she saw a big, shiny rock. She picked it up and showed it to her mom.\n",
      "\"Look, Mommy, I found a rock!\" she said.\n",
      "Her mom smiled and said, \"That's a great rock, Lucy. It's very pretty.\"\n",
      "Lucy was so happy to have a new friend. She showed it to her mom and said, \"Look, Mommy, I found a rock!\"\n",
      "Her mom smiled and said, \"That's great, Lucy. Let's keep it safe.\"\n",
      "So, they went inside and found a nice spot to keep it safe. They were so happy to have found a rock and they played together all day.\n",
      "\n",
      "\n",
      "Step 4000, Loss: 1.46944260597229, Elapsed Time: 31.43 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lucy. She was three years old and she loved to play outside. One day, she went to the park with her mom. She saw a big slide and wanted to go on it.\n",
      "Lucy went up to the slide and climbed up. She was very excited to go on the slide. She climbed up the ladder and slid down the slide. Whe!\n",
      "When she got to the top, she saw a big slide. She was so happy! She climbed up the ladder and slid down the slide. She was so happy!\n",
      "When she got to the top, she saw a big slide. She climbed up the ladder and slid down the slide. She was so happy! She laughed and slid down the slide.\n",
      "At the end of the day, Lucy was tired but happy. She had a fun day at the park.\n",
      "\n",
      "\n",
      "Step 4200, Loss: 1.4679889678955078, Elapsed Time: 31.67 seconds\n",
      "Generated text:\n",
      "Once upon a time there was a little girl named Lucy. She was very happy and loved to play with her friends. One day, she saw a big, red ball in the park. She wanted to play with it, but she was too scared to move.\n",
      "Lucy tried to move the ball, but it was too fast. She tried to move it, but it was too fast. She tried to move it, but it was too fast. She tried to move it, but it was too fast.\n",
      "Then, a kind lady saw Lucy and asked her if she could help her. She said yes, and Lucy was so happy. She gave Lucy a big hug and a big hug.\n",
      "The lady was so happy that she hugged Lucy and thanked her. She said she was a good girl and she was very happy.\n",
      "\n",
      "\n",
      "Step 4400, Loss: 1.4406870603561401, Elapsed Time: 31.44 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lucy. She was very happy and she loved to play with her toys. One day, she was playing with her toys when she saw a big, scary monster. She screamed and ran away.\n",
      "Lucy was very scared and ran away. She ran and hid behind a tree. Suddenly, a big dog came running towards her. Lucy was very scared and ran away.\n",
      "The big dog saw her and ran away. Lucy was very sad and scared. She ran back to her mom and dad and told them what happened. Her mom hugged her and said, \"It's okay, Lucy. We will always be careful and not hurt you.\"\n",
      "The big dog stopped and looked at Lucy. He said, \"I'm sorry, Lucy. I was just scared of you.\"\n",
      "Lucy was happy and hugged her mom. She said, \"It's okay, I'm here. I'm glad I was safe.\"\n",
      "\n",
      "\n",
      "Step 4600, Loss: 1.4200118780136108, Elapsed Time: 32.36 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Jane. She was very happy and loved to play outside. One day, she saw a big, scary monster. It was so scary that she started to scream.\n",
      "The monster was so scared that it started to run away. Jane was so scared that she ran away. She ran and ran until she was safe.\n",
      "The monster was so scared that it ran away. Jane was so scared that she ran away. She never got to be scared.\n",
      "The monster chased Jane and ran away. But she was too fast. She was too scared to run away.\n",
      "The monster chased Jane and ran until she was safe. She was safe and happy.\n",
      "\n",
      "\n",
      "Step 4800, Loss: 1.4398597478866577, Elapsed Time: 31.73 seconds\n",
      "Generated text:\n",
      "Once upon a time there was a little girl named Lucy. She was very happy and loved to play with her friends. One day, she was playing in the park when she saw a big, red ball. She wanted to play with it, so she ran to her mom.\n",
      "\"Mom, what is this?\" asked Lucy.\n",
      "\"It's a ball, sweetheart,\" her mom replied.\n",
      "\"It's a ball, but it's not a toy,\" Lucy said.\n",
      "Her mom smiled and said, \"Yes, it's a good idea. Let's go to the park and play with the ball.\"\n",
      "So, Lucy and her friends played with the ball all day long. They had so much fun and laughed together.\n",
      "\n",
      "\n",
      "Step 5000, Loss: 1.4402837753295898, Elapsed Time: 31.41 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lucy. She was three years old and loved to play in the park. One day, she saw a big, shiny thing in the grass. She wanted to take it home, but it was too heavy for her. She tried to lift it, but it was too heavy. She tried and tried, but it was too heavy. She tried and tried, but it was too heavy. She tried and tried, but it was too heavy. She tried and tried, but it was too heavy. She tried and tried, but it still couldn't. She felt sad and frustrated. She wished she had a friend to help her.\n",
      "\n",
      "\n",
      "Step 5200, Loss: 1.3984873294830322, Elapsed Time: 31.73 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lucy. She was three years old and loved to play with her toys. One day, she was playing with her toys when she accidentally broke one of the pieces. She was very sad and didn't know what to do.\n",
      "Her mom saw her crying and asked her what was wrong. Lucy told her that she had broken pieces. Her mom said, \"Don't worry, we can fix the pieces.\"\n",
      "So Lucy and her mom went to the kitchen and fixed the pieces. When they got to the kitchen, Lucy was so happy and thanked her mom. She said, \"I'm sorry I broke my piece.\"\n",
      "Lucy was so sad and cried. She said, \"I'm sorry, Mom. I didn't mean to break my piece.\"\n",
      "Her mom hugged her and said, \"It's okay, Lucy. Accidents happen. Let's clean it up again and we can fix it again.\"\n",
      "Lucy was so happy and hugged her mom. She hugged her and said, \"I'm sorry, Mom. I didn't mean to break the pieces. I'm sorry I didn't mean to break.\"\n",
      "Her mom hugged her and said, \"It's okay, Lucy. I forgive!!!!\n",
      "\n",
      "Step 5400, Loss: 1.4043035507202148, Elapsed Time: 32.24 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lucy. She was three years old and loved to play with her toys. One day, she was playing with her toys when she heard a loud noise. She looked around and saw a big, scary monster. The monster was roaring and roaring. Lucy was scared and ran away.\n",
      "The monster chased her and Lucy ran away. She ran and hid behind a tree. She never saw the monster again. The monster was gone and Lucy was safe.\n",
      "The monster was very strong and brave. He chased Lucy and ran away. Lucy was safe and sound. She never saw the monster again.\n",
      "\n",
      "\n",
      "Step 5600, Loss: 1.3977570533752441, Elapsed Time: 31.21 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lucy. She was very excited because she was going to the park. She was going to the park and she was going to the park.\n",
      "When she arrived at the park, she saw a big slide. She wanted to slide down the slide, but she was scared. She started to cry.\n",
      "Suddenly, a voice came from the park. It was a little girl who was crying. She had lost her toy. She looked around and saw a man. He was crying and crying.\n",
      "The little girl asked the man if he had seen her toy. The man said, \"I'm sorry, but I can't find my toy.\"\n",
      "The little girl was so happy and thanked the man. She said, \"I'm sorry I lost my toy. I can't find it.\"\n",
      "The man smiled and said, \"It's okay. I'm here. I'll help you find your toy.\"\n",
      "The little girl was so happy and thanked the man. She went back home and told her mom about the park. Her mom was very happy and said, \"I'm glad you're here. I'm glad you're here.\"\n",
      "\n",
      "\n",
      "Step 5800, Loss: 1.38675856590271, Elapsed Time: 32.10 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lucy. She was three years old and loved to play outside. One day, Lucy went to the park with her mom. She saw a big, red ball and wanted to play with it.\n",
      "Lucy ran to her mom and said, \"Mommy, can I play with the ball?\"\n",
      "Her mom said, \"No, Lucy. It's not safe. It's not safe to play with.\"\n",
      "Lucy was sad, but she understood. She said, \"Okay, Mommy. I will play with the ball.\"\n",
      "Her mom smiled and said, \"Okay, Lucy. Let's play with the ball.\"\n",
      "So Lucy and her mom played with the ball and had lots of fun. They had a lot of fun and Lucy was happy.\n",
      "\n",
      "\n",
      "Step 6000, Loss: 1.4106464385986328, Elapsed Time: 31.60 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lucy. She loved to play with her toys and her friends. One day, Lucy's mom asked her to clean up her toys. Lucy didn't want to clean up, so she said, \"No, Mommy! I don't want to clean up.\"\n",
      "Her mom said, \"But Lucy, you need to clean up your toys. You can do it if you don't clean up.\"\n",
      "Lucy didn't want to clean up, so she said, \"But Mommy, I want to clean up my toys. I want to clean up my toys.\"\n",
      "Her mom said, \"Okay, but you have to clean up your toys first. You can do it again tomorrow.\"\n",
      "Lucy nodded and said, \"Okay, Mommy. I will clean up.\"\n",
      "So, Lucy and her mom cleaned up and cleaned up all the toys. They were happy and clean.\n",
      "\n",
      "\n",
      "Step 6200, Loss: 1.3815696239471436, Elapsed Time: 31.65 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lily. She loved to play outside in the park. One day, she saw a big, scary dog. The dog was very fast and ran away.\n",
      "Lily was scared and ran after the dog. She ran as fast as she could. The dog was fast and ran away.\n",
      "Lily was safe and happy. She learned that it's important to be careful when she played in the park. She also learned that it's important to be careful when playing in the park.\n",
      "\n",
      "\n",
      "Step 6400, Loss: 1.3922070264816284, Elapsed Time: 31.11 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lucy. She was three years old and loved to play with her toys. One day, she was playing with her toys when she heard a loud noise. She looked up and saw a big, scary monster!\n",
      "The monster was very scary and it started to shake. Lucy was scared and didn't know what to do. She wanted to run away, but the monster was too fast.\n",
      "Suddenly, the monster started to shake and growl. Lucy was scared and ran away. She never saw the monster again.\n",
      "The monster was gone forever. Lucy was very sad and wished she had never gone to play with her toys. She never forgot the monster and the monster was never seen again.\n",
      "\n",
      "\n",
      "Step 6600, Loss: 1.3984136581420898, Elapsed Time: 31.23 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lucy. She was three years old and loved to play with her toys. One day, she was playing with her toys when she heard a loud noise. She looked up and saw a big, scary monster. She was scared and didn't know what to do.\n",
      "Suddenly, a big, scary monster appeared. It was the monster's owner, who was watching her. She was scared and ran away. Lucy was scared and ran home.\n",
      "The monster was very angry and started to chase her. Lucy was scared and ran as fast as she could. She ran as fast as she could, but the monster was too fast and caught her.\n",
      "The monster was caught and took her away. Lucy was very sad and cried. She never saw her toys again.\n",
      "\n",
      "\n",
      "Step 6800, Loss: 1.4159687757492065, Elapsed Time: 31.67 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lucy. She loved to play with her toys, but one day she found a big box in the attic. She was so excited to open it and see what was inside.\n",
      "Inside the box, she found a shiny new toy. It was a toy car. She wanted to play with it, but it was too expensive. She asked her mom if she could have it, but her mom said no.\n",
      "Lucy was sad and didn't know what to do. She wanted to play with the car, but her mom said no. She said it was too expensive and she should not have it.\n",
      "Lucy was sad and didn't know what to do. She wanted to play with the car, but her mom said no. Lucy was sad and didn't understand why her mom was so sad.\n",
      "Then, her mom came to her room and saw the car. She was so happy and hugged Lucy. She hugged her mom and said, \"I'm sorry, Mom. I didn't mean to spoil you.\"\n",
      "Lucy smiled and hugged her mom. She was so happy and grateful. She learned that sometimes things can be expensive, but it's important to be kind and share with others.\n",
      "!!!!\n",
      "\n",
      "Step 7000, Loss: 1.406973123550415, Elapsed Time: 32.53 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lucy. She was three years old and loved to play outside. One day, Lucy was playing in the park when she saw a big, scary dog. She was scared and ran away.\n",
      "Lucy was very scared. She didn't know what to do. She looked around and saw a big, scary dog. The dog was friendly and wagged its tail. Lucy was so scared that she ran away.\n",
      "The dog was very friendly and wagged its tail. Lucy was so happy that she had a friend. She hugged the dog and said, \"I'm here to play with you!\"\n",
      "The dog was so happy that Lucy had a friend. They played together all day long. They laughed and laughed until it was time to go home.\n",
      "\n",
      "\n",
      "Step 7200, Loss: 1.3830863237380981, Elapsed Time: 31.75 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lucy. She was three years old and loved to play outside.\n",
      "One day, Lucy was playing in the park when she saw a big, scary dog. She was scared and ran away.\n",
      "Lucy's mom saw her and said, \"Don't worry, Lucy. The dog is just a little bit. He is just playing.\"\n",
      "Lucy was so happy that she started to laugh and play. She ran around the park, laughing and having fun.\n",
      "Suddenly, Lucy heard a loud noise. She looked up and saw a big, scary dog. Lucy was scared and ran back home.\n",
      "The next day, Lucy was playing in the park again. She was so scared that she ran back home.\n",
      "\n",
      "\n",
      "Step 7400, Loss: 1.390000581741333, Elapsed Time: 31.51 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lucy. She was very happy and loved to play outside. One day, she went to the park with her mommy and daddy. She saw a big tree and wanted to climb it. She asked her mommy if she could go. Her mommy said yes and Lucy was so excited. She ran up the tree and started to climb. She was so high that she could see the whole park. She was so happy to be outside and she wanted to go home. She ran and ran until she reached the top of the tree. She looked around and saw a big, beautiful butterfly. She was so happy to see it and she wanted to touch it. She ran back to her mommy and daddy and told them about the butterfly. They said they were so lucky to see it and they all hugged. Lucy was so happy to be home with her new friend.\n",
      "\n",
      "\n",
      "Step 7600, Loss: 1.3659923076629639, Elapsed Time: 31.61 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Amy. She was three years old and loved to play. One day, she was playing in the park when she saw a big, shiny ball. She wanted to play with it, so she ran to get it.\n",
      "When she got close, she saw a big, shiny ball. She was so excited and wanted to play with it. She ran to the ball and picked it up. She threw the ball and it went very fast.\n",
      "When she got back to the park, she saw a big, shiny ball. She was so happy and ran to play with it. She kicked the ball and it flew high in the sky. She was so happy and she had so much fun with the big, shiny ball.\n",
      "\n",
      "\n",
      "Step 7800, Loss: 1.361992359161377, Elapsed Time: 31.49 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lucy. She was very excited because today was a special day. She had a big box of makeup that she wanted to try and play with.\n",
      "So, she put on her makeup and went outside to play. She saw a big, scary monster! The monster was very scary and it was so scary.\n",
      "Lucy was scared and started to cry. She wanted to get her makeup, but she was too scared to go inside. She tried to run away, but the monster was too fast.\n",
      "Then, the monster came closer and closer. It bit Lucy's finger and she was so scared. She screamed and cried until the monster was gone.\n",
      "The monster was so scared that it ran away. Lucy was so sad and scared. She wished she had been more careful.\n",
      "The next day, the monster came back to Lucy and she was so happy. She had a new friend and she was so happy to be safe.\n",
      "\n",
      "\n",
      "Step 8000, Loss: 1.3433030843734741, Elapsed Time: 31.83 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lucy. She was very excited because she was going to the park. She ran to the park and saw a big tree. She wanted to climb it.\n",
      "She asked her mom if she could climb the tree. Her mom said yes, so she climbed the tree. She climbed higher and higher until she reached the top.\n",
      "When she got to the top, she saw a big tree. She was so happy! She climbed up the tree and climbed up. She felt so good on the top.\n",
      "When she got to the top, she saw a big, beautiful view. She was so happy! She ran to the top and looked down. She felt so lucky to be able to climb the tree.\n",
      "Lucy was so glad she got to climb the tree. She was so glad she got to climb the tree. She was so glad she got to climb the tree.\n",
      "\n",
      "\n",
      "Step 8200, Loss: 1.3881804943084717, Elapsed Time: 31.76 seconds\n",
      "Generated text:\n",
      "Once upon a time, there was a little girl named Lucy. She was very happy and loved to play with her friends. One day, Lucy's mom asked her to help her. Lucy was very excited and said yes.\n",
      "Lucy's mom said, \"Let's go to the store and buy some food.\" Lucy was very happy and said, \"Yay! I want to buy some food!\"\n",
      "So, Lucy and her mom went to the store to buy food. When they got to the store, Lucy saw a big, yummy food. She asked her mom if she could buy it. Her mom said, \"No, Lucy. We can't buy it.\"\n",
      "Lucy was sad and said, \"But I want to buy something else. I want to buy something else.\"\n",
      "Her mom said, \"Okay, but you have to wait until you get to buy something else. It's a nice, but you have to wait until it's time to get something else.\"\n",
      "Lucy was so excited and she waited patiently. When it was time to go, she was so happy and thanked her mom for the yummy food.\n",
      "Final generated text:\n",
      "Once upon a time, there was a little girl named Lucy. She was very excited to go to the park. She put on her shoes and ran to the park.\n",
      "When she got to the park, she saw a big slide. She wanted to go on it. She ran to the slide and started to slide down. She was so fast!\n",
      "Suddenly, she heard a loud noise. It was a big, scary dog. The dog was barking loudly. Lucy was scared. She ran back to the park and started to run.\n",
      "The dog was so fast that it ran away. Lucy was safe. She was so happy she had gone on the slide.\n"
     ]
    }
   ],
   "source": [
    "model = create_model(rngs=nnx.Rngs(0))\n",
    "optimizer = nnx.ModelAndOptimizer(model, optax.adam(1e-3))\n",
    "metrics = nnx.MultiMetric(\n",
    "  loss=nnx.metrics.Average('loss'),\n",
    ")\n",
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "start_prompt = \"Once upon a time\"\n",
    "start_tokens = tokenizer.encode(start_prompt)[:maxlen]\n",
    "print(f\"Initial generated text:\")\n",
    "generated_text = model.generate_text(\n",
    "    maxlen, start_tokens\n",
    ")\n",
    "\n",
    "metrics_history = {\n",
    "  'train_loss': [],\n",
    "}\n",
    "\n",
    "prep_target_batch = jax.vmap(lambda tokens: jnp.concatenate((tokens[1:], jnp.array([0]))))\n",
    "\n",
    "step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    for batch in text_dl:\n",
    "        if len(batch) % len(jax.devices()) != 0:\n",
    "          continue  # skip the remaining elements\n",
    "        input_batch = jnp.array(jnp.array(batch).T)\n",
    "        target_batch = prep_target_batch(input_batch)\n",
    "        train_step(model, optimizer, metrics, jax.device_put((input_batch, target_batch), NamedSharding(mesh, P('batch', None))))\n",
    "\n",
    "        if (step + 1) % 200 == 0:\n",
    "          for metric, value in metrics.compute().items():\n",
    "              metrics_history[f'train_{metric}'].append(value)\n",
    "          metrics.reset()\n",
    "\n",
    "          elapsed_time = time.time() - start_time\n",
    "          print(f\"\\n\\nStep {step + 1}, Loss: {metrics_history['train_loss'][-1]}, Elapsed Time: {elapsed_time:.2f} seconds\")\n",
    "          start_time = time.time()\n",
    "\n",
    "          print(f\"Generated text:\")\n",
    "          generated_text = model.generate_text(\n",
    "              maxlen, start_tokens\n",
    "          )\n",
    "\n",
    "        step += 1\n",
    "\n",
    "# Final text generation\n",
    "print(f\"Final generated text:\")\n",
    "generated_text = model.generate_text(\n",
    "    maxlen, start_tokens\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thaLs6TD0lt5"
   },
   "source": [
    "Visualize the training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "B6Eg1Cz2y_iP",
    "outputId": "fb96b456-23a8-448f-ebc6-23d807a626d2"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(metrics_history['train_loss'])\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WB-ExEt1Zl1C"
   },
   "source": [
    "As you can see, the model goes from generating completely random words at the beginning to generating sensible tiny stories at the end of the training. So essentially we have pretrained a small LLM to write tiny stories for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "soPqiR1JNmjf"
   },
   "source": [
    "## Saving the checkpoint\n",
    "\n",
    "Save the model checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EkoFGCgSZ1yz",
    "outputId": "13434f77-2d73-4176-db24-7c0b2e232fe6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:[process=0][thread=MainThread] Skipped cross-host ArrayMetadata validation because only one process is found: process_index=0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array_metadatas       d\t\t      _METADATA        _sharding\n",
      "_CHECKPOINT_METADATA  manifest.ocdbt  ocdbt.process_0\n"
     ]
    }
   ],
   "source": [
    "import orbax.checkpoint as orbax\n",
    "\n",
    "state = nnx.state(model)\n",
    "\n",
    "checkpointer = orbax.PyTreeCheckpointer()\n",
    "checkpointer.save('/content/save', state)\n",
    "\n",
    "# Make sure the files are there\n",
    "!ls /content/save/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3813cbf2"
   },
   "source": [
    "## Profiling for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b5d933c6",
    "outputId": "b1233c24-0832-49f8-8568-e4ccbd73ca83"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.8/104.8 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m111.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -Uq tensorboard-plugin-profile tensorflow tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ac5fc4d"
   },
   "source": [
    "Load the tensorboard colab extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "74f0c212"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17c6131f"
   },
   "source": [
    "As we're going to be running this model a number of times, we need some scaffolding to more easily compare our work. For a baseline, we'll need to perform some warmup to guarantee that our code is JIT'd and that our TPUs are warm. For improved comparability, we'll only start tracing after we've finished warmup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "ddfd576e"
   },
   "outputs": [],
   "source": [
    "trace_dir = \"/tmp/jax-trace/\"\n",
    "\n",
    "def loop_step(batch, step):\n",
    "    input_batch = jnp.array(jnp.array(batch).T)\n",
    "    target_batch = prep_target_batch(input_batch)\n",
    "    train_step(model, optimizer, metrics, jax.device_put((input_batch, target_batch), NamedSharding(mesh, P('batch', None))))\n",
    "\n",
    "def generate_trace():\n",
    "    tracing_steps = 30\n",
    "    warmup_steps = 5\n",
    "    for current_step in range(warmup_steps + tracing_steps):\n",
    "        if current_step == warmup_steps:\n",
    "            jax.profiler.start_trace(trace_dir)\n",
    "        with jax.profiler.StepTraceAnnotation(\"train\", step_num=current_step):\n",
    "            batch = next(text_dl)\n",
    "            loop_step(batch, current_step)\n",
    "\n",
    "    jax.profiler.stop_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de70f5b7"
   },
   "source": [
    "Now we'll perform some traces to compare results of different batch sizes. This will take several minutes as we need to reprocess our input data to prepare new batches each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "bc9452a6"
   },
   "outputs": [],
   "source": [
    "trace_dir = \"/tmp/jax-trace-batch-comparison/\"\n",
    "\n",
    "batch_size = 64\n",
    "text_dl = iter(load_and_preprocess_data('TinyStories-train.txt', batch_size, maxlen))\n",
    "generate_trace()\n",
    "\n",
    "batch_size = 256\n",
    "text_dl = iter(load_and_preprocess_data('TinyStories-train.txt', batch_size, maxlen))\n",
    "generate_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ea379965"
   },
   "source": [
    "Run Tensorboard with the Profiler Plugin to compare our runs. Runs are listed in order from newest to oldest, so the top run in the list will be have `batch_size = 256`.\n",
    "\n",
    "The key metrics to focus on here for this hyperparameter are FLOPS Utilization and Average Step Time.\n",
    "\n",
    "In general, we want to maximize FLOPS Utilization while minimizing the step time per training example. In this case, we can see that increasing the batch size from 64 -> 256 achieves both of those. FLOPS increases from 16% to 27%. Average Step Time increase from 100ms to 260ms, however we increased our batch size by 300%. This means we move from 1.5ms per training example to 1.02ms per training example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b86c565a"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir=$trace_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "657967a5"
   },
   "source": [
    "Next, we can explore alternative parallelism methods. In cell #4, we used 4-way data parallel and 2-way tensor parallel. 8-way data parallel is another popular way. Let's compare results between them. To switch to 8-way data parallel, we'll replace the `Mesh` definition with:\n",
    "\n",
    "`mesh = Mesh(mesh_utils.create_device_mesh((8, 1)), ('batch', 'model'))`\n",
    "\n",
    "JAX will automatically figure out how to shard the model and data to use the new partition strategy and nothing else need to be done. Re-connect the TPU runtime and run it again to see how it runs.\n",
    "\n",
    "How simple and powerful is this! And that's the beauty of JAX automatic parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "80daa8dc"
   },
   "outputs": [],
   "source": [
    "trace_dir = \"/tmp/jax-trace-parallelism-comparison/\"\n",
    "\n",
    "mesh = Mesh(mesh_utils.create_device_mesh((4, 2)), ('batch', 'model'))\n",
    "generate_trace()\n",
    "\n",
    "mesh = Mesh(mesh_utils.create_device_mesh((8, 1)), ('batch', 'model'))\n",
    "generate_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ad96e72b"
   },
   "source": [
    "Once again we'll run tensorboard.\n",
    "\n",
    "Looking at the results, we see that the step times are nearly the same, however the FLOPS Utilization is at 13% for 8-way data parallelism compared to 27% or 4-way data parallelism.\n",
    "\n",
    "By looking at the Trace Viewer tool and looking under each TPU's ops, we can see that the TPUs spend a large amount of time idle while waiting for the host, as well as spending a good amount of time in `reduce_sum` operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "780e9c72"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir=$trace_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deca486e"
   },
   "source": [
    "By changing hyperparameters and comparing profiles, we're able to gain significant insights into our bottlenecks and limitations. These are just two examples of hyperparameters to tune, but plenty more of them will have significant effects on training speed and resource utilization."
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "machine_shape": "hm",
   "provenance": []
  },
  "jupytext": {
   "formats": "ipynb,md:myst"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
